{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Social Encoder\n",
    "\n",
    "TODO: implement sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SocialEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim=2, mlp_dim=16, hidden_dim=16, num_layers=1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim: number of input features (two features: x and y coordinates)\n",
    "            mlp_dim: spatial embedding dimension of xy coordinates to high dim\n",
    "            hidden_dim: number of dimensions of the hidden state and therefore the output of LSTM\n",
    "            num_layers: number of different LSTMs (different Weight sets for each LSTM)\n",
    "            \n",
    "        TODO: calc relative distance sorting module\n",
    "        \"\"\"\n",
    "        super(SocialEncoder, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.mlp_dim = mlp_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.mlp_spatial = nn.Sequential(nn.Linear(input_dim, mlp_dim), nn.ReLU())\n",
    "        self.lstm = nn.Sequential(nn.LSTM(mlp_dim, hidden_dim, num_layers))\n",
    "        \n",
    "    def forward(self, input_seq, batch_size, seq_len=20):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_seq: should be compatible with shape seq_len x batch_dim x input_dim\n",
    "            seq_len: length of the time sequence\n",
    "            batch_size: batch size (number of agents ?)\n",
    "        \n",
    "        Returns:\n",
    "            spatial_embedding_seq: xy coordinates embedded in multi dims\n",
    "            out: all the past hidden states\n",
    "            hidden: only the last hidden state as output with shape seq_len, batch_size, hidden_size\n",
    "            encoded_seq: the social property of agent encoded in 64 dim vector (not seq anymore)\n",
    "            \n",
    "        TODO: figure out if dimensions fit, also return the relative distance sorting\n",
    "\n",
    "        \"\"\"\n",
    "        spatial_embedding_seq = self.mlp_spatial(input_seq)\n",
    "        out, hidden = self.lstm(spatial_embedding_seq.view(seq_len, batch_size, -1).float())\n",
    "        encoded_seq = hidden[0]\n",
    "        \n",
    "        #encoded_seq # !!!! change here apply pooling\n",
    "        \n",
    "        return encoded_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 16])\n",
      "torch.Size([3, 1, 16])\n",
      "torch.Size([3, 3, 16])\n"
     ]
    }
   ],
   "source": [
    "input_data = torch.randn(2, 8, 3) #xy, seq_len, num_people\n",
    "SocEn = SocialEncoder()\n",
    "encoded_seq = SocEn(input_data.view(8,3,2), 3, 8)\n",
    "print(encoded_seq.shape) #seq_len, agents, xy (1*16 for each agents)\n",
    "encoded_seq = encoded_seq.view(3,1,16) #agents(batch), seq_len, xy\n",
    "print(encoded_seq.shape)\n",
    "\n",
    "# Placeholder for Sorting\n",
    "sorted_seq = encoded_seq.repeat(1,3,1)\n",
    "print(sorted_seq.shape) # sorted for agents, agents, xy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Social Attention\n",
    "TODO\n",
    "- return the multiplied version with social_features and weights\n",
    "- concat inside forward function after figuring out the h shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SocialAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, social_feats_dim =16, decoder_h_dim=32, mlp_dims=[64,128,64,1], num_max_agents=32):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            social_feats_dim: dimension of embedded and sorted social features of input\n",
    "            decoder_h_dim: dimensions of the hidden state of the decoder\n",
    "            mlp_dims: dimensions of layers of mlp for embedding social features and hiden states of decoder\n",
    "            num_max_agents: maximum possible number of agents in a scene\n",
    "            \n",
    "        TODO: figure out how num_max_agents are related to this function (set zero non agents)\n",
    "        \"\"\"\n",
    "        super(SocialAttention, self).__init__()\n",
    "        \n",
    "        self.social_feats_dim = social_feats_dim\n",
    "        self.decoder_h_dim = decoder_h_dim\n",
    "        self.mlp_dims = mlp_dims\n",
    "        self.num_max_agents = num_max_agents\n",
    "        \n",
    "        self.input_dim = self.decoder_h_dim + self.social_feats_dim \n",
    "        self.num_layers = len(self.mlp_dims)\n",
    "        \n",
    "        # Attention\n",
    "        self.layer1 = nn.Sequential(nn.Linear(self.input_dim, mlp_dims[0]), nn.ReLU())\n",
    "        self.layer2 = nn.Sequential(nn.Linear(mlp_dims[0]   , mlp_dims[1]), nn.ReLU())\n",
    "        self.layer3 = nn.Sequential(nn.Linear(mlp_dims[1]   , mlp_dims[2]), nn.ReLU())\n",
    "        self.layer4 = nn.Sequential(nn.Linear(mlp_dims[2]   , mlp_dims[3]), nn.Softmax(dim=1)) \n",
    "        \n",
    "        \n",
    "    def forward(self, input_soc_attn):#h_states, social_features):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            h_states: hidden states of the decoder LSTM in GAN for agents\n",
    "            social_features: output of encoder, joint social feature vector\n",
    "        Returns:\n",
    "            x : the social context. highlights which agents are most important.\n",
    "        \n",
    "        TODO: figure out h_states dimensions and fix the concat\n",
    "        \"\"\"\n",
    "        #social_weights = torch.concat(h_states, social_features, dim = 0) # fix here\n",
    "        social_weights = self.layer1(input_soc_attn)\n",
    "        social_weights = self.layer2(social_weights)\n",
    "        social_weights = self.layer3(social_weights)\n",
    "        social_weights = self.layer4(social_weights)\n",
    "        \n",
    "        weighted_feats = social_weights#torch.mul(social_weights, social_features)\n",
    "        \n",
    "        return weighted_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3, 48])\n",
      "torch.Size([3, 3, 1])\n",
      "torch.Size([3, 3, 16])\n",
      "torch.Size([3, 1, 48])\n"
     ]
    }
   ],
   "source": [
    "SocAttn = SocialAttention()\n",
    "h_decoder = torch.randn(1,3,32)\n",
    "input_soc_attn = torch.cat( (sorted_seq, h_decoder.view(3, 1, 32).repeat(1, 3, 1)), dim=2   )\n",
    "print(input_soc_attn.shape)\n",
    "\n",
    "social_weights = SocAttn(input_soc_attn)\n",
    "print(social_weights.shape)\n",
    "\n",
    "weighted_feats_soc = torch.mul(sorted_seq, social_weights.repeat(1,1,16))\n",
    "print(weighted_feats_soc.shape) # agents(batch), sorted agents, xy\n",
    "\n",
    "weighted_feats_soc_flat = weighted_feats_soc.reshape(3,1,48)\n",
    "print(weighted_feats_soc_flat.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Physical Encoder\n",
    "TODO:\n",
    "- resnet dimension mismatch\n",
    "- taking output vs features ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhysicalEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, cnn_type='resnet'):\n",
    "        '''\n",
    "        Args:\n",
    "            cnn_type: a string that tells which pretrained model to use 'resnet' or 'vgg'\n",
    "        '''\n",
    "        super(PhysicalEncoder, self).__init__()\n",
    "        \n",
    "        if cnn_type == 'resnet':\n",
    "            self.cnn = models.resnet18(pretrained=True)\n",
    "        elif cnn_type == 'vgg':\n",
    "            self.cnn = models.vgg16(pretrained = True)\n",
    "        else:\n",
    "            print(\"Pretrained model not known\")\n",
    "            \n",
    "        modules = list(self.cnn.children())[:-1]\n",
    "        self.cnn = nn.Sequential(*modules)\n",
    "        for p in self.cnn.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "                \n",
    "    def forward(self, input_scene):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            Input_scene: input scene at time t or static image ?\n",
    "        Returns:\n",
    "            embedded_scene: raw CNN output\n",
    "        \"\"\"\n",
    "        if len(input_scene)==3:\n",
    "            input_scene = input_scene.unsqueeze(0)\n",
    "        embedded_scene = self.cnn(input_scene)\n",
    "        return embedded_scene #self.vgg.features(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "PhyEnc = PhysicalEncoder('vgg')\n",
    "input_scene = torch.randn(3, 640, 480)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512, 20, 15])\n"
     ]
    }
   ],
   "source": [
    "raw_feats = PhyEnc(input_scene.unsqueeze(0))\n",
    "print(raw_feats.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Physical Attention\n",
    "\n",
    "TODO\n",
    "- attention inside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhysicalAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels = 512, feat_dim=[20,15], embedding_dim = 16, decoder_h_dim=32):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_channels: number of channels of the raw CNN output images\n",
    "            feat_dim: image dimensions of the CNN output images\n",
    "            embedding_dim: expected output dimension of the fully connected embedding layer  \n",
    "            decoder_h_dim: hidden state dimension of the decoder\n",
    "        \"\"\"\n",
    "        super(PhysicalAttention, self).__init__()\n",
    "        \n",
    "        self.in_channels = in_channels\n",
    "        self.feat_dim = feat_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.decoder_h_dim = decoder_h_dim\n",
    "        self.attention_dim = embedding_dim + decoder_h_dim # !!!!\n",
    "        \n",
    "        if len(feat_dim) != 2:\n",
    "            self.feat_dim = [ feat_dim[0], feat_dim[0] ]\n",
    "\n",
    " \n",
    "        # Embedding\n",
    "        self.conv = nn.Sequential( nn.Conv2d(512, 1, 3), nn.ReLU() ) # !!!!!\n",
    "        self.embedding_mlp = nn.Sequential(nn.Linear((feat_dim[0]-2)*(feat_dim[1]-2), embedding_dim), nn.Softmax(dim=1))\n",
    "        \n",
    "        # Attention Module\n",
    "        self.attention_mlp = nn.Sequential(nn.Linear(self.attention_dim, embedding_dim), nn.Tanh(), nn.Softmax(dim=1))\n",
    "        \n",
    "    def forward(self, physical_features):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          h_states: hidden states of the decoder LSTM in GAN for agents\n",
    "          physical_features: raw output of VGG or ResNet\n",
    "          \n",
    "        Returns:\n",
    "            x: the physical context. feasible paths. attention weights?\n",
    "            \n",
    "        TODO: VGG or ResNet dimensions, what do we do with the attention weights ?\n",
    "        \"\"\"\n",
    "        \n",
    "        physical_embedding = self.conv(physical_features) \n",
    "        physical_embedding = self.embedding_mlp(physical_embedding.view(1,-1))\n",
    "        \n",
    "      #  attention_input = torch.cat(h_states, physical_embedding) #!!!\n",
    "      #  physical_weights = self.attention_mlp(attention_input)\n",
    "        \n",
    "        weighted_feats = physical_embedding#torch.mul(physical_weights, physical_features)\n",
    "        \n",
    "        return weighted_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16])\n",
      "torch.Size([3, 1, 48])\n",
      "torch.Size([3, 1, 16])\n",
      "torch.Size([3, 1, 16])\n"
     ]
    }
   ],
   "source": [
    "PhyAttn = PhysicalAttention()\n",
    "physical_embeddings = PhyAttn(raw_feats)\n",
    "print(physical_embeddings.shape)\n",
    "\n",
    "h_decoder = torch.randn(1,3,32)\n",
    "input_phy_attn = torch.cat( (physical_embeddings.unsqueeze(0).repeat(3,1,1), h_decoder.view(3, 1, 32)), dim=2   )\n",
    "print(input_phy_attn.shape)\n",
    "\n",
    "physical_weights = PhyAttn.attention_mlp(input_phy_attn)\n",
    "print(physical_weights.shape)\n",
    "\n",
    "weighted_feats_phy = torch.mul(physical_embeddings.unsqueeze(0).repeat(3,1,1), physical_weights)\n",
    "print(weighted_feats_phy.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "TODO:\n",
    "fix the view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderGAN(nn.Module): #also called \"Generator\"\n",
    "    \n",
    "    def __init__(self, input_dim=128, embedding_dim=16, hidden_dim=32, output_dim=2):\n",
    "        super(DecoderGAN, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.mlp_embedding = nn.Sequential(nn.Linear(input_dim, embedding_dim), nn.ReLU())\n",
    "        self.lstm = nn.Sequential(nn.LSTM(embedding_dim, hidden_dim, num_layers=1))\n",
    "        self.mlp_output = nn.Sequential(nn.Linear(hidden_dim, output_dim), nn.ReLU())\n",
    "        \n",
    "        \n",
    "    def forward(self, input_features):#weighted_physical, weighted_social, noise):\n",
    "        #input_features = torch.cat(weighted_physical, weighted_social, noise)\n",
    "        \n",
    "        x = self.mlp_embedding(input_features)\n",
    "        _, x = self.lstm(x.view(1,3,-1)) # !!!\n",
    "        hidden_states = x[0]\n",
    "        xy_estimated = self.mlp_output(hidden_states)\n",
    "        \n",
    "        return hidden_states, xy_estimated\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "DecGan = DecoderGAN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1, 128])\n",
      "torch.Size([1, 3, 32])\n"
     ]
    }
   ],
   "source": [
    "input_dec = torch.cat( (weighted_feats_phy, weighted_feats_soc_flat, torch.randn(3,1,64 )), dim=2 )\n",
    "print(input_dec.shape)\n",
    "\n",
    "hidden_states, xy_estimated = DecGan(input_dec)\n",
    "print(hidden_states.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
