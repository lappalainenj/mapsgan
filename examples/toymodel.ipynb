{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from mapsgan import Trajectories\n",
    "import sys\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "td = Trajectories('/mnt/Clouds/MapsGAN/data/eth/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Assertion `cur_target >= 0 && cur_target < n_classes' failed.  at /opt/conda/conda-bld/pytorch-cpu_1532578932944/work/aten/src/THNN/generic/ClassNLLCriterion.c:93",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-361-b8e2e5bac93d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m#print([idx2char(int(torch.max(hid, 1)[1])] for hid in hidden])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m#     for input, label in zip(x_onehot, labels):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mapsgan/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mapsgan/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    860\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m--> 862\u001b[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[1;32m    863\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mapsgan/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   1548\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1549\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1550\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mapsgan/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   1405\u001b[0m                          .format(input.size(0), target.size(0)))\n\u001b[1;32m   1406\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1408\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Assertion `cur_target >= 0 && cur_target < n_classes' failed.  at /opt/conda/conda-bld/pytorch-cpu_1532578932944/work/aten/src/THNN/generic/ClassNLLCriterion.c:93"
     ]
    }
   ],
   "source": [
    "model = nn.RNN(input_size=input_size, hidden_size=hidden_size, batch_first = True)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optim = torch.optim.Adam(model.parameters(), lr = 0.1)\n",
    "hidden = torch.zeros(num_layers, batch_size, hidden_size)\n",
    "\n",
    "for epoch in range(100):\n",
    "    optim.zero_grad()\n",
    "    loss = 0\n",
    "    output, hidden = model(x_onehot, hidden)\n",
    "    output = output.view(sequence_length, -1)\n",
    "    #print([idx2char(int(torch.max(hid, 1)[1])] for hid in hidden])\n",
    "    loss += criterion(output, labels.squeeze().long())\n",
    "    \n",
    "#     for input, label in zip(x_onehot, labels):\n",
    "#         hidden, output = model(input, hidden)\n",
    "#         _, pred = torch.max(output, 1)\n",
    "#         print(idx2char[int(pred)])\n",
    "#         loss  += criterion(output, label.long())\n",
    "        \n",
    "    print(\"epoch: %d, loss: %1.3f\"%(epoch+1, loss.data[0]))\n",
    "    loss.backward(retain_graph=True)\n",
    "    optim.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trajectory Toy Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyGenerator(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim = 32, hidden_dim = 20, batch_size = 32):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim)\n",
    "        self.linear = nn.Sequential(nn.Linear(hidden_dim, 20*2),\n",
    "                                    nn.ReLU())\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, self.batch_size, self.hidden_dim)\n",
    "        \n",
    "    def forward(self, z):\n",
    "        _, (x, _) = self.lstm(z)\n",
    "        x = self.linear(x)\n",
    "        return x.view(self.hidden_dim, self.batch_size, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim=2, mlp_dim=16, hidden_dim=64, num_layers=1):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.mlp_dim = mlp_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.mlp_spatial_lstm = nn.Sequential(#nn.Linear(input_dim, mlp_dim),\n",
    "                                              #nn.ReLU(),\n",
    "                                              nn.LSTM(#mlp_dim,\n",
    "                                                      input_dim,\n",
    "                                                      hidden_dim,\n",
    "                                                      num_layers))\n",
    "        self.mlp_bool = nn.Sequential(nn.Linear(hidden_dim, 20),\n",
    "                                      nn.Sigmoid())\n",
    "        \n",
    "    def forward(self, input_seq):\n",
    "        _, (hidden, _) = self.mlp_spatial_lstm(input_seq)\n",
    "        return self.mlp_bool(hidden.squeeze()).view(20, 32, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 32, 2])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TGen = ToyGenerator()\n",
    "z = torch.randn(32, 32, 32)\n",
    "TGen(z).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, loss_G_total: 8.284, loss_G_l1: 7.577, loss_G_gan: 0.707, loss_D: 1.386\n",
      "epoch: 2, loss_G_total: 8.188, loss_G_l1: 7.455, loss_G_gan: 0.733, loss_D: 1.363\n",
      "epoch: 3, loss_G_total: 7.792, loss_G_l1: 7.016, loss_G_gan: 0.776, loss_D: 1.327\n",
      "epoch: 4, loss_G_total: 7.204, loss_G_l1: 6.261, loss_G_gan: 0.942, loss_D: 1.210\n",
      "epoch: 5, loss_G_total: 6.323, loss_G_l1: 4.868, loss_G_gan: 1.455, loss_D: 0.988\n",
      "epoch: 6, loss_G_total: 5.483, loss_G_l1: 3.468, loss_G_gan: 2.014, loss_D: 0.872\n",
      "epoch: 7, loss_G_total: 5.296, loss_G_l1: 2.678, loss_G_gan: 2.618, loss_D: 0.810\n",
      "epoch: 8, loss_G_total: 5.668, loss_G_l1: 2.439, loss_G_gan: 3.229, loss_D: 0.776\n",
      "epoch: 9, loss_G_total: 6.169, loss_G_l1: 2.400, loss_G_gan: 3.768, loss_D: 0.756\n",
      "epoch: 10, loss_G_total: 6.556, loss_G_l1: 2.342, loss_G_gan: 4.215, loss_D: 0.740\n",
      "epoch: 11, loss_G_total: 6.803, loss_G_l1: 2.223, loss_G_gan: 4.580, loss_D: 0.723\n",
      "epoch: 12, loss_G_total: 7.068, loss_G_l1: 2.193, loss_G_gan: 4.875, loss_D: 0.704\n",
      "epoch: 13, loss_G_total: 7.310, loss_G_l1: 2.198, loss_G_gan: 5.112, loss_D: 0.682\n",
      "epoch: 14, loss_G_total: 7.533, loss_G_l1: 2.231, loss_G_gan: 5.303, loss_D: 0.659\n",
      "epoch: 15, loss_G_total: 7.579, loss_G_l1: 2.123, loss_G_gan: 5.456, loss_D: 0.634\n",
      "epoch: 16, loss_G_total: 7.562, loss_G_l1: 1.981, loss_G_gan: 5.581, loss_D: 0.608\n",
      "epoch: 17, loss_G_total: 7.558, loss_G_l1: 1.871, loss_G_gan: 5.686, loss_D: 0.582\n",
      "epoch: 18, loss_G_total: 7.605, loss_G_l1: 1.827, loss_G_gan: 5.778, loss_D: 0.555\n",
      "epoch: 19, loss_G_total: 5.428, loss_G_l1: 1.889, loss_G_gan: 3.539, loss_D: 1.370\n",
      "epoch: 20, loss_G_total: 7.988, loss_G_l1: 2.083, loss_G_gan: 5.905, loss_D: 0.514\n",
      "epoch: 21, loss_G_total: 8.148, loss_G_l1: 2.199, loss_G_gan: 5.949, loss_D: 0.498\n",
      "epoch: 22, loss_G_total: 8.265, loss_G_l1: 2.274, loss_G_gan: 5.991, loss_D: 0.481\n",
      "epoch: 23, loss_G_total: 8.326, loss_G_l1: 2.294, loss_G_gan: 6.032, loss_D: 0.463\n",
      "epoch: 24, loss_G_total: 5.885, loss_G_l1: 2.237, loss_G_gan: 3.649, loss_D: 1.066\n",
      "epoch: 25, loss_G_total: 5.851, loss_G_l1: 2.180, loss_G_gan: 3.671, loss_D: 1.043\n",
      "epoch: 26, loss_G_total: 5.811, loss_G_l1: 2.114, loss_G_gan: 3.697, loss_D: 1.007\n",
      "epoch: 27, loss_G_total: 5.805, loss_G_l1: 2.078, loss_G_gan: 3.727, loss_D: 0.963\n",
      "epoch: 28, loss_G_total: 5.836, loss_G_l1: 2.074, loss_G_gan: 3.762, loss_D: 0.915\n",
      "epoch: 29, loss_G_total: 5.845, loss_G_l1: 2.044, loss_G_gan: 3.801, loss_D: 0.865\n",
      "epoch: 30, loss_G_total: 5.823, loss_G_l1: 1.982, loss_G_gan: 3.842, loss_D: 0.812\n",
      "epoch: 31, loss_G_total: 5.751, loss_G_l1: 1.865, loss_G_gan: 3.886, loss_D: 0.758\n",
      "epoch: 32, loss_G_total: 5.746, loss_G_l1: 1.812, loss_G_gan: 3.933, loss_D: 0.702\n",
      "epoch: 33, loss_G_total: 5.798, loss_G_l1: 1.814, loss_G_gan: 3.983, loss_D: 0.645\n",
      "epoch: 34, loss_G_total: 5.828, loss_G_l1: 1.786, loss_G_gan: 4.043, loss_D: 0.588\n",
      "epoch: 35, loss_G_total: 5.788, loss_G_l1: 1.786, loss_G_gan: 4.001, loss_D: 0.534\n",
      "epoch: 36, loss_G_total: 6.040, loss_G_l1: 1.831, loss_G_gan: 4.208, loss_D: 0.484\n",
      "epoch: 37, loss_G_total: 6.217, loss_G_l1: 1.890, loss_G_gan: 4.327, loss_D: 0.442\n",
      "epoch: 38, loss_G_total: 6.312, loss_G_l1: 1.866, loss_G_gan: 4.446, loss_D: 0.407\n",
      "epoch: 39, loss_G_total: 6.421, loss_G_l1: 1.850, loss_G_gan: 4.571, loss_D: 0.377\n",
      "epoch: 40, loss_G_total: 6.492, loss_G_l1: 1.787, loss_G_gan: 4.704, loss_D: 0.350\n",
      "epoch: 41, loss_G_total: 6.607, loss_G_l1: 1.763, loss_G_gan: 4.845, loss_D: 0.326\n",
      "epoch: 42, loss_G_total: 6.719, loss_G_l1: 1.729, loss_G_gan: 4.990, loss_D: 0.305\n",
      "epoch: 43, loss_G_total: 6.865, loss_G_l1: 1.728, loss_G_gan: 5.137, loss_D: 0.285\n",
      "epoch: 44, loss_G_total: 7.027, loss_G_l1: 1.740, loss_G_gan: 5.287, loss_D: 0.267\n",
      "epoch: 45, loss_G_total: 7.133, loss_G_l1: 1.765, loss_G_gan: 5.369, loss_D: 0.250\n",
      "epoch: 46, loss_G_total: 7.279, loss_G_l1: 1.744, loss_G_gan: 5.535, loss_D: 0.233\n",
      "epoch: 47, loss_G_total: 7.484, loss_G_l1: 1.747, loss_G_gan: 5.738, loss_D: 0.217\n",
      "epoch: 48, loss_G_total: 7.633, loss_G_l1: 1.748, loss_G_gan: 5.884, loss_D: 0.202\n",
      "epoch: 49, loss_G_total: 7.770, loss_G_l1: 1.744, loss_G_gan: 6.026, loss_D: 0.188\n",
      "epoch: 50, loss_G_total: 7.902, loss_G_l1: 1.739, loss_G_gan: 6.162, loss_D: 0.174\n",
      "epoch: 51, loss_G_total: 8.043, loss_G_l1: 1.750, loss_G_gan: 6.293, loss_D: 0.161\n",
      "epoch: 52, loss_G_total: 8.177, loss_G_l1: 1.759, loss_G_gan: 6.419, loss_D: 0.148\n",
      "epoch: 53, loss_G_total: 8.271, loss_G_l1: 1.743, loss_G_gan: 6.529, loss_D: 0.136\n",
      "epoch: 54, loss_G_total: 8.343, loss_G_l1: 1.734, loss_G_gan: 6.610, loss_D: 0.125\n",
      "epoch: 55, loss_G_total: 8.418, loss_G_l1: 1.730, loss_G_gan: 6.688, loss_D: 0.114\n",
      "epoch: 56, loss_G_total: 8.501, loss_G_l1: 1.704, loss_G_gan: 6.797, loss_D: 0.105\n",
      "epoch: 57, loss_G_total: 8.717, loss_G_l1: 1.780, loss_G_gan: 6.937, loss_D: 0.096\n",
      "epoch: 58, loss_G_total: 8.912, loss_G_l1: 1.846, loss_G_gan: 7.066, loss_D: 0.088\n",
      "epoch: 59, loss_G_total: 9.011, loss_G_l1: 1.839, loss_G_gan: 7.172, loss_D: 0.080\n",
      "epoch: 60, loss_G_total: 9.080, loss_G_l1: 1.814, loss_G_gan: 7.265, loss_D: 0.073\n",
      "epoch: 61, loss_G_total: 9.137, loss_G_l1: 1.784, loss_G_gan: 7.353, loss_D: 0.067\n",
      "epoch: 62, loss_G_total: 9.225, loss_G_l1: 1.788, loss_G_gan: 7.437, loss_D: 0.061\n",
      "epoch: 63, loss_G_total: 9.280, loss_G_l1: 1.761, loss_G_gan: 7.519, loss_D: 0.056\n",
      "epoch: 64, loss_G_total: 9.329, loss_G_l1: 1.731, loss_G_gan: 7.597, loss_D: 0.052\n",
      "epoch: 65, loss_G_total: 9.411, loss_G_l1: 1.739, loss_G_gan: 7.672, loss_D: 0.048\n",
      "epoch: 66, loss_G_total: 9.452, loss_G_l1: 1.710, loss_G_gan: 7.742, loss_D: 0.044\n",
      "epoch: 67, loss_G_total: 9.515, loss_G_l1: 1.706, loss_G_gan: 7.809, loss_D: 0.041\n",
      "epoch: 68, loss_G_total: 9.574, loss_G_l1: 1.702, loss_G_gan: 7.872, loss_D: 0.038\n",
      "epoch: 69, loss_G_total: 9.668, loss_G_l1: 1.737, loss_G_gan: 7.932, loss_D: 0.035\n",
      "epoch: 70, loss_G_total: 9.726, loss_G_l1: 1.738, loss_G_gan: 7.988, loss_D: 0.033\n",
      "epoch: 71, loss_G_total: 9.740, loss_G_l1: 1.698, loss_G_gan: 8.041, loss_D: 0.030\n",
      "epoch: 72, loss_G_total: 9.796, loss_G_l1: 1.704, loss_G_gan: 8.092, loss_D: 0.028\n",
      "epoch: 73, loss_G_total: 9.851, loss_G_l1: 1.711, loss_G_gan: 8.139, loss_D: 0.027\n",
      "epoch: 74, loss_G_total: 9.886, loss_G_l1: 1.700, loss_G_gan: 8.186, loss_D: 0.025\n",
      "epoch: 75, loss_G_total: 9.924, loss_G_l1: 1.694, loss_G_gan: 8.230, loss_D: 0.024\n",
      "epoch: 76, loss_G_total: 9.982, loss_G_l1: 1.711, loss_G_gan: 8.271, loss_D: 0.022\n",
      "epoch: 77, loss_G_total: 10.029, loss_G_l1: 1.717, loss_G_gan: 8.312, loss_D: 0.021\n",
      "epoch: 78, loss_G_total: 10.055, loss_G_l1: 1.707, loss_G_gan: 8.348, loss_D: 0.020\n",
      "epoch: 79, loss_G_total: 10.093, loss_G_l1: 1.708, loss_G_gan: 8.386, loss_D: 0.019\n",
      "epoch: 80, loss_G_total: 10.136, loss_G_l1: 1.716, loss_G_gan: 8.420, loss_D: 0.018\n",
      "epoch: 81, loss_G_total: 10.168, loss_G_l1: 1.714, loss_G_gan: 8.454, loss_D: 0.017\n",
      "epoch: 82, loss_G_total: 10.187, loss_G_l1: 1.701, loss_G_gan: 8.486, loss_D: 0.016\n",
      "epoch: 83, loss_G_total: 10.213, loss_G_l1: 1.696, loss_G_gan: 8.518, loss_D: 0.016\n",
      "epoch: 84, loss_G_total: 10.284, loss_G_l1: 1.736, loss_G_gan: 8.548, loss_D: 0.015\n",
      "epoch: 85, loss_G_total: 10.307, loss_G_l1: 1.730, loss_G_gan: 8.577, loss_D: 0.014\n",
      "epoch: 86, loss_G_total: 10.322, loss_G_l1: 1.718, loss_G_gan: 8.605, loss_D: 0.014\n",
      "epoch: 87, loss_G_total: 10.351, loss_G_l1: 1.720, loss_G_gan: 8.631, loss_D: 0.013\n",
      "epoch: 88, loss_G_total: 10.347, loss_G_l1: 1.691, loss_G_gan: 8.655, loss_D: 0.013\n",
      "epoch: 89, loss_G_total: 10.368, loss_G_l1: 1.690, loss_G_gan: 8.679, loss_D: 0.012\n",
      "epoch: 90, loss_G_total: 10.400, loss_G_l1: 1.698, loss_G_gan: 8.701, loss_D: 0.012\n",
      "epoch: 91, loss_G_total: 10.426, loss_G_l1: 1.702, loss_G_gan: 8.724, loss_D: 0.012\n",
      "epoch: 92, loss_G_total: 10.439, loss_G_l1: 1.693, loss_G_gan: 8.746, loss_D: 0.011\n",
      "epoch: 93, loss_G_total: 10.457, loss_G_l1: 1.689, loss_G_gan: 8.768, loss_D: 0.011\n",
      "epoch: 94, loss_G_total: 10.490, loss_G_l1: 1.703, loss_G_gan: 8.787, loss_D: 0.011\n",
      "epoch: 95, loss_G_total: 10.504, loss_G_l1: 1.697, loss_G_gan: 8.807, loss_D: 0.010\n",
      "epoch: 96, loss_G_total: 10.555, loss_G_l1: 1.729, loss_G_gan: 8.825, loss_D: 0.010\n",
      "epoch: 97, loss_G_total: 10.536, loss_G_l1: 1.694, loss_G_gan: 8.843, loss_D: 0.010\n",
      "epoch: 98, loss_G_total: 10.563, loss_G_l1: 1.703, loss_G_gan: 8.860, loss_D: 0.009\n",
      "epoch: 99, loss_G_total: 10.564, loss_G_l1: 1.687, loss_G_gan: 8.877, loss_D: 0.009\n",
      "epoch: 100, loss_G_total: 10.583, loss_G_l1: 1.690, loss_G_gan: 8.894, loss_D: 0.009\n",
      "epoch: 101, loss_G_total: 10.633, loss_G_l1: 1.723, loss_G_gan: 8.910, loss_D: 0.009\n",
      "epoch: 102, loss_G_total: 10.603, loss_G_l1: 1.676, loss_G_gan: 8.927, loss_D: 0.008\n",
      "epoch: 103, loss_G_total: 10.648, loss_G_l1: 1.707, loss_G_gan: 8.942, loss_D: 0.008\n",
      "epoch: 104, loss_G_total: 10.647, loss_G_l1: 1.691, loss_G_gan: 8.956, loss_D: 0.008\n",
      "epoch: 105, loss_G_total: 10.688, loss_G_l1: 1.717, loss_G_gan: 8.971, loss_D: 0.008\n",
      "epoch: 106, loss_G_total: 10.713, loss_G_l1: 1.727, loss_G_gan: 8.986, loss_D: 0.008\n",
      "epoch: 107, loss_G_total: 10.687, loss_G_l1: 1.688, loss_G_gan: 8.999, loss_D: 0.008\n",
      "epoch: 108, loss_G_total: 10.723, loss_G_l1: 1.710, loss_G_gan: 9.012, loss_D: 0.007\n",
      "epoch: 109, loss_G_total: 10.712, loss_G_l1: 1.687, loss_G_gan: 9.025, loss_D: 0.007\n",
      "epoch: 110, loss_G_total: 10.739, loss_G_l1: 1.701, loss_G_gan: 9.038, loss_D: 0.007\n",
      "epoch: 111, loss_G_total: 10.761, loss_G_l1: 1.711, loss_G_gan: 9.050, loss_D: 0.007\n",
      "epoch: 112, loss_G_total: 10.789, loss_G_l1: 1.727, loss_G_gan: 9.062, loss_D: 0.007\n",
      "epoch: 113, loss_G_total: 10.790, loss_G_l1: 1.716, loss_G_gan: 9.073, loss_D: 0.007\n",
      "epoch: 114, loss_G_total: 10.800, loss_G_l1: 1.716, loss_G_gan: 9.084, loss_D: 0.007\n",
      "epoch: 115, loss_G_total: 10.783, loss_G_l1: 1.688, loss_G_gan: 9.095, loss_D: 0.006\n",
      "epoch: 116, loss_G_total: 10.811, loss_G_l1: 1.704, loss_G_gan: 9.107, loss_D: 0.006\n",
      "epoch: 117, loss_G_total: 10.839, loss_G_l1: 1.720, loss_G_gan: 9.119, loss_D: 0.006\n",
      "epoch: 118, loss_G_total: 10.827, loss_G_l1: 1.697, loss_G_gan: 9.130, loss_D: 0.006\n",
      "epoch: 119, loss_G_total: 10.831, loss_G_l1: 1.691, loss_G_gan: 9.140, loss_D: 0.006\n",
      "epoch: 120, loss_G_total: 10.857, loss_G_l1: 1.707, loss_G_gan: 9.150, loss_D: 0.006\n",
      "epoch: 121, loss_G_total: 10.880, loss_G_l1: 1.720, loss_G_gan: 9.160, loss_D: 0.006\n",
      "epoch: 122, loss_G_total: 10.881, loss_G_l1: 1.710, loss_G_gan: 9.171, loss_D: 0.006\n",
      "epoch: 123, loss_G_total: 10.873, loss_G_l1: 1.691, loss_G_gan: 9.182, loss_D: 0.006\n",
      "epoch: 124, loss_G_total: 10.886, loss_G_l1: 1.693, loss_G_gan: 9.193, loss_D: 0.006\n",
      "epoch: 125, loss_G_total: 10.889, loss_G_l1: 1.686, loss_G_gan: 9.203, loss_D: 0.005\n",
      "epoch: 126, loss_G_total: 10.899, loss_G_l1: 1.686, loss_G_gan: 9.213, loss_D: 0.005\n",
      "epoch: 127, loss_G_total: 10.922, loss_G_l1: 1.700, loss_G_gan: 9.222, loss_D: 0.005\n",
      "epoch: 128, loss_G_total: 10.927, loss_G_l1: 1.696, loss_G_gan: 9.231, loss_D: 0.005\n",
      "epoch: 129, loss_G_total: 10.936, loss_G_l1: 1.694, loss_G_gan: 9.242, loss_D: 0.005\n",
      "epoch: 130, loss_G_total: 10.943, loss_G_l1: 1.692, loss_G_gan: 9.251, loss_D: 0.005\n",
      "epoch: 131, loss_G_total: 10.954, loss_G_l1: 1.694, loss_G_gan: 9.260, loss_D: 0.005\n",
      "epoch: 132, loss_G_total: 10.953, loss_G_l1: 1.685, loss_G_gan: 9.268, loss_D: 0.005\n",
      "epoch: 133, loss_G_total: 10.974, loss_G_l1: 1.698, loss_G_gan: 9.276, loss_D: 0.005\n",
      "epoch: 134, loss_G_total: 10.972, loss_G_l1: 1.687, loss_G_gan: 9.285, loss_D: 0.005\n",
      "epoch: 135, loss_G_total: 10.991, loss_G_l1: 1.697, loss_G_gan: 9.294, loss_D: 0.005\n",
      "epoch: 136, loss_G_total: 10.992, loss_G_l1: 1.689, loss_G_gan: 9.302, loss_D: 0.005\n",
      "epoch: 137, loss_G_total: 11.004, loss_G_l1: 1.695, loss_G_gan: 9.309, loss_D: 0.005\n",
      "epoch: 138, loss_G_total: 11.008, loss_G_l1: 1.690, loss_G_gan: 9.319, loss_D: 0.004\n",
      "epoch: 139, loss_G_total: 11.019, loss_G_l1: 1.691, loss_G_gan: 9.327, loss_D: 0.004\n",
      "epoch: 140, loss_G_total: 11.051, loss_G_l1: 1.715, loss_G_gan: 9.335, loss_D: 0.004\n",
      "epoch: 141, loss_G_total: 11.035, loss_G_l1: 1.689, loss_G_gan: 9.346, loss_D: 0.004\n",
      "epoch: 142, loss_G_total: 11.044, loss_G_l1: 1.690, loss_G_gan: 9.355, loss_D: 0.004\n",
      "epoch: 143, loss_G_total: 11.054, loss_G_l1: 1.691, loss_G_gan: 9.363, loss_D: 0.004\n",
      "epoch: 144, loss_G_total: 11.078, loss_G_l1: 1.707, loss_G_gan: 9.371, loss_D: 0.004\n",
      "epoch: 145, loss_G_total: 11.067, loss_G_l1: 1.689, loss_G_gan: 9.378, loss_D: 0.004\n",
      "epoch: 146, loss_G_total: 11.089, loss_G_l1: 1.703, loss_G_gan: 9.386, loss_D: 0.004\n",
      "epoch: 147, loss_G_total: 11.082, loss_G_l1: 1.688, loss_G_gan: 9.393, loss_D: 0.004\n",
      "epoch: 148, loss_G_total: 11.082, loss_G_l1: 1.680, loss_G_gan: 9.402, loss_D: 0.004\n",
      "epoch: 149, loss_G_total: 11.120, loss_G_l1: 1.709, loss_G_gan: 9.411, loss_D: 0.004\n",
      "epoch: 150, loss_G_total: 11.108, loss_G_l1: 1.689, loss_G_gan: 9.419, loss_D: 0.004\n",
      "epoch: 151, loss_G_total: 11.119, loss_G_l1: 1.693, loss_G_gan: 9.426, loss_D: 0.004\n",
      "epoch: 152, loss_G_total: 11.130, loss_G_l1: 1.697, loss_G_gan: 9.432, loss_D: 0.004\n",
      "epoch: 153, loss_G_total: 11.138, loss_G_l1: 1.700, loss_G_gan: 9.438, loss_D: 0.004\n",
      "epoch: 154, loss_G_total: 11.142, loss_G_l1: 1.694, loss_G_gan: 9.447, loss_D: 0.004\n",
      "epoch: 155, loss_G_total: 11.144, loss_G_l1: 1.688, loss_G_gan: 9.456, loss_D: 0.004\n",
      "epoch: 156, loss_G_total: 11.168, loss_G_l1: 1.703, loss_G_gan: 9.465, loss_D: 0.004\n",
      "epoch: 157, loss_G_total: 11.191, loss_G_l1: 1.718, loss_G_gan: 9.473, loss_D: 0.004\n",
      "epoch: 158, loss_G_total: 11.214, loss_G_l1: 1.734, loss_G_gan: 9.479, loss_D: 0.003\n",
      "epoch: 159, loss_G_total: 11.167, loss_G_l1: 1.680, loss_G_gan: 9.487, loss_D: 0.003\n",
      "epoch: 160, loss_G_total: 11.212, loss_G_l1: 1.719, loss_G_gan: 9.493, loss_D: 0.003\n",
      "epoch: 161, loss_G_total: 11.208, loss_G_l1: 1.705, loss_G_gan: 9.503, loss_D: 0.003\n",
      "epoch: 162, loss_G_total: 11.199, loss_G_l1: 1.688, loss_G_gan: 9.511, loss_D: 0.003\n",
      "epoch: 163, loss_G_total: 11.214, loss_G_l1: 1.695, loss_G_gan: 9.520, loss_D: 0.003\n",
      "epoch: 164, loss_G_total: 11.205, loss_G_l1: 1.679, loss_G_gan: 9.527, loss_D: 0.003\n",
      "epoch: 165, loss_G_total: 11.217, loss_G_l1: 1.683, loss_G_gan: 9.534, loss_D: 0.003\n",
      "epoch: 166, loss_G_total: 11.225, loss_G_l1: 1.683, loss_G_gan: 9.542, loss_D: 0.003\n",
      "epoch: 167, loss_G_total: 11.259, loss_G_l1: 1.712, loss_G_gan: 9.547, loss_D: 0.003\n",
      "epoch: 168, loss_G_total: 11.253, loss_G_l1: 1.698, loss_G_gan: 9.555, loss_D: 0.003\n",
      "epoch: 169, loss_G_total: 11.260, loss_G_l1: 1.698, loss_G_gan: 9.562, loss_D: 0.003\n",
      "epoch: 170, loss_G_total: 11.261, loss_G_l1: 1.691, loss_G_gan: 9.570, loss_D: 0.003\n",
      "epoch: 171, loss_G_total: 11.273, loss_G_l1: 1.698, loss_G_gan: 9.575, loss_D: 0.003\n",
      "epoch: 172, loss_G_total: 11.277, loss_G_l1: 1.695, loss_G_gan: 9.582, loss_D: 0.003\n",
      "epoch: 173, loss_G_total: 11.297, loss_G_l1: 1.708, loss_G_gan: 9.589, loss_D: 0.003\n",
      "epoch: 174, loss_G_total: 11.288, loss_G_l1: 1.691, loss_G_gan: 9.597, loss_D: 0.003\n",
      "epoch: 175, loss_G_total: 11.296, loss_G_l1: 1.691, loss_G_gan: 9.605, loss_D: 0.003\n",
      "epoch: 176, loss_G_total: 11.298, loss_G_l1: 1.686, loss_G_gan: 9.612, loss_D: 0.003\n",
      "epoch: 177, loss_G_total: 11.311, loss_G_l1: 1.693, loss_G_gan: 9.619, loss_D: 0.003\n",
      "epoch: 178, loss_G_total: 11.316, loss_G_l1: 1.690, loss_G_gan: 9.626, loss_D: 0.003\n",
      "epoch: 179, loss_G_total: 11.322, loss_G_l1: 1.689, loss_G_gan: 9.633, loss_D: 0.003\n",
      "epoch: 180, loss_G_total: 11.336, loss_G_l1: 1.695, loss_G_gan: 9.641, loss_D: 0.003\n",
      "epoch: 181, loss_G_total: 11.339, loss_G_l1: 1.692, loss_G_gan: 9.647, loss_D: 0.003\n",
      "epoch: 182, loss_G_total: 11.345, loss_G_l1: 1.692, loss_G_gan: 9.653, loss_D: 0.003\n",
      "epoch: 183, loss_G_total: 11.359, loss_G_l1: 1.699, loss_G_gan: 9.660, loss_D: 0.003\n",
      "epoch: 184, loss_G_total: 11.363, loss_G_l1: 1.696, loss_G_gan: 9.668, loss_D: 0.003\n",
      "epoch: 185, loss_G_total: 11.364, loss_G_l1: 1.690, loss_G_gan: 9.675, loss_D: 0.003\n",
      "epoch: 186, loss_G_total: 11.375, loss_G_l1: 1.693, loss_G_gan: 9.682, loss_D: 0.003\n",
      "epoch: 187, loss_G_total: 11.361, loss_G_l1: 1.673, loss_G_gan: 9.688, loss_D: 0.003\n",
      "epoch: 188, loss_G_total: 11.381, loss_G_l1: 1.688, loss_G_gan: 9.693, loss_D: 0.003\n",
      "epoch: 189, loss_G_total: 11.391, loss_G_l1: 1.690, loss_G_gan: 9.700, loss_D: 0.003\n",
      "epoch: 190, loss_G_total: 11.402, loss_G_l1: 1.696, loss_G_gan: 9.707, loss_D: 0.003\n",
      "epoch: 191, loss_G_total: 11.404, loss_G_l1: 1.690, loss_G_gan: 9.714, loss_D: 0.002\n",
      "epoch: 192, loss_G_total: 11.406, loss_G_l1: 1.686, loss_G_gan: 9.720, loss_D: 0.002\n",
      "epoch: 193, loss_G_total: 11.417, loss_G_l1: 1.691, loss_G_gan: 9.727, loss_D: 0.002\n",
      "epoch: 194, loss_G_total: 11.418, loss_G_l1: 1.685, loss_G_gan: 9.733, loss_D: 0.002\n",
      "epoch: 195, loss_G_total: 11.428, loss_G_l1: 1.689, loss_G_gan: 9.739, loss_D: 0.002\n",
      "epoch: 196, loss_G_total: 11.432, loss_G_l1: 1.687, loss_G_gan: 9.746, loss_D: 0.002\n",
      "epoch: 197, loss_G_total: 11.443, loss_G_l1: 1.691, loss_G_gan: 9.752, loss_D: 0.002\n",
      "epoch: 198, loss_G_total: 11.449, loss_G_l1: 1.691, loss_G_gan: 9.759, loss_D: 0.002\n",
      "epoch: 199, loss_G_total: 11.458, loss_G_l1: 1.692, loss_G_gan: 9.766, loss_D: 0.002\n",
      "epoch: 200, loss_G_total: 11.463, loss_G_l1: 1.690, loss_G_gan: 9.773, loss_D: 0.002\n"
     ]
    }
   ],
   "source": [
    "TGen = ToyGenerator()\n",
    "D = Discriminator()\n",
    "ganloss_G = nn.BCELoss()\n",
    "l1loss_G = nn.L1Loss()\n",
    "ganloss_D = nn.BCELoss()\n",
    "optimG = torch.optim.Adam(TGen.parameters(), lr = 0.1)\n",
    "optimD = torch.optim.Adam(D.parameters(), lr = 0.01)\n",
    "epochs = 200\n",
    "loss_G_l1_list = []\n",
    "loss_G_gan_list = []\n",
    "loss_D_list = []\n",
    "for epoch in range(epochs):\n",
    "    data_idx = 1#np.random.randint(len(td)) # get random sample\n",
    "    labels = td[data_idx]['lossmask'] # get lossmask (20, 32)\n",
    "    groundtruth = torch.cat((td[data_idx]['train'], td[data_idx]['groundtruth']), dim = 0)  # groundtruth (20, 32, 2)\n",
    "    \n",
    "    '''Train generator'''\n",
    "    loss_G = 0\n",
    "    optimG.zero_grad()\n",
    "    z = torch.randn(32, 32, 32)\n",
    "    prediction = TGen(z)\n",
    "    out_D_pred = D(prediction)\n",
    "    loss_G_l1 = l1loss_G(prediction[:,  0:td.peds_per_seq[data_idx]], groundtruth[:,  0:td.peds_per_seq[data_idx]]) # l1loss\n",
    "    loss_G_l1_list.append(loss_G_l1)\n",
    "    loss_G_gan = ganloss_G(out_D_pred.squeeze()[:, 0:td.peds_per_seq[data_idx]], labels[:, 0:td.peds_per_seq[data_idx]]) # ganloss\n",
    "    loss_G_gan_list.append(loss_G_gan)\n",
    "    loss_G = loss_G_l1 + loss_G_gan\n",
    "    loss_G.backward(retain_graph=True)\n",
    "    optimG.step()\n",
    "\n",
    "    '''Train discriminator'''\n",
    "    optimD.zero_grad()\n",
    "    loss_D = 0\n",
    "    out_D_real = D(seq.unsqueeze(0))\n",
    "    out_D_fake = D(prediction)\n",
    "    loss_D += ganloss_D(out_D_real.squeeze()[:, 0:td.peds_per_seq[data_idx]], labels[:, 0:td.peds_per_seq[data_idx]]) # on real\n",
    "    loss_D += ganloss_D(out_D_fake.squeeze()[:, 0:td.peds_per_seq[data_idx]], ((labels-1)*(-1))[:, 0:td.peds_per_seq[data_idx]]) # on fake\n",
    "    loss_D_list.append(loss_D)\n",
    "    loss_D.backward(retain_graph=True)\n",
    "    optimD.step()\n",
    "\n",
    "    print(\"epoch: %d, loss_G_total: %1.3f, loss_G_l1: %1.3f, loss_G_gan: %1.3f, loss_D: %1.3f\"%(epoch+1, loss_G, loss_G_l1, loss_G_gan, loss_D))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.randn(32, 32, 32)\n",
    "predicted = TGen(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[10.9467,  6.0442],\n",
       "         [ 9.7658,  6.3908]], grad_fn=<SliceBackward>),\n",
       " tensor([[12.5100,  6.1900],\n",
       "         [12.0900,  6.9500]]))"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted[0, 0:td.peds_per_seq[data_idx]], groundtruth[0,0:td.peds_per_seq[data_idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mapsgan)",
   "language": "python",
   "name": "mapsgan"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
