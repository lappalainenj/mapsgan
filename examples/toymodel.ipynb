{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from mapsgan import Trajectories, TrajectoryDataset\n",
    "import sys\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "td = Trajectories('/mnt/Clouds/MapsGAN/data/eth/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdsg = TrajectoryDataset('/mnt/Clouds/MapsGAN/data/eth/test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trajectory Toy Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyGenerator(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim = 48, hidden_dim = 20, batch_size = 32, out_len = 12):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim)\n",
    "        self.linear = nn.Sequential(nn.Linear(hidden_dim, out_len*2),\n",
    "                                    nn.ReLU())\n",
    "        self.out_len = out_len\n",
    "        self.hidden = self.init_hidden()\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        return (torch.zeros(1, self.batch_size, self.hidden_dim),\n",
    "                torch.zeros(1, self.batch_size, self.hidden_dim))\n",
    "        \n",
    "    def forward(self, z, social_feats):\n",
    "        x = torch.cat((social_feats, z), dim = 2)\n",
    "        out, self.hidden = self.lstm(x, self.hidden)\n",
    "        x = self.linear(self.hidden[0])\n",
    "        return x.view(self.out_len, self.batch_size, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToySocialEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, obs_len = 8, embedding_dim=16, hidden_dim=16, batch_size = 2):\n",
    "        super(ToySocialEncoder, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.obs_len = obs_len\n",
    "        \n",
    "        self.linear = nn.Sequential(nn.Linear(2, embedding_dim),\n",
    "                                    nn.ReLU())\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.hidden = self.init_hidden()\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        return (torch.zeros(1, self.batch_size, self.hidden_dim),\n",
    "                torch.zeros(1, self.batch_size, self.hidden_dim))\n",
    "        \n",
    "    def forward(self, input_seq):\n",
    "        hidden = self.init_hidden()\n",
    "        embedding = torch.zeros(self.obs_len, self.batch_size, self.embedding_dim)\n",
    "        for i, seq in enumerate(input_seq):\n",
    "            embedding[i] = self.linear(seq)\n",
    "        out, self.hidden = self.lstm(embedding, self.hidden)\n",
    "        return self.hidden[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SEplusG(nn.Module):\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.se = ToySocialEncoder(**kwargs['Encoder'])\n",
    "        self.g = ToyGenerator(**kwargs['Generator'])\n",
    "    \n",
    "    def forward(self, train, z):\n",
    "        x = self.se(train)\n",
    "        return self.g(z, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim=2, mlp_dim=16, hidden_dim=64, num_layers=1):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.mlp_dim = mlp_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.mlp_spatial_lstm = nn.Sequential(#nn.Linear(input_dim, mlp_dim),\n",
    "                                              #nn.ReLU(),\n",
    "                                              nn.LSTM(#mlp_dim,\n",
    "                                                      input_dim,\n",
    "                                                      hidden_dim,\n",
    "                                                      num_layers))\n",
    "        self.mlp_bool = nn.Sequential(nn.Linear(hidden_dim, 20),\n",
    "                                      nn.Sigmoid())\n",
    "        \n",
    "    def forward(self, input_seq):\n",
    "        _, (hidden, _) = self.mlp_spatial_lstm(input_seq)\n",
    "        return self.mlp_bool(hidden.squeeze()).view(20, self.input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 32, 16])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tse = ToySocialEncoder()\n",
    "tse(td[0]['train']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 32, 2])"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SeG = SEplusG()\n",
    "z = torch.randn(8, 32, 32)\n",
    "SeG(td[0]['train'], z).shape\n",
    "#torch.cat((td[0]['train'], z), dim = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 32, 2])"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TGen = ToyGenerator()\n",
    "z = torch.randn(32, 32, 32)\n",
    "TGen(z, social_feats).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[7.0500, 8.4400],\n",
       "         [8.0900, 8.8400]],\n",
       "\n",
       "        [[6.9300, 8.2600],\n",
       "         [8.0900, 8.8400]],\n",
       "\n",
       "        [[6.8700, 8.0900],\n",
       "         [8.0900, 8.8400]],\n",
       "\n",
       "        [[7.0200, 7.9100],\n",
       "         [8.0900, 8.8400]],\n",
       "\n",
       "        [[7.0700, 7.7800],\n",
       "         [8.0900, 8.8400]],\n",
       "\n",
       "        [[7.1000, 7.8200],\n",
       "         [8.0900, 8.8400]],\n",
       "\n",
       "        [[6.7700, 8.0900],\n",
       "         [8.0900, 8.8400]],\n",
       "\n",
       "        [[6.7700, 8.0900],\n",
       "         [8.0700, 8.8400]]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td[16]['train'][:, :2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 0: Sizes of tensors must match except in dimension 2. Got 1 and 8 in dimension 0 at /opt/conda/conda-bld/pytorch-cpu_1532578932944/work/aten/src/TH/generic/THTensorMath.cpp:3616",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-9b0f812d255e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mloss_G\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0moptimG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mestimate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSeG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# prediction is of length 12\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mout_D_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mloss_G_l1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml1loss_G\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# l1loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mapsgan/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-5fe61e1daf88>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, train, z)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/mapsgan/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-51cf9ffd619b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, z, social_feats)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocial_feats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msocial_feats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 0: Sizes of tensors must match except in dimension 2. Got 1 and 8 in dimension 0 at /opt/conda/conda-bld/pytorch-cpu_1532578932944/work/aten/src/TH/generic/THTensorMath.cpp:3616"
     ]
    }
   ],
   "source": [
    "# 8 to 12, embedding, only 2 agents\n",
    "kwargs = {'Encoder':dict(obs_len = 8, embedding_dim=16, hidden_dim=16, batch_size = 2),\n",
    "          'Generator':dict(input_dim = 18, hidden_dim = 20, batch_size = 2, out_len = 12)}\n",
    "SeG = SEplusG(**kwargs)\n",
    "D = Discriminator()\n",
    "ganloss_G = nn.BCELoss()\n",
    "l1loss_G = nn.L1Loss()\n",
    "lambda_G = 2\n",
    "ganloss_D = nn.BCELoss()\n",
    "optimG = torch.optim.Adam(SeG.parameters(), lr = 0.01)\n",
    "optimD = torch.optim.Adam(D.parameters(), lr = 0.01)\n",
    "epochs = 300\n",
    "loss_G_l1_list = []\n",
    "loss_G_gan_list = []\n",
    "loss_D_list = []\n",
    "#hidden = SeG.g.init_hidden()\n",
    "for epoch in range(epochs):\n",
    "    data_idx = 0#np.random.choice([0, 25, 38, 59, 42])#epoch%len(td)#np.random.randint(len(td))#np.random.choice([0, 25, 38, 59, 42])#np.random.randint(5) # len(td)) # get random sample\n",
    "    labels = td[data_idx]['lossmask'][:, :2] # get lossmask (20, 32)\n",
    "    train = td[data_idx]['train'][:, :2]\n",
    "    prediction = td[data_idx]['groundtruth'][:, :2]\n",
    "    groundtruth = torch.cat((train, prediction), dim = 0)  # groundtruth (20, 32, 2)\n",
    "    z = torch.randn(8, 2, 2)\n",
    "    \n",
    "    '''Train generator'''\n",
    "    loss_G = 0\n",
    "    optimG.zero_grad()\n",
    "    estimate = SeG(train, z) # prediction is of length 12\n",
    "    out_D_pred = D(torch.cat((train, estimate), dim = 0))\n",
    "    loss_G_l1 = l1loss_G(estimate, prediction) # l1loss\n",
    "    loss_G_l1_list.append(loss_G_l1)\n",
    "    loss_G_gan = ganloss_G(out_D_pred, labels) # ganloss\n",
    "    loss_G_gan_list.append(loss_G_gan)\n",
    "    loss_G = lambda_G*loss_G_l1 + loss_G_gan\n",
    "    loss_G.backward(retain_graph = True)\n",
    "    optimG.step()\n",
    "\n",
    "    '''Train discriminator'''\n",
    "    optimD.zero_grad()\n",
    "    loss_D = 0\n",
    "    out_D_real = D(groundtruth)\n",
    "    out_D_fake = D(torch.cat((train, estimate), dim = 0))\n",
    "    loss_D += ganloss_D(out_D_real, labels) # on real\n",
    "    loss_D += ganloss_D(out_D_fake, ((labels-1)*(-1))) # on fake\n",
    "    loss_D_list.append(loss_D)\n",
    "    loss_D.backward(retain_graph = True)\n",
    "    optimD.step()\n",
    "\n",
    "    print(\"epoch: %d, loss_G_total: %1.3f, loss_G_l1: %1.3f, loss_G_gan: %1.3f, loss_D: %1.3f\"%(epoch+1, loss_G, loss_G_l1, loss_G_gan, loss_D))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[10.3100,  5.9700],\n",
       "         [ 9.5700,  6.2400],\n",
       "         [ 8.7300,  6.3400],\n",
       "         [ 7.9400,  6.5000],\n",
       "         [ 7.1700,  6.6200],\n",
       "         [ 6.4700,  6.6800],\n",
       "         [ 5.8600,  6.8200],\n",
       "         [ 5.2400,  6.9800],\n",
       "         [ 4.8700,  7.1600],\n",
       "         [ 4.5100,  7.5800],\n",
       "         [ 4.2000,  7.3000],\n",
       "         [ 3.9500,  7.7100]]), tensor([[0.0000, 6.0941],\n",
       "         [9.5477, 6.9327],\n",
       "         [8.6940, 7.1441],\n",
       "         [4.9717, 0.0000],\n",
       "         [7.1611, 6.6788],\n",
       "         [0.0000, 6.7028],\n",
       "         [0.0000, 6.0941],\n",
       "         [9.5478, 6.9328],\n",
       "         [8.6941, 7.1441],\n",
       "         [4.9718, 0.0000],\n",
       "         [7.1612, 6.6789],\n",
       "         [0.0000, 6.7028]], grad_fn=<SelectBackward>))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction[:, 0], estimate[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, loss_G_total: 7.813, loss_G_l1: 7.121, loss_G_gan: 0.692, loss_D: 1.385\n",
      "epoch: 2, loss_G_total: 7.966, loss_G_l1: 7.273, loss_G_gan: 0.692, loss_D: 1.367\n",
      "epoch: 3, loss_G_total: 8.662, loss_G_l1: 7.964, loss_G_gan: 0.698, loss_D: 1.360\n",
      "epoch: 4, loss_G_total: 8.627, loss_G_l1: 7.927, loss_G_gan: 0.700, loss_D: 1.348\n",
      "epoch: 5, loss_G_total: 8.606, loss_G_l1: 7.901, loss_G_gan: 0.705, loss_D: 1.336\n",
      "epoch: 6, loss_G_total: 8.567, loss_G_l1: 7.855, loss_G_gan: 0.712, loss_D: 1.325\n",
      "epoch: 7, loss_G_total: 8.497, loss_G_l1: 7.776, loss_G_gan: 0.721, loss_D: 1.316\n",
      "epoch: 8, loss_G_total: 8.386, loss_G_l1: 7.646, loss_G_gan: 0.740, loss_D: 1.301\n",
      "epoch: 9, loss_G_total: 8.284, loss_G_l1: 7.512, loss_G_gan: 0.772, loss_D: 1.279\n",
      "epoch: 10, loss_G_total: 8.193, loss_G_l1: 7.356, loss_G_gan: 0.837, loss_D: 1.235\n",
      "epoch: 11, loss_G_total: 8.115, loss_G_l1: 7.140, loss_G_gan: 0.974, loss_D: 1.161\n",
      "epoch: 12, loss_G_total: 8.202, loss_G_l1: 6.936, loss_G_gan: 1.266, loss_D: 1.022\n",
      "epoch: 13, loss_G_total: 8.248, loss_G_l1: 6.726, loss_G_gan: 1.523, loss_D: 0.960\n",
      "epoch: 14, loss_G_total: 8.335, loss_G_l1: 6.464, loss_G_gan: 1.871, loss_D: 0.806\n",
      "epoch: 15, loss_G_total: 8.484, loss_G_l1: 6.259, loss_G_gan: 2.225, loss_D: 0.725\n",
      "epoch: 16, loss_G_total: 8.741, loss_G_l1: 6.056, loss_G_gan: 2.686, loss_D: 0.656\n",
      "epoch: 17, loss_G_total: 8.887, loss_G_l1: 5.801, loss_G_gan: 3.087, loss_D: 0.599\n",
      "epoch: 18, loss_G_total: 9.011, loss_G_l1: 5.549, loss_G_gan: 3.463, loss_D: 0.547\n",
      "epoch: 19, loss_G_total: 9.108, loss_G_l1: 5.323, loss_G_gan: 3.785, loss_D: 0.502\n",
      "epoch: 20, loss_G_total: 9.024, loss_G_l1: 5.076, loss_G_gan: 3.947, loss_D: 0.453\n",
      "epoch: 21, loss_G_total: 9.048, loss_G_l1: 4.853, loss_G_gan: 4.195, loss_D: 0.412\n",
      "epoch: 22, loss_G_total: 9.284, loss_G_l1: 4.604, loss_G_gan: 4.680, loss_D: 0.378\n",
      "epoch: 23, loss_G_total: 8.382, loss_G_l1: 3.271, loss_G_gan: 5.111, loss_D: 0.936\n",
      "epoch: 24, loss_G_total: 8.074, loss_G_l1: 2.693, loss_G_gan: 5.381, loss_D: 0.469\n",
      "epoch: 25, loss_G_total: 6.550, loss_G_l1: 2.953, loss_G_gan: 3.596, loss_D: 0.705\n",
      "epoch: 26, loss_G_total: 7.213, loss_G_l1: 3.675, loss_G_gan: 3.539, loss_D: 0.722\n",
      "epoch: 27, loss_G_total: 7.566, loss_G_l1: 2.639, loss_G_gan: 4.927, loss_D: 0.547\n",
      "epoch: 28, loss_G_total: 8.148, loss_G_l1: 3.114, loss_G_gan: 5.034, loss_D: 0.853\n",
      "epoch: 29, loss_G_total: 7.829, loss_G_l1: 2.967, loss_G_gan: 4.863, loss_D: 0.797\n",
      "epoch: 30, loss_G_total: 7.281, loss_G_l1: 2.854, loss_G_gan: 4.427, loss_D: 0.749\n",
      "epoch: 31, loss_G_total: 6.871, loss_G_l1: 2.799, loss_G_gan: 4.072, loss_D: 0.630\n",
      "epoch: 32, loss_G_total: 6.080, loss_G_l1: 2.274, loss_G_gan: 3.806, loss_D: 0.505\n",
      "epoch: 33, loss_G_total: 6.183, loss_G_l1: 2.464, loss_G_gan: 3.720, loss_D: 0.477\n",
      "epoch: 34, loss_G_total: 6.532, loss_G_l1: 2.675, loss_G_gan: 3.857, loss_D: 0.444\n",
      "epoch: 35, loss_G_total: 7.022, loss_G_l1: 2.929, loss_G_gan: 4.092, loss_D: 0.422\n",
      "epoch: 36, loss_G_total: 7.432, loss_G_l1: 3.173, loss_G_gan: 4.258, loss_D: 0.415\n",
      "epoch: 37, loss_G_total: 7.765, loss_G_l1: 3.373, loss_G_gan: 4.392, loss_D: 0.402\n",
      "epoch: 38, loss_G_total: 8.125, loss_G_l1: 3.567, loss_G_gan: 4.558, loss_D: 0.379\n",
      "epoch: 39, loss_G_total: 8.452, loss_G_l1: 3.721, loss_G_gan: 4.732, loss_D: 0.359\n",
      "epoch: 40, loss_G_total: 7.064, loss_G_l1: 2.242, loss_G_gan: 4.822, loss_D: 0.488\n",
      "epoch: 41, loss_G_total: 7.570, loss_G_l1: 3.775, loss_G_gan: 3.795, loss_D: 0.550\n",
      "epoch: 42, loss_G_total: 7.843, loss_G_l1: 3.971, loss_G_gan: 3.872, loss_D: 0.525\n",
      "epoch: 43, loss_G_total: 7.834, loss_G_l1: 3.857, loss_G_gan: 3.978, loss_D: 0.504\n",
      "epoch: 44, loss_G_total: 8.210, loss_G_l1: 4.182, loss_G_gan: 4.027, loss_D: 0.469\n",
      "epoch: 45, loss_G_total: 7.062, loss_G_l1: 3.060, loss_G_gan: 4.002, loss_D: 1.084\n",
      "epoch: 46, loss_G_total: 7.009, loss_G_l1: 3.035, loss_G_gan: 3.974, loss_D: 1.035\n",
      "epoch: 47, loss_G_total: 6.985, loss_G_l1: 2.404, loss_G_gan: 4.581, loss_D: 0.492\n",
      "epoch: 48, loss_G_total: 6.990, loss_G_l1: 2.411, loss_G_gan: 4.579, loss_D: 0.448\n",
      "epoch: 49, loss_G_total: 7.013, loss_G_l1: 2.415, loss_G_gan: 4.598, loss_D: 0.434\n",
      "epoch: 50, loss_G_total: 8.909, loss_G_l1: 4.140, loss_G_gan: 4.769, loss_D: 0.642\n",
      "epoch: 51, loss_G_total: 9.452, loss_G_l1: 4.451, loss_G_gan: 5.001, loss_D: 0.601\n",
      "epoch: 52, loss_G_total: 8.970, loss_G_l1: 4.311, loss_G_gan: 4.659, loss_D: 0.406\n",
      "epoch: 53, loss_G_total: 9.109, loss_G_l1: 5.022, loss_G_gan: 4.087, loss_D: 0.386\n",
      "epoch: 54, loss_G_total: 8.764, loss_G_l1: 4.720, loss_G_gan: 4.045, loss_D: 0.360\n",
      "epoch: 55, loss_G_total: 9.380, loss_G_l1: 5.167, loss_G_gan: 4.213, loss_D: 0.309\n",
      "epoch: 56, loss_G_total: 9.283, loss_G_l1: 4.890, loss_G_gan: 4.393, loss_D: 0.301\n",
      "epoch: 57, loss_G_total: 9.130, loss_G_l1: 4.601, loss_G_gan: 4.529, loss_D: 0.297\n",
      "epoch: 58, loss_G_total: 8.989, loss_G_l1: 4.303, loss_G_gan: 4.686, loss_D: 0.294\n",
      "epoch: 59, loss_G_total: 8.774, loss_G_l1: 3.999, loss_G_gan: 4.774, loss_D: 0.292\n",
      "epoch: 60, loss_G_total: 8.449, loss_G_l1: 3.678, loss_G_gan: 4.770, loss_D: 0.292\n",
      "epoch: 61, loss_G_total: 8.088, loss_G_l1: 3.347, loss_G_gan: 4.741, loss_D: 0.293\n",
      "epoch: 62, loss_G_total: 7.755, loss_G_l1: 3.034, loss_G_gan: 4.721, loss_D: 0.292\n",
      "epoch: 63, loss_G_total: 7.434, loss_G_l1: 2.726, loss_G_gan: 4.708, loss_D: 0.290\n",
      "epoch: 64, loss_G_total: 7.098, loss_G_l1: 2.458, loss_G_gan: 4.640, loss_D: 0.290\n",
      "epoch: 65, loss_G_total: 6.553, loss_G_l1: 2.239, loss_G_gan: 4.314, loss_D: 0.292\n",
      "epoch: 66, loss_G_total: 5.282, loss_G_l1: 2.026, loss_G_gan: 3.256, loss_D: 0.514\n",
      "epoch: 67, loss_G_total: 6.387, loss_G_l1: 1.796, loss_G_gan: 4.592, loss_D: 0.424\n",
      "epoch: 68, loss_G_total: 5.979, loss_G_l1: 1.575, loss_G_gan: 4.404, loss_D: 0.425\n",
      "epoch: 69, loss_G_total: 5.586, loss_G_l1: 1.370, loss_G_gan: 4.217, loss_D: 0.426\n",
      "epoch: 70, loss_G_total: 5.244, loss_G_l1: 1.233, loss_G_gan: 4.011, loss_D: 0.569\n",
      "epoch: 71, loss_G_total: 6.356, loss_G_l1: 2.018, loss_G_gan: 4.338, loss_D: 0.423\n",
      "epoch: 72, loss_G_total: 7.407, loss_G_l1: 2.830, loss_G_gan: 4.577, loss_D: 0.424\n",
      "epoch: 73, loss_G_total: 8.467, loss_G_l1: 4.035, loss_G_gan: 4.432, loss_D: 0.451\n",
      "epoch: 74, loss_G_total: 7.667, loss_G_l1: 4.078, loss_G_gan: 3.589, loss_D: 0.455\n",
      "epoch: 75, loss_G_total: 7.012, loss_G_l1: 4.093, loss_G_gan: 2.919, loss_D: 1.715\n",
      "epoch: 76, loss_G_total: 7.038, loss_G_l1: 4.091, loss_G_gan: 2.947, loss_D: 1.718\n",
      "epoch: 77, loss_G_total: 7.045, loss_G_l1: 4.072, loss_G_gan: 2.973, loss_D: 1.660\n",
      "epoch: 78, loss_G_total: 7.027, loss_G_l1: 4.037, loss_G_gan: 2.990, loss_D: 1.575\n",
      "epoch: 79, loss_G_total: 6.984, loss_G_l1: 3.991, loss_G_gan: 2.993, loss_D: 1.476\n",
      "epoch: 80, loss_G_total: 6.920, loss_G_l1: 3.934, loss_G_gan: 2.986, loss_D: 1.368\n",
      "epoch: 81, loss_G_total: 6.855, loss_G_l1: 3.878, loss_G_gan: 2.977, loss_D: 1.257\n",
      "epoch: 82, loss_G_total: 6.802, loss_G_l1: 3.831, loss_G_gan: 2.971, loss_D: 1.145\n",
      "epoch: 83, loss_G_total: 6.764, loss_G_l1: 3.793, loss_G_gan: 2.972, loss_D: 1.036\n",
      "epoch: 84, loss_G_total: 6.730, loss_G_l1: 3.749, loss_G_gan: 2.981, loss_D: 0.933\n",
      "epoch: 85, loss_G_total: 6.698, loss_G_l1: 3.695, loss_G_gan: 3.003, loss_D: 0.839\n",
      "epoch: 86, loss_G_total: 5.451, loss_G_l1: 2.133, loss_G_gan: 3.318, loss_D: 0.788\n",
      "epoch: 87, loss_G_total: 5.360, loss_G_l1: 2.018, loss_G_gan: 3.342, loss_D: 0.723\n",
      "epoch: 88, loss_G_total: 5.268, loss_G_l1: 1.888, loss_G_gan: 3.380, loss_D: 0.682\n",
      "epoch: 89, loss_G_total: 5.206, loss_G_l1: 1.753, loss_G_gan: 3.453, loss_D: 0.665\n",
      "epoch: 90, loss_G_total: 5.077, loss_G_l1: 1.609, loss_G_gan: 3.468, loss_D: 0.654\n",
      "epoch: 91, loss_G_total: 4.936, loss_G_l1: 1.475, loss_G_gan: 3.461, loss_D: 0.659\n",
      "epoch: 92, loss_G_total: 4.788, loss_G_l1: 1.400, loss_G_gan: 3.388, loss_D: 0.674\n",
      "epoch: 93, loss_G_total: 7.377, loss_G_l1: 3.777, loss_G_gan: 3.599, loss_D: 0.591\n",
      "epoch: 94, loss_G_total: 6.353, loss_G_l1: 2.809, loss_G_gan: 3.545, loss_D: 0.756\n",
      "epoch: 95, loss_G_total: 7.216, loss_G_l1: 3.820, loss_G_gan: 3.396, loss_D: 0.807\n",
      "epoch: 96, loss_G_total: 7.596, loss_G_l1: 4.490, loss_G_gan: 3.106, loss_D: 0.950\n",
      "epoch: 97, loss_G_total: 7.208, loss_G_l1: 3.662, loss_G_gan: 3.546, loss_D: 0.817\n",
      "epoch: 98, loss_G_total: 7.980, loss_G_l1: 4.195, loss_G_gan: 3.785, loss_D: 0.647\n",
      "epoch: 99, loss_G_total: 8.001, loss_G_l1: 4.159, loss_G_gan: 3.842, loss_D: 0.707\n",
      "epoch: 100, loss_G_total: 7.948, loss_G_l1: 4.141, loss_G_gan: 3.807, loss_D: 0.823\n",
      "epoch: 101, loss_G_total: 7.815, loss_G_l1: 4.167, loss_G_gan: 3.648, loss_D: 0.666\n",
      "epoch: 102, loss_G_total: 7.811, loss_G_l1: 4.087, loss_G_gan: 3.725, loss_D: 0.684\n",
      "epoch: 103, loss_G_total: 7.690, loss_G_l1: 4.082, loss_G_gan: 3.608, loss_D: 0.651\n",
      "epoch: 104, loss_G_total: 7.649, loss_G_l1: 4.145, loss_G_gan: 3.504, loss_D: 0.636\n",
      "epoch: 105, loss_G_total: 7.629, loss_G_l1: 4.232, loss_G_gan: 3.397, loss_D: 0.626\n",
      "epoch: 106, loss_G_total: 7.642, loss_G_l1: 4.332, loss_G_gan: 3.310, loss_D: 0.611\n",
      "epoch: 107, loss_G_total: 7.706, loss_G_l1: 4.455, loss_G_gan: 3.251, loss_D: 0.594\n",
      "epoch: 108, loss_G_total: 7.783, loss_G_l1: 4.554, loss_G_gan: 3.229, loss_D: 0.583\n",
      "epoch: 109, loss_G_total: 7.891, loss_G_l1: 4.650, loss_G_gan: 3.240, loss_D: 0.569\n",
      "epoch: 110, loss_G_total: 7.008, loss_G_l1: 3.555, loss_G_gan: 3.453, loss_D: 0.752\n",
      "epoch: 111, loss_G_total: 7.488, loss_G_l1: 4.736, loss_G_gan: 2.752, loss_D: 0.655\n",
      "epoch: 112, loss_G_total: 7.766, loss_G_l1: 5.011, loss_G_gan: 2.755, loss_D: 0.639\n",
      "epoch: 113, loss_G_total: 7.787, loss_G_l1: 5.007, loss_G_gan: 2.780, loss_D: 0.631\n",
      "epoch: 114, loss_G_total: 8.041, loss_G_l1: 5.224, loss_G_gan: 2.817, loss_D: 0.608\n",
      "epoch: 115, loss_G_total: 5.773, loss_G_l1: 2.969, loss_G_gan: 2.804, loss_D: 1.160\n",
      "epoch: 116, loss_G_total: 5.663, loss_G_l1: 2.938, loss_G_gan: 2.725, loss_D: 1.113\n",
      "epoch: 117, loss_G_total: 5.789, loss_G_l1: 2.733, loss_G_gan: 3.056, loss_D: 0.564\n",
      "epoch: 118, loss_G_total: 5.771, loss_G_l1: 2.783, loss_G_gan: 2.988, loss_D: 0.544\n",
      "epoch: 119, loss_G_total: 5.798, loss_G_l1: 2.864, loss_G_gan: 2.934, loss_D: 0.510\n",
      "epoch: 120, loss_G_total: 7.151, loss_G_l1: 3.959, loss_G_gan: 3.192, loss_D: 0.403\n",
      "epoch: 121, loss_G_total: 8.170, loss_G_l1: 4.785, loss_G_gan: 3.385, loss_D: 0.382\n",
      "epoch: 122, loss_G_total: 7.937, loss_G_l1: 4.595, loss_G_gan: 3.342, loss_D: 0.377\n",
      "epoch: 123, loss_G_total: 8.687, loss_G_l1: 5.300, loss_G_gan: 3.387, loss_D: 0.372\n",
      "epoch: 124, loss_G_total: 8.515, loss_G_l1: 5.039, loss_G_gan: 3.475, loss_D: 0.363\n",
      "epoch: 125, loss_G_total: 8.500, loss_G_l1: 4.781, loss_G_gan: 3.719, loss_D: 0.346\n",
      "epoch: 126, loss_G_total: 8.275, loss_G_l1: 4.485, loss_G_gan: 3.790, loss_D: 0.339\n",
      "epoch: 127, loss_G_total: 8.066, loss_G_l1: 4.212, loss_G_gan: 3.855, loss_D: 0.334\n",
      "epoch: 128, loss_G_total: 7.853, loss_G_l1: 3.939, loss_G_gan: 3.914, loss_D: 0.329\n",
      "epoch: 129, loss_G_total: 7.626, loss_G_l1: 3.662, loss_G_gan: 3.964, loss_D: 0.325\n",
      "epoch: 130, loss_G_total: 7.367, loss_G_l1: 3.364, loss_G_gan: 4.003, loss_D: 0.323\n",
      "epoch: 131, loss_G_total: 7.076, loss_G_l1: 3.052, loss_G_gan: 4.023, loss_D: 0.324\n",
      "epoch: 132, loss_G_total: 6.767, loss_G_l1: 2.761, loss_G_gan: 4.006, loss_D: 0.342\n",
      "epoch: 133, loss_G_total: 6.329, loss_G_l1: 2.458, loss_G_gan: 3.871, loss_D: 0.339\n",
      "epoch: 134, loss_G_total: 5.472, loss_G_l1: 2.148, loss_G_gan: 3.324, loss_D: 0.395\n",
      "epoch: 135, loss_G_total: 4.560, loss_G_l1: 1.895, loss_G_gan: 2.665, loss_D: 0.484\n",
      "epoch: 136, loss_G_total: 4.296, loss_G_l1: 1.690, loss_G_gan: 2.606, loss_D: 0.483\n",
      "epoch: 137, loss_G_total: 4.305, loss_G_l1: 1.541, loss_G_gan: 2.764, loss_D: 0.466\n",
      "epoch: 138, loss_G_total: 3.982, loss_G_l1: 1.495, loss_G_gan: 2.487, loss_D: 0.687\n",
      "epoch: 139, loss_G_total: 4.092, loss_G_l1: 1.519, loss_G_gan: 2.573, loss_D: 0.921\n",
      "epoch: 140, loss_G_total: 4.239, loss_G_l1: 1.609, loss_G_gan: 2.631, loss_D: 0.928\n",
      "epoch: 141, loss_G_total: 5.717, loss_G_l1: 2.771, loss_G_gan: 2.946, loss_D: 0.903\n",
      "epoch: 142, loss_G_total: 6.413, loss_G_l1: 3.395, loss_G_gan: 3.018, loss_D: 0.857\n",
      "epoch: 143, loss_G_total: 7.987, loss_G_l1: 5.195, loss_G_gan: 2.792, loss_D: 0.811\n",
      "epoch: 144, loss_G_total: 7.303, loss_G_l1: 5.336, loss_G_gan: 1.967, loss_D: 0.784\n",
      "epoch: 145, loss_G_total: 8.558, loss_G_l1: 5.437, loss_G_gan: 3.121, loss_D: 0.721\n",
      "epoch: 146, loss_G_total: 8.573, loss_G_l1: 5.523, loss_G_gan: 3.050, loss_D: 0.695\n",
      "epoch: 147, loss_G_total: 8.555, loss_G_l1: 5.584, loss_G_gan: 2.971, loss_D: 0.677\n",
      "epoch: 148, loss_G_total: 8.543, loss_G_l1: 5.627, loss_G_gan: 2.917, loss_D: 0.669\n",
      "epoch: 149, loss_G_total: 8.563, loss_G_l1: 5.656, loss_G_gan: 2.907, loss_D: 0.667\n",
      "epoch: 150, loss_G_total: 8.612, loss_G_l1: 5.673, loss_G_gan: 2.939, loss_D: 0.667\n",
      "epoch: 151, loss_G_total: 8.657, loss_G_l1: 5.683, loss_G_gan: 2.974, loss_D: 0.665\n",
      "epoch: 152, loss_G_total: 8.700, loss_G_l1: 5.684, loss_G_gan: 3.016, loss_D: 0.662\n",
      "epoch: 153, loss_G_total: 8.741, loss_G_l1: 5.674, loss_G_gan: 3.067, loss_D: 0.662\n",
      "epoch: 154, loss_G_total: 8.794, loss_G_l1: 5.655, loss_G_gan: 3.139, loss_D: 0.662\n",
      "epoch: 155, loss_G_total: 8.841, loss_G_l1: 5.630, loss_G_gan: 3.212, loss_D: 0.661\n",
      "epoch: 156, loss_G_total: 7.863, loss_G_l1: 4.437, loss_G_gan: 3.426, loss_D: 0.655\n",
      "epoch: 157, loss_G_total: 7.992, loss_G_l1: 4.388, loss_G_gan: 3.604, loss_D: 0.651\n",
      "epoch: 158, loss_G_total: 8.094, loss_G_l1: 4.327, loss_G_gan: 3.768, loss_D: 0.676\n",
      "epoch: 159, loss_G_total: 6.156, loss_G_l1: 4.270, loss_G_gan: 1.887, loss_D: 0.715\n",
      "epoch: 160, loss_G_total: 7.696, loss_G_l1: 4.246, loss_G_gan: 3.449, loss_D: 3.554\n",
      "epoch: 161, loss_G_total: 4.888, loss_G_l1: 4.215, loss_G_gan: 0.673, loss_D: 2.148\n",
      "epoch: 162, loss_G_total: 6.310, loss_G_l1: 4.199, loss_G_gan: 2.112, loss_D: 2.665\n",
      "epoch: 163, loss_G_total: 6.679, loss_G_l1: 4.460, loss_G_gan: 2.219, loss_D: 2.651\n",
      "epoch: 164, loss_G_total: 6.843, loss_G_l1: 5.171, loss_G_gan: 1.672, loss_D: 2.107\n",
      "epoch: 165, loss_G_total: 7.116, loss_G_l1: 5.637, loss_G_gan: 1.479, loss_D: 1.923\n",
      "epoch: 166, loss_G_total: 6.902, loss_G_l1: 5.763, loss_G_gan: 1.139, loss_D: 1.600\n",
      "epoch: 167, loss_G_total: 6.130, loss_G_l1: 5.267, loss_G_gan: 0.863, loss_D: 1.448\n",
      "epoch: 168, loss_G_total: 5.872, loss_G_l1: 4.900, loss_G_gan: 0.973, loss_D: 1.412\n",
      "epoch: 169, loss_G_total: 5.705, loss_G_l1: 4.824, loss_G_gan: 0.881, loss_D: 1.401\n",
      "epoch: 170, loss_G_total: 5.683, loss_G_l1: 4.816, loss_G_gan: 0.867, loss_D: 1.388\n",
      "epoch: 171, loss_G_total: 5.745, loss_G_l1: 4.857, loss_G_gan: 0.887, loss_D: 1.356\n",
      "epoch: 172, loss_G_total: 5.349, loss_G_l1: 4.648, loss_G_gan: 0.701, loss_D: 1.488\n",
      "epoch: 173, loss_G_total: 5.504, loss_G_l1: 4.746, loss_G_gan: 0.758, loss_D: 1.458\n",
      "epoch: 174, loss_G_total: 5.710, loss_G_l1: 4.906, loss_G_gan: 0.803, loss_D: 1.350\n",
      "epoch: 175, loss_G_total: 5.951, loss_G_l1: 5.081, loss_G_gan: 0.870, loss_D: 1.292\n",
      "epoch: 176, loss_G_total: 6.142, loss_G_l1: 5.228, loss_G_gan: 0.914, loss_D: 1.243\n",
      "epoch: 177, loss_G_total: 6.387, loss_G_l1: 5.412, loss_G_gan: 0.975, loss_D: 1.216\n",
      "epoch: 178, loss_G_total: 6.634, loss_G_l1: 5.583, loss_G_gan: 1.051, loss_D: 1.205\n",
      "epoch: 179, loss_G_total: 6.820, loss_G_l1: 5.709, loss_G_gan: 1.110, loss_D: 1.204\n",
      "epoch: 180, loss_G_total: 5.294, loss_G_l1: 4.171, loss_G_gan: 1.123, loss_D: 1.245\n",
      "epoch: 181, loss_G_total: 6.373, loss_G_l1: 5.198, loss_G_gan: 1.175, loss_D: 1.323\n",
      "epoch: 182, loss_G_total: 6.657, loss_G_l1: 5.458, loss_G_gan: 1.199, loss_D: 1.311\n",
      "epoch: 183, loss_G_total: 6.626, loss_G_l1: 5.420, loss_G_gan: 1.206, loss_D: 1.281\n",
      "epoch: 184, loss_G_total: 6.814, loss_G_l1: 5.623, loss_G_gan: 1.192, loss_D: 1.256\n",
      "epoch: 185, loss_G_total: 4.774, loss_G_l1: 3.652, loss_G_gan: 1.122, loss_D: 1.481\n",
      "epoch: 186, loss_G_total: 4.955, loss_G_l1: 3.872, loss_G_gan: 1.083, loss_D: 1.435\n",
      "epoch: 187, loss_G_total: 5.185, loss_G_l1: 4.039, loss_G_gan: 1.146, loss_D: 1.238\n",
      "epoch: 188, loss_G_total: 5.408, loss_G_l1: 4.296, loss_G_gan: 1.111, loss_D: 1.219\n",
      "epoch: 189, loss_G_total: 5.689, loss_G_l1: 4.611, loss_G_gan: 1.078, loss_D: 1.204\n",
      "epoch: 190, loss_G_total: 4.712, loss_G_l1: 3.572, loss_G_gan: 1.140, loss_D: 1.220\n",
      "epoch: 191, loss_G_total: 4.763, loss_G_l1: 3.650, loss_G_gan: 1.113, loss_D: 1.238\n",
      "epoch: 192, loss_G_total: 5.914, loss_G_l1: 4.785, loss_G_gan: 1.129, loss_D: 1.200\n",
      "epoch: 193, loss_G_total: 6.915, loss_G_l1: 5.807, loss_G_gan: 1.108, loss_D: 1.144\n",
      "epoch: 194, loss_G_total: 6.680, loss_G_l1: 5.591, loss_G_gan: 1.088, loss_D: 1.151\n",
      "epoch: 195, loss_G_total: 6.253, loss_G_l1: 5.162, loss_G_gan: 1.091, loss_D: 1.123\n",
      "epoch: 196, loss_G_total: 6.054, loss_G_l1: 4.921, loss_G_gan: 1.133, loss_D: 1.098\n",
      "epoch: 197, loss_G_total: 5.875, loss_G_l1: 4.678, loss_G_gan: 1.197, loss_D: 1.086\n",
      "epoch: 198, loss_G_total: 5.700, loss_G_l1: 4.442, loss_G_gan: 1.259, loss_D: 1.081\n",
      "epoch: 199, loss_G_total: 5.524, loss_G_l1: 4.211, loss_G_gan: 1.312, loss_D: 1.082\n",
      "epoch: 200, loss_G_total: 5.351, loss_G_l1: 3.987, loss_G_gan: 1.364, loss_D: 1.091\n",
      "epoch: 201, loss_G_total: 5.168, loss_G_l1: 3.770, loss_G_gan: 1.399, loss_D: 1.106\n",
      "epoch: 202, loss_G_total: 4.981, loss_G_l1: 3.568, loss_G_gan: 1.413, loss_D: 1.149\n",
      "epoch: 203, loss_G_total: 4.655, loss_G_l1: 3.367, loss_G_gan: 1.288, loss_D: 1.234\n",
      "epoch: 204, loss_G_total: 4.481, loss_G_l1: 3.179, loss_G_gan: 1.301, loss_D: 1.218\n",
      "epoch: 205, loss_G_total: 4.314, loss_G_l1: 3.012, loss_G_gan: 1.302, loss_D: 1.207\n",
      "epoch: 206, loss_G_total: 4.152, loss_G_l1: 2.870, loss_G_gan: 1.282, loss_D: 1.169\n",
      "epoch: 207, loss_G_total: 4.059, loss_G_l1: 2.807, loss_G_gan: 1.252, loss_D: 1.155\n",
      "epoch: 208, loss_G_total: 4.000, loss_G_l1: 2.808, loss_G_gan: 1.192, loss_D: 1.159\n",
      "epoch: 209, loss_G_total: 4.040, loss_G_l1: 2.893, loss_G_gan: 1.147, loss_D: 1.159\n",
      "epoch: 210, loss_G_total: 4.118, loss_G_l1: 3.021, loss_G_gan: 1.097, loss_D: 1.162\n",
      "epoch: 211, loss_G_total: 5.787, loss_G_l1: 4.760, loss_G_gan: 1.027, loss_D: 1.188\n",
      "epoch: 212, loss_G_total: 6.237, loss_G_l1: 5.260, loss_G_gan: 0.977, loss_D: 1.189\n",
      "epoch: 213, loss_G_total: 7.323, loss_G_l1: 6.311, loss_G_gan: 1.012, loss_D: 1.238\n",
      "epoch: 214, loss_G_total: 7.334, loss_G_l1: 6.330, loss_G_gan: 1.004, loss_D: 1.234\n",
      "epoch: 215, loss_G_total: 7.354, loss_G_l1: 6.346, loss_G_gan: 1.008, loss_D: 1.226\n",
      "epoch: 216, loss_G_total: 7.383, loss_G_l1: 6.358, loss_G_gan: 1.025, loss_D: 1.213\n",
      "epoch: 217, loss_G_total: 7.403, loss_G_l1: 6.358, loss_G_gan: 1.045, loss_D: 1.198\n",
      "epoch: 218, loss_G_total: 7.415, loss_G_l1: 6.347, loss_G_gan: 1.068, loss_D: 1.183\n",
      "epoch: 219, loss_G_total: 7.426, loss_G_l1: 6.331, loss_G_gan: 1.095, loss_D: 1.171\n",
      "epoch: 220, loss_G_total: 7.435, loss_G_l1: 6.310, loss_G_gan: 1.125, loss_D: 1.162\n",
      "epoch: 221, loss_G_total: 7.443, loss_G_l1: 6.289, loss_G_gan: 1.154, loss_D: 1.156\n",
      "epoch: 222, loss_G_total: 7.445, loss_G_l1: 6.264, loss_G_gan: 1.181, loss_D: 1.153\n",
      "epoch: 223, loss_G_total: 7.438, loss_G_l1: 6.232, loss_G_gan: 1.206, loss_D: 1.151\n",
      "epoch: 224, loss_G_total: 7.423, loss_G_l1: 6.196, loss_G_gan: 1.227, loss_D: 1.149\n",
      "epoch: 225, loss_G_total: 7.401, loss_G_l1: 6.157, loss_G_gan: 1.243, loss_D: 1.145\n",
      "epoch: 226, loss_G_total: 6.496, loss_G_l1: 5.234, loss_G_gan: 1.262, loss_D: 1.137\n",
      "epoch: 227, loss_G_total: 6.484, loss_G_l1: 5.198, loss_G_gan: 1.286, loss_D: 1.133\n",
      "epoch: 228, loss_G_total: 6.464, loss_G_l1: 5.151, loss_G_gan: 1.313, loss_D: 1.129\n",
      "epoch: 229, loss_G_total: 6.463, loss_G_l1: 5.122, loss_G_gan: 1.341, loss_D: 1.125\n",
      "epoch: 230, loss_G_total: 6.466, loss_G_l1: 5.098, loss_G_gan: 1.368, loss_D: 1.121\n",
      "epoch: 231, loss_G_total: 6.451, loss_G_l1: 5.061, loss_G_gan: 1.390, loss_D: 1.118\n",
      "epoch: 232, loss_G_total: 6.445, loss_G_l1: 5.036, loss_G_gan: 1.409, loss_D: 1.115\n",
      "epoch: 233, loss_G_total: 5.038, loss_G_l1: 3.721, loss_G_gan: 1.318, loss_D: 1.238\n",
      "epoch: 234, loss_G_total: 7.008, loss_G_l1: 5.565, loss_G_gan: 1.443, loss_D: 1.121\n",
      "epoch: 235, loss_G_total: 7.197, loss_G_l1: 5.783, loss_G_gan: 1.414, loss_D: 1.131\n",
      "epoch: 236, loss_G_total: 7.187, loss_G_l1: 5.779, loss_G_gan: 1.407, loss_D: 1.210\n",
      "epoch: 237, loss_G_total: 6.897, loss_G_l1: 5.462, loss_G_gan: 1.435, loss_D: 1.126\n",
      "epoch: 238, loss_G_total: 5.774, loss_G_l1: 4.470, loss_G_gan: 1.304, loss_D: 1.254\n",
      "epoch: 239, loss_G_total: 5.797, loss_G_l1: 4.486, loss_G_gan: 1.311, loss_D: 1.238\n",
      "epoch: 240, loss_G_total: 5.883, loss_G_l1: 4.560, loss_G_gan: 1.322, loss_D: 1.191\n",
      "epoch: 241, loss_G_total: 6.013, loss_G_l1: 4.673, loss_G_gan: 1.339, loss_D: 1.156\n",
      "epoch: 242, loss_G_total: 5.932, loss_G_l1: 4.407, loss_G_gan: 1.525, loss_D: 1.095\n",
      "epoch: 243, loss_G_total: 6.184, loss_G_l1: 4.651, loss_G_gan: 1.532, loss_D: 1.094\n",
      "epoch: 244, loss_G_total: 6.463, loss_G_l1: 4.926, loss_G_gan: 1.537, loss_D: 1.092\n",
      "epoch: 245, loss_G_total: 6.727, loss_G_l1: 5.188, loss_G_gan: 1.539, loss_D: 1.091\n",
      "epoch: 246, loss_G_total: 6.991, loss_G_l1: 5.457, loss_G_gan: 1.534, loss_D: 1.090\n",
      "epoch: 247, loss_G_total: 7.263, loss_G_l1: 5.733, loss_G_gan: 1.529, loss_D: 1.086\n",
      "epoch: 248, loss_G_total: 7.502, loss_G_l1: 5.975, loss_G_gan: 1.527, loss_D: 1.085\n",
      "epoch: 249, loss_G_total: 7.699, loss_G_l1: 6.172, loss_G_gan: 1.527, loss_D: 1.084\n",
      "epoch: 250, loss_G_total: 5.131, loss_G_l1: 3.642, loss_G_gan: 1.488, loss_D: 1.107\n",
      "epoch: 251, loss_G_total: 6.637, loss_G_l1: 5.202, loss_G_gan: 1.435, loss_D: 1.105\n",
      "epoch: 252, loss_G_total: 6.837, loss_G_l1: 5.417, loss_G_gan: 1.421, loss_D: 1.118\n",
      "epoch: 253, loss_G_total: 6.760, loss_G_l1: 5.344, loss_G_gan: 1.417, loss_D: 1.118\n",
      "epoch: 254, loss_G_total: 6.913, loss_G_l1: 5.505, loss_G_gan: 1.408, loss_D: 1.109\n",
      "epoch: 255, loss_G_total: 4.869, loss_G_l1: 3.566, loss_G_gan: 1.302, loss_D: 1.238\n",
      "epoch: 256, loss_G_total: 5.147, loss_G_l1: 3.818, loss_G_gan: 1.329, loss_D: 1.216\n",
      "epoch: 257, loss_G_total: 5.463, loss_G_l1: 4.027, loss_G_gan: 1.437, loss_D: 1.106\n",
      "epoch: 258, loss_G_total: 5.810, loss_G_l1: 4.367, loss_G_gan: 1.444, loss_D: 1.102\n",
      "epoch: 259, loss_G_total: 6.153, loss_G_l1: 4.703, loss_G_gan: 1.450, loss_D: 1.100\n",
      "epoch: 260, loss_G_total: 4.789, loss_G_l1: 3.250, loss_G_gan: 1.539, loss_D: 1.136\n",
      "epoch: 261, loss_G_total: 4.771, loss_G_l1: 3.208, loss_G_gan: 1.563, loss_D: 1.170\n",
      "epoch: 262, loss_G_total: 5.932, loss_G_l1: 4.354, loss_G_gan: 1.578, loss_D: 1.117\n",
      "epoch: 263, loss_G_total: 7.200, loss_G_l1: 5.691, loss_G_gan: 1.510, loss_D: 1.110\n",
      "epoch: 264, loss_G_total: 7.002, loss_G_l1: 5.485, loss_G_gan: 1.517, loss_D: 1.100\n",
      "epoch: 265, loss_G_total: 6.547, loss_G_l1: 5.017, loss_G_gan: 1.531, loss_D: 1.090\n",
      "epoch: 266, loss_G_total: 6.331, loss_G_l1: 4.790, loss_G_gan: 1.541, loss_D: 1.085\n",
      "epoch: 267, loss_G_total: 6.118, loss_G_l1: 4.561, loss_G_gan: 1.557, loss_D: 1.080\n",
      "epoch: 268, loss_G_total: 5.915, loss_G_l1: 4.341, loss_G_gan: 1.575, loss_D: 1.075\n",
      "epoch: 269, loss_G_total: 5.710, loss_G_l1: 4.125, loss_G_gan: 1.585, loss_D: 1.071\n",
      "epoch: 270, loss_G_total: 5.513, loss_G_l1: 3.915, loss_G_gan: 1.598, loss_D: 1.080\n",
      "epoch: 271, loss_G_total: 5.271, loss_G_l1: 3.711, loss_G_gan: 1.560, loss_D: 1.161\n",
      "epoch: 272, loss_G_total: 4.953, loss_G_l1: 3.522, loss_G_gan: 1.431, loss_D: 1.176\n",
      "epoch: 273, loss_G_total: 4.777, loss_G_l1: 3.331, loss_G_gan: 1.446, loss_D: 1.170\n",
      "epoch: 274, loss_G_total: 4.620, loss_G_l1: 3.154, loss_G_gan: 1.466, loss_D: 1.159\n",
      "epoch: 275, loss_G_total: 4.481, loss_G_l1: 2.996, loss_G_gan: 1.485, loss_D: 1.150\n",
      "epoch: 276, loss_G_total: 4.364, loss_G_l1: 2.878, loss_G_gan: 1.487, loss_D: 1.143\n",
      "epoch: 277, loss_G_total: 4.316, loss_G_l1: 2.838, loss_G_gan: 1.479, loss_D: 1.136\n",
      "epoch: 278, loss_G_total: 4.322, loss_G_l1: 2.857, loss_G_gan: 1.465, loss_D: 1.129\n",
      "epoch: 279, loss_G_total: 4.392, loss_G_l1: 2.950, loss_G_gan: 1.442, loss_D: 1.123\n",
      "epoch: 280, loss_G_total: 4.471, loss_G_l1: 3.066, loss_G_gan: 1.405, loss_D: 1.119\n",
      "epoch: 281, loss_G_total: 6.147, loss_G_l1: 4.786, loss_G_gan: 1.361, loss_D: 1.119\n",
      "epoch: 282, loss_G_total: 6.595, loss_G_l1: 5.285, loss_G_gan: 1.309, loss_D: 1.117\n",
      "epoch: 283, loss_G_total: 7.793, loss_G_l1: 6.426, loss_G_gan: 1.367, loss_D: 1.129\n",
      "epoch: 284, loss_G_total: 7.785, loss_G_l1: 6.438, loss_G_gan: 1.347, loss_D: 1.129\n",
      "epoch: 285, loss_G_total: 7.777, loss_G_l1: 6.445, loss_G_gan: 1.332, loss_D: 1.129\n",
      "epoch: 286, loss_G_total: 7.771, loss_G_l1: 6.449, loss_G_gan: 1.322, loss_D: 1.127\n",
      "epoch: 287, loss_G_total: 7.758, loss_G_l1: 6.441, loss_G_gan: 1.318, loss_D: 1.125\n",
      "epoch: 288, loss_G_total: 7.740, loss_G_l1: 6.421, loss_G_gan: 1.318, loss_D: 1.123\n",
      "epoch: 289, loss_G_total: 7.720, loss_G_l1: 6.397, loss_G_gan: 1.323, loss_D: 1.121\n",
      "epoch: 290, loss_G_total: 7.697, loss_G_l1: 6.368, loss_G_gan: 1.329, loss_D: 1.119\n",
      "epoch: 291, loss_G_total: 7.676, loss_G_l1: 6.338, loss_G_gan: 1.337, loss_D: 1.117\n",
      "epoch: 292, loss_G_total: 7.652, loss_G_l1: 6.305, loss_G_gan: 1.347, loss_D: 1.116\n",
      "epoch: 293, loss_G_total: 7.623, loss_G_l1: 6.266, loss_G_gan: 1.357, loss_D: 1.115\n",
      "epoch: 294, loss_G_total: 7.592, loss_G_l1: 6.223, loss_G_gan: 1.369, loss_D: 1.114\n",
      "epoch: 295, loss_G_total: 7.559, loss_G_l1: 6.177, loss_G_gan: 1.382, loss_D: 1.112\n",
      "epoch: 296, loss_G_total: 6.679, loss_G_l1: 5.244, loss_G_gan: 1.434, loss_D: 1.108\n",
      "epoch: 297, loss_G_total: 6.681, loss_G_l1: 5.225, loss_G_gan: 1.455, loss_D: 1.106\n",
      "epoch: 298, loss_G_total: 6.669, loss_G_l1: 5.193, loss_G_gan: 1.476, loss_D: 1.105\n",
      "epoch: 299, loss_G_total: 6.650, loss_G_l1: 5.156, loss_G_gan: 1.494, loss_D: 1.104\n",
      "epoch: 300, loss_G_total: 6.623, loss_G_l1: 5.113, loss_G_gan: 1.510, loss_D: 1.103\n",
      "epoch: 301, loss_G_total: 6.592, loss_G_l1: 5.068, loss_G_gan: 1.524, loss_D: 1.101\n",
      "epoch: 302, loss_G_total: 6.569, loss_G_l1: 5.033, loss_G_gan: 1.536, loss_D: 1.100\n",
      "epoch: 303, loss_G_total: 5.238, loss_G_l1: 3.706, loss_G_gan: 1.532, loss_D: 1.076\n",
      "epoch: 304, loss_G_total: 7.085, loss_G_l1: 5.527, loss_G_gan: 1.558, loss_D: 1.110\n",
      "epoch: 305, loss_G_total: 7.237, loss_G_l1: 5.755, loss_G_gan: 1.482, loss_D: 1.117\n",
      "epoch: 306, loss_G_total: 7.227, loss_G_l1: 5.753, loss_G_gan: 1.473, loss_D: 1.113\n",
      "epoch: 307, loss_G_total: 6.916, loss_G_l1: 5.416, loss_G_gan: 1.500, loss_D: 1.115\n",
      "epoch: 308, loss_G_total: 5.948, loss_G_l1: 4.447, loss_G_gan: 1.501, loss_D: 1.089\n",
      "epoch: 309, loss_G_total: 5.961, loss_G_l1: 4.445, loss_G_gan: 1.515, loss_D: 1.100\n",
      "epoch: 310, loss_G_total: 6.041, loss_G_l1: 4.513, loss_G_gan: 1.528, loss_D: 1.097\n",
      "epoch: 311, loss_G_total: 6.163, loss_G_l1: 4.623, loss_G_gan: 1.540, loss_D: 1.092\n",
      "epoch: 312, loss_G_total: 5.933, loss_G_l1: 4.325, loss_G_gan: 1.609, loss_D: 1.083\n",
      "epoch: 313, loss_G_total: 6.145, loss_G_l1: 4.531, loss_G_gan: 1.614, loss_D: 1.080\n",
      "epoch: 314, loss_G_total: 6.424, loss_G_l1: 4.802, loss_G_gan: 1.622, loss_D: 1.077\n",
      "epoch: 315, loss_G_total: 6.692, loss_G_l1: 5.060, loss_G_gan: 1.632, loss_D: 1.075\n",
      "epoch: 316, loss_G_total: 6.940, loss_G_l1: 5.295, loss_G_gan: 1.645, loss_D: 1.073\n",
      "epoch: 317, loss_G_total: 7.214, loss_G_l1: 5.554, loss_G_gan: 1.660, loss_D: 1.072\n",
      "epoch: 318, loss_G_total: 7.465, loss_G_l1: 5.791, loss_G_gan: 1.674, loss_D: 1.071\n",
      "epoch: 319, loss_G_total: 7.670, loss_G_l1: 5.984, loss_G_gan: 1.686, loss_D: 1.073\n",
      "epoch: 320, loss_G_total: 5.297, loss_G_l1: 3.604, loss_G_gan: 1.693, loss_D: 1.101\n",
      "epoch: 321, loss_G_total: 6.802, loss_G_l1: 5.164, loss_G_gan: 1.638, loss_D: 1.095\n",
      "epoch: 322, loss_G_total: 7.022, loss_G_l1: 5.386, loss_G_gan: 1.636, loss_D: 1.102\n",
      "epoch: 323, loss_G_total: 6.966, loss_G_l1: 5.326, loss_G_gan: 1.640, loss_D: 1.101\n",
      "epoch: 324, loss_G_total: 7.130, loss_G_l1: 5.493, loss_G_gan: 1.636, loss_D: 1.097\n",
      "epoch: 325, loss_G_total: 5.089, loss_G_l1: 3.608, loss_G_gan: 1.481, loss_D: 1.227\n",
      "epoch: 326, loss_G_total: 5.419, loss_G_l1: 3.866, loss_G_gan: 1.553, loss_D: 1.185\n",
      "epoch: 327, loss_G_total: 5.704, loss_G_l1: 4.073, loss_G_gan: 1.631, loss_D: 1.101\n",
      "epoch: 328, loss_G_total: 6.035, loss_G_l1: 4.414, loss_G_gan: 1.622, loss_D: 1.098\n",
      "epoch: 329, loss_G_total: 6.373, loss_G_l1: 4.755, loss_G_gan: 1.618, loss_D: 1.097\n",
      "epoch: 330, loss_G_total: 4.990, loss_G_l1: 3.293, loss_G_gan: 1.697, loss_D: 1.361\n",
      "epoch: 331, loss_G_total: 4.909, loss_G_l1: 3.261, loss_G_gan: 1.648, loss_D: 1.129\n",
      "epoch: 332, loss_G_total: 6.111, loss_G_l1: 4.424, loss_G_gan: 1.687, loss_D: 1.087\n",
      "epoch: 333, loss_G_total: 7.436, loss_G_l1: 5.842, loss_G_gan: 1.593, loss_D: 1.083\n",
      "epoch: 334, loss_G_total: 7.219, loss_G_l1: 5.639, loss_G_gan: 1.580, loss_D: 1.080\n",
      "epoch: 335, loss_G_total: 6.874, loss_G_l1: 5.250, loss_G_gan: 1.624, loss_D: 1.075\n",
      "epoch: 336, loss_G_total: 6.649, loss_G_l1: 5.026, loss_G_gan: 1.624, loss_D: 1.074\n",
      "epoch: 337, loss_G_total: 6.433, loss_G_l1: 4.799, loss_G_gan: 1.634, loss_D: 1.072\n",
      "epoch: 338, loss_G_total: 6.237, loss_G_l1: 4.581, loss_G_gan: 1.656, loss_D: 1.067\n",
      "epoch: 339, loss_G_total: 6.059, loss_G_l1: 4.367, loss_G_gan: 1.692, loss_D: 1.061\n",
      "epoch: 340, loss_G_total: 5.906, loss_G_l1: 4.159, loss_G_gan: 1.747, loss_D: 1.056\n",
      "epoch: 341, loss_G_total: 5.771, loss_G_l1: 3.957, loss_G_gan: 1.815, loss_D: 1.174\n",
      "epoch: 342, loss_G_total: 5.586, loss_G_l1: 3.769, loss_G_gan: 1.817, loss_D: 1.174\n",
      "epoch: 343, loss_G_total: 5.403, loss_G_l1: 3.579, loss_G_gan: 1.823, loss_D: 1.173\n",
      "epoch: 344, loss_G_total: 5.214, loss_G_l1: 3.402, loss_G_gan: 1.812, loss_D: 1.169\n",
      "epoch: 345, loss_G_total: 5.035, loss_G_l1: 3.245, loss_G_gan: 1.790, loss_D: 1.162\n",
      "epoch: 346, loss_G_total: 4.862, loss_G_l1: 3.102, loss_G_gan: 1.761, loss_D: 1.153\n",
      "epoch: 347, loss_G_total: 4.719, loss_G_l1: 2.995, loss_G_gan: 1.724, loss_D: 1.143\n",
      "epoch: 348, loss_G_total: 4.641, loss_G_l1: 2.959, loss_G_gan: 1.682, loss_D: 1.134\n",
      "epoch: 349, loss_G_total: 4.626, loss_G_l1: 2.988, loss_G_gan: 1.637, loss_D: 1.125\n",
      "epoch: 350, loss_G_total: 4.652, loss_G_l1: 3.062, loss_G_gan: 1.590, loss_D: 1.118\n",
      "epoch: 351, loss_G_total: 6.168, loss_G_l1: 4.620, loss_G_gan: 1.548, loss_D: 1.117\n",
      "epoch: 352, loss_G_total: 6.579, loss_G_l1: 5.077, loss_G_gan: 1.503, loss_D: 1.112\n",
      "epoch: 353, loss_G_total: 7.745, loss_G_l1: 6.256, loss_G_gan: 1.489, loss_D: 1.122\n",
      "epoch: 354, loss_G_total: 7.712, loss_G_l1: 6.254, loss_G_gan: 1.458, loss_D: 1.123\n",
      "epoch: 355, loss_G_total: 7.677, loss_G_l1: 6.249, loss_G_gan: 1.428, loss_D: 1.124\n",
      "epoch: 356, loss_G_total: 7.640, loss_G_l1: 6.239, loss_G_gan: 1.402, loss_D: 1.125\n",
      "epoch: 357, loss_G_total: 7.597, loss_G_l1: 6.216, loss_G_gan: 1.381, loss_D: 1.125\n",
      "epoch: 358, loss_G_total: 7.550, loss_G_l1: 6.183, loss_G_gan: 1.367, loss_D: 1.125\n",
      "epoch: 359, loss_G_total: 7.505, loss_G_l1: 6.144, loss_G_gan: 1.361, loss_D: 1.124\n",
      "epoch: 360, loss_G_total: 7.463, loss_G_l1: 6.101, loss_G_gan: 1.362, loss_D: 1.122\n",
      "epoch: 361, loss_G_total: 7.426, loss_G_l1: 6.058, loss_G_gan: 1.369, loss_D: 1.121\n",
      "epoch: 362, loss_G_total: 7.391, loss_G_l1: 6.011, loss_G_gan: 1.380, loss_D: 1.119\n",
      "epoch: 363, loss_G_total: 7.366, loss_G_l1: 5.972, loss_G_gan: 1.394, loss_D: 1.117\n",
      "epoch: 364, loss_G_total: 7.346, loss_G_l1: 5.935, loss_G_gan: 1.411, loss_D: 1.115\n",
      "epoch: 365, loss_G_total: 7.343, loss_G_l1: 5.914, loss_G_gan: 1.430, loss_D: 1.113\n",
      "epoch: 366, loss_G_total: 6.384, loss_G_l1: 4.889, loss_G_gan: 1.495, loss_D: 1.111\n",
      "epoch: 367, loss_G_total: 6.395, loss_G_l1: 4.876, loss_G_gan: 1.520, loss_D: 1.110\n",
      "epoch: 368, loss_G_total: 6.391, loss_G_l1: 4.849, loss_G_gan: 1.542, loss_D: 1.109\n",
      "epoch: 369, loss_G_total: 6.380, loss_G_l1: 4.817, loss_G_gan: 1.563, loss_D: 1.108\n",
      "epoch: 370, loss_G_total: 6.359, loss_G_l1: 4.778, loss_G_gan: 1.581, loss_D: 1.108\n",
      "epoch: 371, loss_G_total: 6.324, loss_G_l1: 4.728, loss_G_gan: 1.597, loss_D: 1.109\n",
      "epoch: 372, loss_G_total: 6.281, loss_G_l1: 4.671, loss_G_gan: 1.610, loss_D: 1.109\n",
      "epoch: 373, loss_G_total: 5.733, loss_G_l1: 4.040, loss_G_gan: 1.694, loss_D: 1.071\n",
      "epoch: 374, loss_G_total: 6.927, loss_G_l1: 5.289, loss_G_gan: 1.638, loss_D: 1.106\n",
      "epoch: 375, loss_G_total: 7.204, loss_G_l1: 5.639, loss_G_gan: 1.565, loss_D: 1.107\n",
      "epoch: 376, loss_G_total: 7.317, loss_G_l1: 5.702, loss_G_gan: 1.615, loss_D: 1.063\n",
      "epoch: 377, loss_G_total: 6.836, loss_G_l1: 5.254, loss_G_gan: 1.582, loss_D: 1.108\n",
      "epoch: 378, loss_G_total: 6.200, loss_G_l1: 4.554, loss_G_gan: 1.646, loss_D: 1.081\n",
      "epoch: 379, loss_G_total: 6.194, loss_G_l1: 4.538, loss_G_gan: 1.656, loss_D: 1.104\n",
      "epoch: 380, loss_G_total: 6.209, loss_G_l1: 4.546, loss_G_gan: 1.663, loss_D: 1.102\n",
      "epoch: 381, loss_G_total: 6.261, loss_G_l1: 4.589, loss_G_gan: 1.672, loss_D: 1.098\n",
      "epoch: 382, loss_G_total: 5.955, loss_G_l1: 4.265, loss_G_gan: 1.690, loss_D: 1.095\n",
      "epoch: 383, loss_G_total: 6.141, loss_G_l1: 4.450, loss_G_gan: 1.690, loss_D: 1.090\n",
      "epoch: 384, loss_G_total: 6.315, loss_G_l1: 4.626, loss_G_gan: 1.689, loss_D: 1.085\n",
      "epoch: 385, loss_G_total: 6.509, loss_G_l1: 4.821, loss_G_gan: 1.688, loss_D: 1.081\n",
      "epoch: 386, loss_G_total: 6.741, loss_G_l1: 5.054, loss_G_gan: 1.687, loss_D: 1.078\n",
      "epoch: 387, loss_G_total: 6.937, loss_G_l1: 5.249, loss_G_gan: 1.688, loss_D: 1.076\n",
      "epoch: 388, loss_G_total: 7.104, loss_G_l1: 5.414, loss_G_gan: 1.690, loss_D: 1.075\n",
      "epoch: 389, loss_G_total: 7.309, loss_G_l1: 5.615, loss_G_gan: 1.694, loss_D: 1.074\n",
      "epoch: 390, loss_G_total: 5.331, loss_G_l1: 3.639, loss_G_gan: 1.692, loss_D: 1.102\n",
      "epoch: 391, loss_G_total: 6.776, loss_G_l1: 5.119, loss_G_gan: 1.657, loss_D: 1.066\n",
      "epoch: 392, loss_G_total: 7.022, loss_G_l1: 5.369, loss_G_gan: 1.652, loss_D: 1.068\n",
      "epoch: 393, loss_G_total: 6.980, loss_G_l1: 5.327, loss_G_gan: 1.653, loss_D: 1.066\n",
      "epoch: 394, loss_G_total: 7.168, loss_G_l1: 5.519, loss_G_gan: 1.649, loss_D: 1.060\n",
      "epoch: 395, loss_G_total: 5.265, loss_G_l1: 3.668, loss_G_gan: 1.597, loss_D: 1.178\n",
      "epoch: 396, loss_G_total: 5.598, loss_G_l1: 3.935, loss_G_gan: 1.663, loss_D: 1.157\n",
      "epoch: 397, loss_G_total: 5.795, loss_G_l1: 4.187, loss_G_gan: 1.609, loss_D: 1.105\n",
      "epoch: 398, loss_G_total: 6.082, loss_G_l1: 4.482, loss_G_gan: 1.600, loss_D: 1.101\n",
      "epoch: 399, loss_G_total: 6.419, loss_G_l1: 4.824, loss_G_gan: 1.595, loss_D: 1.098\n",
      "epoch: 400, loss_G_total: 4.855, loss_G_l1: 3.329, loss_G_gan: 1.526, loss_D: 1.093\n",
      "epoch: 401, loss_G_total: 4.945, loss_G_l1: 3.308, loss_G_gan: 1.637, loss_D: 1.107\n",
      "epoch: 402, loss_G_total: 6.205, loss_G_l1: 4.463, loss_G_gan: 1.741, loss_D: 1.080\n",
      "epoch: 403, loss_G_total: 7.688, loss_G_l1: 6.008, loss_G_gan: 1.681, loss_D: 1.076\n",
      "epoch: 404, loss_G_total: 7.503, loss_G_l1: 5.803, loss_G_gan: 1.700, loss_D: 1.072\n",
      "epoch: 405, loss_G_total: 7.295, loss_G_l1: 5.496, loss_G_gan: 1.798, loss_D: 1.066\n",
      "epoch: 406, loss_G_total: 7.104, loss_G_l1: 5.273, loss_G_gan: 1.831, loss_D: 1.062\n",
      "epoch: 407, loss_G_total: 6.918, loss_G_l1: 5.049, loss_G_gan: 1.869, loss_D: 1.058\n",
      "epoch: 408, loss_G_total: 6.743, loss_G_l1: 4.834, loss_G_gan: 1.909, loss_D: 1.055\n",
      "epoch: 409, loss_G_total: 6.574, loss_G_l1: 4.624, loss_G_gan: 1.950, loss_D: 1.052\n",
      "epoch: 410, loss_G_total: 6.411, loss_G_l1: 4.421, loss_G_gan: 1.990, loss_D: 1.050\n",
      "epoch: 411, loss_G_total: 6.244, loss_G_l1: 4.224, loss_G_gan: 2.020, loss_D: 1.158\n",
      "epoch: 412, loss_G_total: 5.999, loss_G_l1: 4.042, loss_G_gan: 1.957, loss_D: 1.162\n",
      "epoch: 413, loss_G_total: 5.823, loss_G_l1: 3.859, loss_G_gan: 1.965, loss_D: 1.162\n",
      "epoch: 414, loss_G_total: 5.645, loss_G_l1: 3.688, loss_G_gan: 1.957, loss_D: 1.159\n",
      "epoch: 415, loss_G_total: 5.475, loss_G_l1: 3.537, loss_G_gan: 1.938, loss_D: 1.154\n",
      "epoch: 416, loss_G_total: 5.309, loss_G_l1: 3.400, loss_G_gan: 1.909, loss_D: 1.147\n",
      "epoch: 417, loss_G_total: 5.148, loss_G_l1: 3.277, loss_G_gan: 1.872, loss_D: 1.139\n",
      "epoch: 418, loss_G_total: 5.032, loss_G_l1: 3.202, loss_G_gan: 1.830, loss_D: 1.131\n",
      "epoch: 419, loss_G_total: 4.966, loss_G_l1: 3.181, loss_G_gan: 1.785, loss_D: 1.124\n",
      "epoch: 420, loss_G_total: 4.936, loss_G_l1: 3.196, loss_G_gan: 1.740, loss_D: 1.119\n",
      "epoch: 421, loss_G_total: 6.216, loss_G_l1: 4.511, loss_G_gan: 1.705, loss_D: 1.116\n",
      "epoch: 422, loss_G_total: 6.523, loss_G_l1: 4.857, loss_G_gan: 1.666, loss_D: 1.110\n",
      "epoch: 423, loss_G_total: 7.946, loss_G_l1: 6.280, loss_G_gan: 1.666, loss_D: 1.131\n",
      "epoch: 424, loss_G_total: 7.916, loss_G_l1: 6.274, loss_G_gan: 1.642, loss_D: 1.132\n",
      "epoch: 425, loss_G_total: 7.883, loss_G_l1: 6.264, loss_G_gan: 1.619, loss_D: 1.132\n",
      "epoch: 426, loss_G_total: 7.849, loss_G_l1: 6.252, loss_G_gan: 1.597, loss_D: 1.131\n",
      "epoch: 427, loss_G_total: 7.806, loss_G_l1: 6.228, loss_G_gan: 1.578, loss_D: 1.130\n",
      "epoch: 428, loss_G_total: 7.755, loss_G_l1: 6.193, loss_G_gan: 1.562, loss_D: 1.128\n",
      "epoch: 429, loss_G_total: 7.704, loss_G_l1: 6.155, loss_G_gan: 1.549, loss_D: 1.127\n",
      "epoch: 430, loss_G_total: 7.652, loss_G_l1: 6.113, loss_G_gan: 1.540, loss_D: 1.126\n",
      "epoch: 431, loss_G_total: 7.606, loss_G_l1: 6.071, loss_G_gan: 1.535, loss_D: 1.124\n",
      "epoch: 432, loss_G_total: 7.562, loss_G_l1: 6.027, loss_G_gan: 1.535, loss_D: 1.123\n",
      "epoch: 433, loss_G_total: 7.516, loss_G_l1: 5.977, loss_G_gan: 1.539, loss_D: 1.122\n",
      "epoch: 434, loss_G_total: 7.472, loss_G_l1: 5.925, loss_G_gan: 1.547, loss_D: 1.120\n",
      "epoch: 435, loss_G_total: 7.430, loss_G_l1: 5.872, loss_G_gan: 1.559, loss_D: 1.118\n",
      "epoch: 436, loss_G_total: 6.400, loss_G_l1: 4.773, loss_G_gan: 1.627, loss_D: 1.115\n",
      "epoch: 437, loss_G_total: 6.394, loss_G_l1: 4.745, loss_G_gan: 1.649, loss_D: 1.113\n",
      "epoch: 438, loss_G_total: 6.377, loss_G_l1: 4.706, loss_G_gan: 1.670, loss_D: 1.112\n",
      "epoch: 439, loss_G_total: 6.379, loss_G_l1: 4.688, loss_G_gan: 1.691, loss_D: 1.111\n",
      "epoch: 440, loss_G_total: 6.377, loss_G_l1: 4.667, loss_G_gan: 1.710, loss_D: 1.110\n",
      "epoch: 441, loss_G_total: 6.359, loss_G_l1: 4.633, loss_G_gan: 1.726, loss_D: 1.110\n",
      "epoch: 442, loss_G_total: 6.328, loss_G_l1: 4.589, loss_G_gan: 1.739, loss_D: 1.110\n",
      "epoch: 443, loss_G_total: 5.922, loss_G_l1: 4.101, loss_G_gan: 1.821, loss_D: 1.069\n",
      "epoch: 444, loss_G_total: 7.005, loss_G_l1: 5.244, loss_G_gan: 1.761, loss_D: 1.103\n",
      "epoch: 445, loss_G_total: 7.302, loss_G_l1: 5.631, loss_G_gan: 1.671, loss_D: 1.106\n",
      "epoch: 446, loss_G_total: 7.446, loss_G_l1: 5.703, loss_G_gan: 1.743, loss_D: 1.046\n",
      "epoch: 447, loss_G_total: 6.936, loss_G_l1: 5.250, loss_G_gan: 1.687, loss_D: 1.107\n",
      "epoch: 448, loss_G_total: 6.351, loss_G_l1: 4.596, loss_G_gan: 1.755, loss_D: 1.079\n",
      "epoch: 449, loss_G_total: 6.344, loss_G_l1: 4.577, loss_G_gan: 1.766, loss_D: 1.103\n",
      "epoch: 450, loss_G_total: 6.360, loss_G_l1: 4.587, loss_G_gan: 1.773, loss_D: 1.100\n",
      "epoch: 451, loss_G_total: 6.414, loss_G_l1: 4.630, loss_G_gan: 1.783, loss_D: 1.096\n",
      "epoch: 452, loss_G_total: 6.106, loss_G_l1: 4.306, loss_G_gan: 1.800, loss_D: 1.090\n",
      "epoch: 453, loss_G_total: 6.292, loss_G_l1: 4.494, loss_G_gan: 1.799, loss_D: 1.082\n",
      "epoch: 454, loss_G_total: 6.468, loss_G_l1: 4.671, loss_G_gan: 1.797, loss_D: 1.075\n",
      "epoch: 455, loss_G_total: 6.649, loss_G_l1: 4.853, loss_G_gan: 1.795, loss_D: 1.071\n",
      "epoch: 456, loss_G_total: 6.882, loss_G_l1: 5.087, loss_G_gan: 1.796, loss_D: 1.069\n",
      "epoch: 457, loss_G_total: 7.083, loss_G_l1: 5.284, loss_G_gan: 1.798, loss_D: 1.068\n",
      "epoch: 458, loss_G_total: 7.256, loss_G_l1: 5.453, loss_G_gan: 1.803, loss_D: 1.068\n",
      "epoch: 459, loss_G_total: 7.456, loss_G_l1: 5.647, loss_G_gan: 1.809, loss_D: 1.067\n",
      "epoch: 460, loss_G_total: 5.510, loss_G_l1: 3.697, loss_G_gan: 1.813, loss_D: 1.096\n",
      "epoch: 461, loss_G_total: 6.930, loss_G_l1: 5.118, loss_G_gan: 1.812, loss_D: 1.058\n",
      "epoch: 462, loss_G_total: 7.177, loss_G_l1: 5.367, loss_G_gan: 1.810, loss_D: 1.059\n",
      "epoch: 463, loss_G_total: 7.139, loss_G_l1: 5.330, loss_G_gan: 1.808, loss_D: 1.058\n",
      "epoch: 464, loss_G_total: 7.325, loss_G_l1: 5.524, loss_G_gan: 1.801, loss_D: 1.054\n",
      "epoch: 465, loss_G_total: 5.415, loss_G_l1: 3.651, loss_G_gan: 1.763, loss_D: 1.150\n",
      "epoch: 466, loss_G_total: 5.698, loss_G_l1: 3.914, loss_G_gan: 1.784, loss_D: 1.136\n",
      "epoch: 467, loss_G_total: 5.898, loss_G_l1: 4.151, loss_G_gan: 1.747, loss_D: 1.094\n",
      "epoch: 468, loss_G_total: 6.189, loss_G_l1: 4.449, loss_G_gan: 1.740, loss_D: 1.090\n",
      "epoch: 469, loss_G_total: 6.528, loss_G_l1: 4.791, loss_G_gan: 1.737, loss_D: 1.086\n",
      "epoch: 470, loss_G_total: 5.068, loss_G_l1: 3.349, loss_G_gan: 1.719, loss_D: 1.080\n",
      "epoch: 471, loss_G_total: 5.131, loss_G_l1: 3.334, loss_G_gan: 1.797, loss_D: 1.098\n",
      "epoch: 472, loss_G_total: 6.383, loss_G_l1: 4.485, loss_G_gan: 1.897, loss_D: 1.071\n",
      "epoch: 473, loss_G_total: 7.792, loss_G_l1: 5.954, loss_G_gan: 1.839, loss_D: 1.067\n",
      "epoch: 474, loss_G_total: 7.607, loss_G_l1: 5.747, loss_G_gan: 1.860, loss_D: 1.064\n",
      "epoch: 475, loss_G_total: 7.374, loss_G_l1: 5.408, loss_G_gan: 1.966, loss_D: 1.058\n",
      "epoch: 476, loss_G_total: 7.177, loss_G_l1: 5.180, loss_G_gan: 1.997, loss_D: 1.055\n",
      "epoch: 477, loss_G_total: 6.981, loss_G_l1: 4.952, loss_G_gan: 2.029, loss_D: 1.053\n",
      "epoch: 478, loss_G_total: 6.793, loss_G_l1: 4.731, loss_G_gan: 2.062, loss_D: 1.051\n",
      "epoch: 479, loss_G_total: 6.610, loss_G_l1: 4.517, loss_G_gan: 2.093, loss_D: 1.049\n",
      "epoch: 480, loss_G_total: 6.430, loss_G_l1: 4.308, loss_G_gan: 2.122, loss_D: 1.048\n",
      "epoch: 481, loss_G_total: 6.246, loss_G_l1: 4.105, loss_G_gan: 2.141, loss_D: 1.146\n",
      "epoch: 482, loss_G_total: 5.974, loss_G_l1: 3.917, loss_G_gan: 2.057, loss_D: 1.151\n",
      "epoch: 483, loss_G_total: 5.794, loss_G_l1: 3.728, loss_G_gan: 2.066, loss_D: 1.151\n",
      "epoch: 484, loss_G_total: 5.602, loss_G_l1: 3.551, loss_G_gan: 2.051, loss_D: 1.149\n",
      "epoch: 485, loss_G_total: 5.420, loss_G_l1: 3.394, loss_G_gan: 2.025, loss_D: 1.145\n",
      "epoch: 486, loss_G_total: 5.244, loss_G_l1: 3.253, loss_G_gan: 1.992, loss_D: 1.139\n",
      "epoch: 487, loss_G_total: 5.081, loss_G_l1: 3.129, loss_G_gan: 1.952, loss_D: 1.132\n",
      "epoch: 488, loss_G_total: 4.988, loss_G_l1: 3.081, loss_G_gan: 1.907, loss_D: 1.126\n",
      "epoch: 489, loss_G_total: 4.932, loss_G_l1: 3.073, loss_G_gan: 1.859, loss_D: 1.121\n",
      "epoch: 490, loss_G_total: 4.925, loss_G_l1: 3.115, loss_G_gan: 1.810, loss_D: 1.118\n",
      "epoch: 491, loss_G_total: 6.284, loss_G_l1: 4.512, loss_G_gan: 1.771, loss_D: 1.114\n",
      "epoch: 492, loss_G_total: 6.685, loss_G_l1: 4.959, loss_G_gan: 1.726, loss_D: 1.109\n",
      "epoch: 493, loss_G_total: 8.016, loss_G_l1: 6.292, loss_G_gan: 1.725, loss_D: 1.134\n",
      "epoch: 494, loss_G_total: 7.985, loss_G_l1: 6.289, loss_G_gan: 1.697, loss_D: 1.135\n",
      "epoch: 495, loss_G_total: 7.952, loss_G_l1: 6.281, loss_G_gan: 1.671, loss_D: 1.135\n",
      "epoch: 496, loss_G_total: 7.918, loss_G_l1: 6.270, loss_G_gan: 1.648, loss_D: 1.133\n",
      "epoch: 497, loss_G_total: 7.877, loss_G_l1: 6.250, loss_G_gan: 1.628, loss_D: 1.131\n",
      "epoch: 498, loss_G_total: 7.825, loss_G_l1: 6.215, loss_G_gan: 1.611, loss_D: 1.129\n",
      "epoch: 499, loss_G_total: 7.775, loss_G_l1: 6.178, loss_G_gan: 1.598, loss_D: 1.127\n",
      "epoch: 500, loss_G_total: 7.726, loss_G_l1: 6.137, loss_G_gan: 1.589, loss_D: 1.125\n"
     ]
    }
   ],
   "source": [
    "# 8 to 12, embedding, variable amount of agents\n",
    "SeG = SEplusG()\n",
    "D = Discriminator()\n",
    "ganloss_G = nn.BCELoss()\n",
    "l1loss_G = nn.L1Loss()\n",
    "lambda_G = 1\n",
    "ganloss_D = nn.BCELoss()\n",
    "optimG = torch.optim.Adam(SeG.parameters(), lr = 0.01)\n",
    "optimD = torch.optim.Adam(D.parameters(), lr = 0.01)\n",
    "epochs = 500\n",
    "loss_G_l1_list = []\n",
    "loss_G_gan_list = []\n",
    "loss_D_list = []\n",
    "#hidden = SeG.g.init_hidden()\n",
    "for epoch in range(epochs):\n",
    "    data_idx = epoch%len(td)#np.random.randint(len(td))#np.random.choice([0, 25, 38, 59, 42])#np.random.randint(5) # len(td)) # get random sample\n",
    "    labels = td[data_idx]['lossmask'] # get lossmask (20, 32)\n",
    "    train = td[data_idx]['train']\n",
    "    groundtruth = torch.cat((train, td[data_idx]['groundtruth']), dim = 0)  # groundtruth (20, 32, 2)\n",
    "    z = torch.randn(8, 32, 32)\n",
    "    \n",
    "    '''Train generator'''\n",
    "    loss_G = 0\n",
    "    optimG.zero_grad()\n",
    "    prediction = SeG(train, z) # prediction is of length 12\n",
    "    out_D_pred = D(torch.cat((train, prediction), dim = 0))\n",
    "    loss_G_l1 = l1loss_G(prediction[:,  0:td.peds_per_seq[data_idx]], groundtruth[8::,  0:td.peds_per_seq[data_idx]]) # l1loss\n",
    "    loss_G_l1_list.append(loss_G_l1)\n",
    "    loss_G_gan = ganloss_G(out_D_pred.squeeze()[:, 0:td.peds_per_seq[data_idx]], labels[:, 0:td.peds_per_seq[data_idx]]) # ganloss\n",
    "    loss_G_gan_list.append(loss_G_gan)\n",
    "    loss_G = lambda_G*loss_G_l1 + loss_G_gan\n",
    "    loss_G.backward(retain_graph = True)\n",
    "    optimG.step()\n",
    "\n",
    "    '''Train discriminator'''\n",
    "    optimD.zero_grad()\n",
    "    loss_D = 0\n",
    "    out_D_real = D(groundtruth)\n",
    "    out_D_fake = D(torch.cat((train, prediction), dim = 0))\n",
    "    loss_D += ganloss_D(out_D_real.squeeze()[:, 0:td.peds_per_seq[data_idx]], labels[:, 0:td.peds_per_seq[data_idx]]) # on real\n",
    "    loss_D += ganloss_D(out_D_fake.squeeze()[:, 0:td.peds_per_seq[data_idx]], ((labels-1)*(-1))[:, 0:td.peds_per_seq[data_idx]]) # on fake\n",
    "    loss_D_list.append(loss_D)\n",
    "    loss_D.backward()\n",
    "    optimD.step()\n",
    "\n",
    "    print(\"epoch: %d, loss_G_total: %1.3f, loss_G_l1: %1.3f, loss_G_gan: %1.3f, loss_D: %1.3f\"%(epoch+1, loss_G, loss_G_l1, loss_G_gan, loss_D))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.7231, 7.9609],\n",
       "        [3.3286, 8.0379],\n",
       "        [4.0818, 7.9306],\n",
       "        [2.7376, 8.0046],\n",
       "        [3.3288, 8.0389],\n",
       "        [4.0815, 7.9301],\n",
       "        [2.7379, 8.0050],\n",
       "        [3.3289, 8.0391],\n",
       "        [4.0815, 7.9300],\n",
       "        [2.7377, 8.0046],\n",
       "        [3.3290, 8.0390],\n",
       "        [4.0818, 7.9307]], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[2.7447, 8.0896],\n",
       "         [3.3645, 8.1810],\n",
       "         [4.1176, 8.0754],\n",
       "         [2.7769, 8.1577],\n",
       "         [3.3650, 8.1819],\n",
       "         [4.1173, 8.0751],\n",
       "         [2.7768, 8.1580],\n",
       "         [3.3648, 8.1813],\n",
       "         [4.1180, 8.0756],\n",
       "         [2.7770, 8.1580],\n",
       "         [3.3649, 8.1820],\n",
       "         [4.1179, 8.0760]], grad_fn=<SelectBackward>),\n",
       " tensor([[ 1.3400,  5.3400],\n",
       "         [ 2.2800,  5.2700],\n",
       "         [ 3.1800,  5.2000],\n",
       "         [ 4.1100,  5.1600],\n",
       "         [ 5.0300,  5.0900],\n",
       "         [ 6.0200,  5.0400],\n",
       "         [ 7.0100,  5.0300],\n",
       "         [ 8.0500,  4.9600],\n",
       "         [ 8.9400,  4.8900],\n",
       "         [ 9.8800,  4.8400],\n",
       "         [10.7400,  4.7500],\n",
       "         [11.7000,  4.6000]]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 32\n",
    "SeG(td[idx]['train'], torch.randn(8, 32, 32))[:, 1], td[idx]['groundtruth'][:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice([0, 25, 38, 59, 42])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[12.5000,  3.8300],\n",
       "         [10.7400,  7.2000],\n",
       "         [-0.1700,  6.3200],\n",
       "         [11.2300,  6.6500],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000]],\n",
       "\n",
       "        [[12.3700,  3.7400],\n",
       "         [ 9.7200,  7.1000],\n",
       "         [ 0.8200,  6.3100],\n",
       "         [10.0600,  6.7500],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000]],\n",
       "\n",
       "        [[12.2800,  3.6200],\n",
       "         [ 8.7300,  7.0900],\n",
       "         [ 1.7400,  6.3100],\n",
       "         [ 9.1800,  6.5800],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000]],\n",
       "\n",
       "        [[12.3700,  3.5600],\n",
       "         [ 7.7400,  6.9700],\n",
       "         [ 2.6400,  6.3600],\n",
       "         [ 8.1300,  6.4800],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000]],\n",
       "\n",
       "        [[12.5500,  3.5600],\n",
       "         [ 6.8600,  6.7500],\n",
       "         [ 3.4600,  6.6100],\n",
       "         [ 7.2100,  6.0500],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000]],\n",
       "\n",
       "        [[12.5900,  3.5800],\n",
       "         [ 6.0400,  6.4500],\n",
       "         [ 4.3300,  6.8200],\n",
       "         [ 6.2400,  5.7500],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000]],\n",
       "\n",
       "        [[12.5300,  3.6000],\n",
       "         [ 5.3700,  6.1500],\n",
       "         [ 5.1900,  6.9900],\n",
       "         [ 5.2400,  5.5500],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000]],\n",
       "\n",
       "        [[12.5000,  3.6000],\n",
       "         [ 4.4400,  6.0100],\n",
       "         [ 6.0900,  7.0800],\n",
       "         [ 4.3400,  5.3000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000]]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td[42]['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, loss_G_total: 15.258, loss_G_l1: 7.285, loss_G_gan: 0.688, loss_D: 1.386\n",
      "epoch: 2, loss_G_total: 16.645, loss_G_l1: 7.975, loss_G_gan: 0.694, loss_D: 1.370\n",
      "epoch: 3, loss_G_total: 16.623, loss_G_l1: 7.960, loss_G_gan: 0.703, loss_D: 1.353\n",
      "epoch: 4, loss_G_total: 16.574, loss_G_l1: 7.930, loss_G_gan: 0.715, loss_D: 1.337\n",
      "epoch: 5, loss_G_total: 15.776, loss_G_l1: 7.518, loss_G_gan: 0.740, loss_D: 1.322\n",
      "epoch: 6, loss_G_total: 15.204, loss_G_l1: 7.224, loss_G_gan: 0.755, loss_D: 1.314\n",
      "epoch: 7, loss_G_total: 16.558, loss_G_l1: 7.902, loss_G_gan: 0.754, loss_D: 1.307\n",
      "epoch: 8, loss_G_total: 15.167, loss_G_l1: 7.181, loss_G_gan: 0.806, loss_D: 1.285\n",
      "epoch: 9, loss_G_total: 16.551, loss_G_l1: 7.845, loss_G_gan: 0.862, loss_D: 1.256\n",
      "epoch: 10, loss_G_total: 16.577, loss_G_l1: 7.793, loss_G_gan: 0.992, loss_D: 1.129\n",
      "epoch: 11, loss_G_total: 15.306, loss_G_l1: 7.016, loss_G_gan: 1.274, loss_D: 0.962\n",
      "epoch: 12, loss_G_total: 16.701, loss_G_l1: 7.622, loss_G_gan: 1.456, loss_D: 0.879\n",
      "epoch: 13, loss_G_total: 16.834, loss_G_l1: 7.538, loss_G_gan: 1.759, loss_D: 0.777\n",
      "epoch: 14, loss_G_total: 16.861, loss_G_l1: 7.363, loss_G_gan: 2.134, loss_D: 0.687\n",
      "epoch: 15, loss_G_total: 15.695, loss_G_l1: 6.474, loss_G_gan: 2.746, loss_D: 0.591\n",
      "epoch: 16, loss_G_total: 15.693, loss_G_l1: 6.235, loss_G_gan: 3.223, loss_D: 0.528\n",
      "epoch: 17, loss_G_total: 15.802, loss_G_l1: 6.056, loss_G_gan: 3.691, loss_D: 0.475\n",
      "epoch: 18, loss_G_total: 17.131, loss_G_l1: 6.657, loss_G_gan: 3.817, loss_D: 0.455\n",
      "epoch: 19, loss_G_total: 17.073, loss_G_l1: 6.441, loss_G_gan: 4.191, loss_D: 0.417\n",
      "epoch: 20, loss_G_total: 17.016, loss_G_l1: 6.235, loss_G_gan: 4.546, loss_D: 0.387\n",
      "epoch: 21, loss_G_total: 16.845, loss_G_l1: 5.983, loss_G_gan: 4.879, loss_D: 0.363\n",
      "epoch: 22, loss_G_total: 15.836, loss_G_l1: 5.122, loss_G_gan: 5.592, loss_D: 0.329\n",
      "epoch: 23, loss_G_total: 15.515, loss_G_l1: 4.866, loss_G_gan: 5.783, loss_D: 0.318\n",
      "epoch: 24, loss_G_total: 16.344, loss_G_l1: 5.347, loss_G_gan: 5.651, loss_D: 0.318\n",
      "epoch: 25, loss_G_total: 14.909, loss_G_l1: 4.325, loss_G_gan: 6.259, loss_D: 0.299\n",
      "epoch: 26, loss_G_total: 15.813, loss_G_l1: 4.898, loss_G_gan: 6.018, loss_D: 0.298\n",
      "epoch: 27, loss_G_total: 15.593, loss_G_l1: 4.716, loss_G_gan: 6.161, loss_D: 0.291\n",
      "epoch: 28, loss_G_total: 13.852, loss_G_l1: 3.567, loss_G_gan: 6.717, loss_D: 0.286\n",
      "epoch: 29, loss_G_total: 14.726, loss_G_l1: 4.286, loss_G_gan: 6.154, loss_D: 0.289\n",
      "epoch: 30, loss_G_total: 13.043, loss_G_l1: 3.119, loss_G_gan: 6.806, loss_D: 0.286\n",
      "epoch: 31, loss_G_total: 12.336, loss_G_l1: 2.627, loss_G_gan: 7.081, loss_D: 0.284\n",
      "epoch: 32, loss_G_total: 12.745, loss_G_l1: 2.746, loss_G_gan: 7.252, loss_D: 0.291\n",
      "epoch: 33, loss_G_total: 10.364, loss_G_l1: 3.500, loss_G_gan: 3.364, loss_D: 1.866\n",
      "epoch: 34, loss_G_total: 13.612, loss_G_l1: 3.320, loss_G_gan: 6.972, loss_D: 0.282\n",
      "epoch: 35, loss_G_total: 11.653, loss_G_l1: 2.024, loss_G_gan: 7.605, loss_D: 0.991\n",
      "epoch: 36, loss_G_total: 12.948, loss_G_l1: 2.948, loss_G_gan: 7.052, loss_D: 0.770\n",
      "epoch: 37, loss_G_total: 10.377, loss_G_l1: 1.891, loss_G_gan: 6.594, loss_D: 0.713\n",
      "epoch: 38, loss_G_total: 10.065, loss_G_l1: 2.646, loss_G_gan: 4.774, loss_D: 0.403\n",
      "epoch: 39, loss_G_total: 8.445, loss_G_l1: 2.504, loss_G_gan: 3.437, loss_D: 0.506\n",
      "epoch: 40, loss_G_total: 6.672, loss_G_l1: 1.887, loss_G_gan: 2.899, loss_D: 0.997\n",
      "epoch: 41, loss_G_total: 7.091, loss_G_l1: 2.212, loss_G_gan: 2.667, loss_D: 0.890\n",
      "epoch: 42, loss_G_total: 7.057, loss_G_l1: 2.080, loss_G_gan: 2.897, loss_D: 0.489\n",
      "epoch: 43, loss_G_total: 6.965, loss_G_l1: 1.988, loss_G_gan: 2.990, loss_D: 0.510\n",
      "epoch: 44, loss_G_total: 7.151, loss_G_l1: 2.005, loss_G_gan: 3.142, loss_D: 0.534\n",
      "epoch: 45, loss_G_total: 7.798, loss_G_l1: 2.265, loss_G_gan: 3.267, loss_D: 0.534\n",
      "epoch: 46, loss_G_total: 7.926, loss_G_l1: 2.275, loss_G_gan: 3.377, loss_D: 0.542\n",
      "epoch: 47, loss_G_total: 7.218, loss_G_l1: 1.909, loss_G_gan: 3.399, loss_D: 0.536\n",
      "epoch: 48, loss_G_total: 7.203, loss_G_l1: 1.897, loss_G_gan: 3.408, loss_D: 0.538\n",
      "epoch: 49, loss_G_total: 7.115, loss_G_l1: 1.867, loss_G_gan: 3.382, loss_D: 0.533\n",
      "epoch: 50, loss_G_total: 6.990, loss_G_l1: 1.831, loss_G_gan: 3.328, loss_D: 0.541\n",
      "epoch: 51, loss_G_total: 6.882, loss_G_l1: 1.775, loss_G_gan: 3.331, loss_D: 0.543\n",
      "epoch: 52, loss_G_total: 8.230, loss_G_l1: 2.393, loss_G_gan: 3.443, loss_D: 0.546\n",
      "epoch: 53, loss_G_total: 6.680, loss_G_l1: 1.679, loss_G_gan: 3.323, loss_D: 0.542\n",
      "epoch: 54, loss_G_total: 6.580, loss_G_l1: 1.639, loss_G_gan: 3.302, loss_D: 0.541\n",
      "epoch: 55, loss_G_total: 6.443, loss_G_l1: 1.583, loss_G_gan: 3.276, loss_D: 0.538\n",
      "epoch: 56, loss_G_total: 6.294, loss_G_l1: 1.522, loss_G_gan: 3.249, loss_D: 0.537\n",
      "epoch: 57, loss_G_total: 8.196, loss_G_l1: 2.427, loss_G_gan: 3.342, loss_D: 0.538\n",
      "epoch: 58, loss_G_total: 7.651, loss_G_l1: 2.165, loss_G_gan: 3.322, loss_D: 0.536\n",
      "epoch: 59, loss_G_total: 5.892, loss_G_l1: 1.357, loss_G_gan: 3.179, loss_D: 0.534\n",
      "epoch: 60, loss_G_total: 5.785, loss_G_l1: 1.314, loss_G_gan: 3.157, loss_D: 0.532\n",
      "epoch: 61, loss_G_total: 8.021, loss_G_l1: 2.389, loss_G_gan: 3.242, loss_D: 0.533\n",
      "epoch: 62, loss_G_total: 7.942, loss_G_l1: 2.370, loss_G_gan: 3.202, loss_D: 0.533\n",
      "epoch: 63, loss_G_total: 7.227, loss_G_l1: 2.028, loss_G_gan: 3.171, loss_D: 0.531\n",
      "epoch: 64, loss_G_total: 5.411, loss_G_l1: 1.175, loss_G_gan: 3.061, loss_D: 0.530\n",
      "epoch: 65, loss_G_total: 5.373, loss_G_l1: 1.156, loss_G_gan: 3.061, loss_D: 0.530\n",
      "epoch: 66, loss_G_total: 5.294, loss_G_l1: 1.113, loss_G_gan: 3.069, loss_D: 0.530\n",
      "epoch: 67, loss_G_total: 5.210, loss_G_l1: 1.064, loss_G_gan: 3.083, loss_D: 0.530\n",
      "epoch: 68, loss_G_total: 5.103, loss_G_l1: 1.000, loss_G_gan: 3.103, loss_D: 0.530\n",
      "epoch: 69, loss_G_total: 7.794, loss_G_l1: 2.294, loss_G_gan: 3.207, loss_D: 0.533\n",
      "epoch: 70, loss_G_total: 4.990, loss_G_l1: 0.918, loss_G_gan: 3.155, loss_D: 0.529\n",
      "epoch: 71, loss_G_total: 4.983, loss_G_l1: 0.899, loss_G_gan: 3.186, loss_D: 0.529\n",
      "epoch: 72, loss_G_total: 4.932, loss_G_l1: 0.857, loss_G_gan: 3.218, loss_D: 0.529\n",
      "epoch: 73, loss_G_total: 8.027, loss_G_l1: 2.348, loss_G_gan: 3.330, loss_D: 0.532\n",
      "epoch: 74, loss_G_total: 4.946, loss_G_l1: 0.833, loss_G_gan: 3.280, loss_D: 0.528\n",
      "epoch: 75, loss_G_total: 7.304, loss_G_l1: 1.959, loss_G_gan: 3.387, loss_D: 0.528\n",
      "epoch: 76, loss_G_total: 4.981, loss_G_l1: 0.824, loss_G_gan: 3.333, loss_D: 0.527\n",
      "epoch: 77, loss_G_total: 8.162, loss_G_l1: 2.363, loss_G_gan: 3.436, loss_D: 0.531\n",
      "epoch: 78, loss_G_total: 7.363, loss_G_l1: 1.953, loss_G_gan: 3.457, loss_D: 0.526\n",
      "epoch: 79, loss_G_total: 7.350, loss_G_l1: 1.936, loss_G_gan: 3.478, loss_D: 0.525\n",
      "epoch: 80, loss_G_total: 5.136, loss_G_l1: 0.863, loss_G_gan: 3.411, loss_D: 0.525\n",
      "epoch: 81, loss_G_total: 7.306, loss_G_l1: 1.893, loss_G_gan: 3.520, loss_D: 0.524\n",
      "epoch: 82, loss_G_total: 5.217, loss_G_l1: 0.887, loss_G_gan: 3.443, loss_D: 0.524\n",
      "epoch: 83, loss_G_total: 5.260, loss_G_l1: 0.900, loss_G_gan: 3.460, loss_D: 0.524\n",
      "epoch: 84, loss_G_total: 5.241, loss_G_l1: 0.881, loss_G_gan: 3.479, loss_D: 0.523\n",
      "epoch: 85, loss_G_total: 5.257, loss_G_l1: 0.877, loss_G_gan: 3.502, loss_D: 0.523\n",
      "epoch: 86, loss_G_total: 7.357, loss_G_l1: 1.854, loss_G_gan: 3.650, loss_D: 0.520\n",
      "epoch: 87, loss_G_total: 7.385, loss_G_l1: 1.853, loss_G_gan: 3.679, loss_D: 0.519\n",
      "epoch: 88, loss_G_total: 5.276, loss_G_l1: 0.851, loss_G_gan: 3.574, loss_D: 0.521\n",
      "epoch: 89, loss_G_total: 5.286, loss_G_l1: 0.846, loss_G_gan: 3.594, loss_D: 0.520\n",
      "epoch: 90, loss_G_total: 5.250, loss_G_l1: 0.819, loss_G_gan: 3.612, loss_D: 0.520\n",
      "epoch: 91, loss_G_total: 5.229, loss_G_l1: 0.801, loss_G_gan: 3.628, loss_D: 0.519\n",
      "epoch: 92, loss_G_total: 7.550, loss_G_l1: 1.887, loss_G_gan: 3.776, loss_D: 0.514\n",
      "epoch: 93, loss_G_total: 7.583, loss_G_l1: 1.896, loss_G_gan: 3.791, loss_D: 0.512\n",
      "epoch: 94, loss_G_total: 5.197, loss_G_l1: 0.762, loss_G_gan: 3.672, loss_D: 0.514\n",
      "epoch: 95, loss_G_total: 5.219, loss_G_l1: 0.763, loss_G_gan: 3.693, loss_D: 0.510\n",
      "epoch: 96, loss_G_total: 5.239, loss_G_l1: 0.758, loss_G_gan: 3.724, loss_D: 0.503\n",
      "epoch: 97, loss_G_total: 8.595, loss_G_l1: 2.340, loss_G_gan: 3.915, loss_D: 0.530\n",
      "epoch: 98, loss_G_total: 7.788, loss_G_l1: 1.935, loss_G_gan: 3.918, loss_D: 0.493\n",
      "epoch: 99, loss_G_total: 8.444, loss_G_l1: 2.322, loss_G_gan: 3.799, loss_D: 0.520\n",
      "epoch: 100, loss_G_total: 5.329, loss_G_l1: 0.773, loss_G_gan: 3.783, loss_D: 0.495\n",
      "epoch: 101, loss_G_total: 8.546, loss_G_l1: 2.282, loss_G_gan: 3.983, loss_D: 0.528\n",
      "epoch: 102, loss_G_total: 8.522, loss_G_l1: 2.254, loss_G_gan: 4.014, loss_D: 0.524\n",
      "epoch: 103, loss_G_total: 7.746, loss_G_l1: 1.863, loss_G_gan: 4.019, loss_D: 0.509\n",
      "epoch: 104, loss_G_total: 5.657, loss_G_l1: 0.872, loss_G_gan: 3.914, loss_D: 0.519\n",
      "epoch: 105, loss_G_total: 5.713, loss_G_l1: 0.907, loss_G_gan: 3.900, loss_D: 0.518\n",
      "epoch: 106, loss_G_total: 5.665, loss_G_l1: 0.895, loss_G_gan: 3.875, loss_D: 0.516\n",
      "epoch: 107, loss_G_total: 5.611, loss_G_l1: 0.882, loss_G_gan: 3.846, loss_D: 0.515\n",
      "epoch: 108, loss_G_total: 5.545, loss_G_l1: 0.862, loss_G_gan: 3.821, loss_D: 0.515\n",
      "epoch: 109, loss_G_total: 7.517, loss_G_l1: 1.799, loss_G_gan: 3.920, loss_D: 0.507\n",
      "epoch: 110, loss_G_total: 8.297, loss_G_l1: 2.177, loss_G_gan: 3.944, loss_D: 0.516\n",
      "epoch: 111, loss_G_total: 5.528, loss_G_l1: 0.837, loss_G_gan: 3.854, loss_D: 0.513\n",
      "epoch: 112, loss_G_total: 7.608, loss_G_l1: 1.789, loss_G_gan: 4.031, loss_D: 0.500\n",
      "epoch: 113, loss_G_total: 5.643, loss_G_l1: 0.854, loss_G_gan: 3.935, loss_D: 0.510\n",
      "epoch: 114, loss_G_total: 8.463, loss_G_l1: 2.169, loss_G_gan: 4.124, loss_D: 0.511\n",
      "epoch: 115, loss_G_total: 5.706, loss_G_l1: 0.861, loss_G_gan: 3.983, loss_D: 0.508\n",
      "epoch: 116, loss_G_total: 7.651, loss_G_l1: 1.752, loss_G_gan: 4.147, loss_D: 0.493\n",
      "epoch: 117, loss_G_total: 5.698, loss_G_l1: 0.869, loss_G_gan: 3.959, loss_D: 0.503\n",
      "epoch: 118, loss_G_total: 5.652, loss_G_l1: 0.862, loss_G_gan: 3.928, loss_D: 0.500\n",
      "epoch: 119, loss_G_total: 8.356, loss_G_l1: 2.141, loss_G_gan: 4.073, loss_D: 0.506\n",
      "epoch: 120, loss_G_total: 5.598, loss_G_l1: 0.854, loss_G_gan: 3.889, loss_D: 0.497\n",
      "epoch: 121, loss_G_total: 7.542, loss_G_l1: 1.730, loss_G_gan: 4.082, loss_D: 0.482\n",
      "epoch: 122, loss_G_total: 5.721, loss_G_l1: 0.863, loss_G_gan: 3.995, loss_D: 0.489\n",
      "epoch: 123, loss_G_total: 5.812, loss_G_l1: 0.861, loss_G_gan: 4.090, loss_D: 0.485\n",
      "epoch: 124, loss_G_total: 5.866, loss_G_l1: 0.847, loss_G_gan: 4.173, loss_D: 0.482\n",
      "epoch: 125, loss_G_total: 8.717, loss_G_l1: 2.160, loss_G_gan: 4.397, loss_D: 0.547\n",
      "epoch: 126, loss_G_total: 8.680, loss_G_l1: 2.163, loss_G_gan: 4.354, loss_D: 0.542\n",
      "epoch: 127, loss_G_total: 8.556, loss_G_l1: 2.150, loss_G_gan: 4.257, loss_D: 0.530\n",
      "epoch: 128, loss_G_total: 5.768, loss_G_l1: 0.869, loss_G_gan: 4.030, loss_D: 0.478\n",
      "epoch: 129, loss_G_total: 5.737, loss_G_l1: 0.879, loss_G_gan: 3.979, loss_D: 0.482\n",
      "epoch: 130, loss_G_total: 7.533, loss_G_l1: 1.728, loss_G_gan: 4.076, loss_D: 0.489\n",
      "epoch: 131, loss_G_total: 7.560, loss_G_l1: 1.720, loss_G_gan: 4.120, loss_D: 0.477\n",
      "epoch: 132, loss_G_total: 5.872, loss_G_l1: 0.888, loss_G_gan: 4.097, loss_D: 0.472\n",
      "epoch: 133, loss_G_total: 5.958, loss_G_l1: 0.894, loss_G_gan: 4.170, loss_D: 0.472\n",
      "epoch: 134, loss_G_total: 5.968, loss_G_l1: 0.883, loss_G_gan: 4.202, loss_D: 0.471\n",
      "epoch: 135, loss_G_total: 5.919, loss_G_l1: 0.858, loss_G_gan: 4.202, loss_D: 0.468\n",
      "epoch: 136, loss_G_total: 5.884, loss_G_l1: 0.839, loss_G_gan: 4.206, loss_D: 0.463\n",
      "epoch: 137, loss_G_total: 5.888, loss_G_l1: 0.830, loss_G_gan: 4.229, loss_D: 0.456\n",
      "epoch: 138, loss_G_total: 5.888, loss_G_l1: 0.823, loss_G_gan: 4.241, loss_D: 0.452\n",
      "epoch: 139, loss_G_total: 5.828, loss_G_l1: 0.793, loss_G_gan: 4.242, loss_D: 0.449\n",
      "epoch: 140, loss_G_total: 5.850, loss_G_l1: 0.758, loss_G_gan: 4.334, loss_D: 0.429\n",
      "epoch: 141, loss_G_total: 5.931, loss_G_l1: 0.734, loss_G_gan: 4.463, loss_D: 0.522\n",
      "epoch: 142, loss_G_total: 5.792, loss_G_l1: 0.688, loss_G_gan: 4.415, loss_D: 0.517\n",
      "epoch: 143, loss_G_total: 5.567, loss_G_l1: 0.660, loss_G_gan: 4.247, loss_D: 0.502\n",
      "epoch: 144, loss_G_total: 9.214, loss_G_l1: 2.494, loss_G_gan: 4.226, loss_D: 0.540\n",
      "epoch: 145, loss_G_total: 8.333, loss_G_l1: 2.119, loss_G_gan: 4.094, loss_D: 0.512\n",
      "epoch: 146, loss_G_total: 5.232, loss_G_l1: 0.662, loss_G_gan: 3.908, loss_D: 0.531\n",
      "epoch: 147, loss_G_total: 8.450, loss_G_l1: 2.156, loss_G_gan: 4.138, loss_D: 0.507\n",
      "epoch: 148, loss_G_total: 5.424, loss_G_l1: 0.662, loss_G_gan: 4.100, loss_D: 0.505\n",
      "epoch: 149, loss_G_total: 5.581, loss_G_l1: 0.648, loss_G_gan: 4.284, loss_D: 0.501\n",
      "epoch: 150, loss_G_total: 5.656, loss_G_l1: 0.628, loss_G_gan: 4.401, loss_D: 0.510\n",
      "epoch: 151, loss_G_total: 8.962, loss_G_l1: 2.176, loss_G_gan: 4.609, loss_D: 0.503\n",
      "epoch: 152, loss_G_total: 5.600, loss_G_l1: 0.586, loss_G_gan: 4.428, loss_D: 0.511\n",
      "epoch: 153, loss_G_total: 9.675, loss_G_l1: 2.578, loss_G_gan: 4.519, loss_D: 0.538\n",
      "epoch: 154, loss_G_total: 9.533, loss_G_l1: 2.567, loss_G_gan: 4.398, loss_D: 0.528\n",
      "epoch: 155, loss_G_total: 5.330, loss_G_l1: 0.604, loss_G_gan: 4.122, loss_D: 0.501\n",
      "epoch: 156, loss_G_total: 5.263, loss_G_l1: 0.604, loss_G_gan: 4.056, loss_D: 0.506\n",
      "epoch: 157, loss_G_total: 5.240, loss_G_l1: 0.596, loss_G_gan: 4.047, loss_D: 0.506\n",
      "epoch: 158, loss_G_total: 8.416, loss_G_l1: 2.103, loss_G_gan: 4.210, loss_D: 0.490\n",
      "epoch: 159, loss_G_total: 8.502, loss_G_l1: 2.093, loss_G_gan: 4.315, loss_D: 0.478\n",
      "epoch: 160, loss_G_total: 8.601, loss_G_l1: 2.075, loss_G_gan: 4.451, loss_D: 0.471\n",
      "epoch: 161, loss_G_total: 5.759, loss_G_l1: 0.655, loss_G_gan: 4.449, loss_D: 0.507\n",
      "epoch: 162, loss_G_total: 5.805, loss_G_l1: 0.662, loss_G_gan: 4.481, loss_D: 0.510\n",
      "epoch: 163, loss_G_total: 8.601, loss_G_l1: 2.017, loss_G_gan: 4.567, loss_D: 0.469\n",
      "epoch: 164, loss_G_total: 5.755, loss_G_l1: 0.684, loss_G_gan: 4.388, loss_D: 0.494\n",
      "epoch: 165, loss_G_total: 8.366, loss_G_l1: 1.988, loss_G_gan: 4.391, loss_D: 0.457\n",
      "epoch: 166, loss_G_total: 5.571, loss_G_l1: 0.699, loss_G_gan: 4.173, loss_D: 0.485\n",
      "epoch: 167, loss_G_total: 9.004, loss_G_l1: 2.384, loss_G_gan: 4.236, loss_D: 0.493\n",
      "epoch: 168, loss_G_total: 8.952, loss_G_l1: 2.361, loss_G_gan: 4.229, loss_D: 0.491\n",
      "epoch: 169, loss_G_total: 8.104, loss_G_l1: 1.915, loss_G_gan: 4.273, loss_D: 0.456\n",
      "epoch: 170, loss_G_total: 5.769, loss_G_l1: 0.769, loss_G_gan: 4.232, loss_D: 0.477\n",
      "epoch: 171, loss_G_total: 5.879, loss_G_l1: 0.774, loss_G_gan: 4.331, loss_D: 0.476\n",
      "epoch: 172, loss_G_total: 5.949, loss_G_l1: 0.775, loss_G_gan: 4.400, loss_D: 0.476\n",
      "epoch: 173, loss_G_total: 9.095, loss_G_l1: 2.245, loss_G_gan: 4.605, loss_D: 0.496\n",
      "epoch: 174, loss_G_total: 5.952, loss_G_l1: 0.784, loss_G_gan: 4.385, loss_D: 0.467\n",
      "epoch: 175, loss_G_total: 5.878, loss_G_l1: 0.784, loss_G_gan: 4.310, loss_D: 0.461\n",
      "epoch: 176, loss_G_total: 5.776, loss_G_l1: 0.771, loss_G_gan: 4.234, loss_D: 0.461\n",
      "epoch: 177, loss_G_total: 5.707, loss_G_l1: 0.750, loss_G_gan: 4.206, loss_D: 0.463\n",
      "epoch: 178, loss_G_total: 5.711, loss_G_l1: 0.734, loss_G_gan: 4.243, loss_D: 0.460\n",
      "epoch: 179, loss_G_total: 5.759, loss_G_l1: 0.721, loss_G_gan: 4.317, loss_D: 0.457\n",
      "epoch: 180, loss_G_total: 5.774, loss_G_l1: 0.692, loss_G_gan: 4.391, loss_D: 0.458\n",
      "epoch: 181, loss_G_total: 5.801, loss_G_l1: 0.682, loss_G_gan: 4.437, loss_D: 0.459\n",
      "epoch: 182, loss_G_total: 9.531, loss_G_l1: 2.485, loss_G_gan: 4.561, loss_D: 0.492\n",
      "epoch: 183, loss_G_total: 8.694, loss_G_l1: 2.085, loss_G_gan: 4.523, loss_D: 0.430\n",
      "epoch: 184, loss_G_total: 8.728, loss_G_l1: 2.100, loss_G_gan: 4.527, loss_D: 0.426\n",
      "epoch: 185, loss_G_total: 5.756, loss_G_l1: 0.647, loss_G_gan: 4.461, loss_D: 0.453\n",
      "epoch: 186, loss_G_total: 8.790, loss_G_l1: 2.110, loss_G_gan: 4.570, loss_D: 0.415\n",
      "epoch: 187, loss_G_total: 9.677, loss_G_l1: 2.539, loss_G_gan: 4.599, loss_D: 0.469\n",
      "epoch: 188, loss_G_total: 5.726, loss_G_l1: 0.650, loss_G_gan: 4.426, loss_D: 0.454\n",
      "epoch: 189, loss_G_total: 5.627, loss_G_l1: 0.641, loss_G_gan: 4.346, loss_D: 0.453\n",
      "epoch: 190, loss_G_total: 5.553, loss_G_l1: 0.626, loss_G_gan: 4.301, loss_D: 0.451\n",
      "epoch: 191, loss_G_total: 9.482, loss_G_l1: 2.513, loss_G_gan: 4.456, loss_D: 0.449\n",
      "epoch: 192, loss_G_total: 8.699, loss_G_l1: 2.092, loss_G_gan: 4.515, loss_D: 0.407\n",
      "epoch: 193, loss_G_total: 9.638, loss_G_l1: 2.487, loss_G_gan: 4.665, loss_D: 0.450\n",
      "epoch: 194, loss_G_total: 8.817, loss_G_l1: 2.059, loss_G_gan: 4.699, loss_D: 0.391\n",
      "epoch: 195, loss_G_total: 5.972, loss_G_l1: 0.683, loss_G_gan: 4.605, loss_D: 0.439\n",
      "epoch: 196, loss_G_total: 8.740, loss_G_l1: 2.013, loss_G_gan: 4.715, loss_D: 0.386\n",
      "epoch: 197, loss_G_total: 6.017, loss_G_l1: 0.725, loss_G_gan: 4.567, loss_D: 0.426\n",
      "epoch: 198, loss_G_total: 5.956, loss_G_l1: 0.721, loss_G_gan: 4.513, loss_D: 0.422\n",
      "epoch: 199, loss_G_total: 8.556, loss_G_l1: 1.974, loss_G_gan: 4.608, loss_D: 0.397\n",
      "epoch: 200, loss_G_total: 9.401, loss_G_l1: 2.363, loss_G_gan: 4.675, loss_D: 0.426\n",
      "epoch: 201, loss_G_total: 6.023, loss_G_l1: 0.741, loss_G_gan: 4.541, loss_D: 0.418\n",
      "epoch: 202, loss_G_total: 9.401, loss_G_l1: 2.341, loss_G_gan: 4.720, loss_D: 0.414\n",
      "epoch: 203, loss_G_total: 9.320, loss_G_l1: 2.321, loss_G_gan: 4.678, loss_D: 0.404\n",
      "epoch: 204, loss_G_total: 6.029, loss_G_l1: 0.789, loss_G_gan: 4.451, loss_D: 0.419\n",
      "epoch: 205, loss_G_total: 9.109, loss_G_l1: 2.273, loss_G_gan: 4.564, loss_D: 0.396\n",
      "epoch: 206, loss_G_total: 9.088, loss_G_l1: 2.244, loss_G_gan: 4.600, loss_D: 0.390\n",
      "epoch: 207, loss_G_total: 9.067, loss_G_l1: 2.205, loss_G_gan: 4.656, loss_D: 0.384\n",
      "epoch: 208, loss_G_total: 6.207, loss_G_l1: 0.852, loss_G_gan: 4.502, loss_D: 0.401\n",
      "epoch: 209, loss_G_total: 6.074, loss_G_l1: 0.863, loss_G_gan: 4.348, loss_D: 0.424\n",
      "epoch: 210, loss_G_total: 6.115, loss_G_l1: 0.867, loss_G_gan: 4.382, loss_D: 0.421\n",
      "epoch: 211, loss_G_total: 6.176, loss_G_l1: 0.857, loss_G_gan: 4.462, loss_D: 0.415\n",
      "epoch: 212, loss_G_total: 8.227, loss_G_l1: 1.769, loss_G_gan: 4.689, loss_D: 0.373\n",
      "epoch: 213, loss_G_total: 8.333, loss_G_l1: 1.769, loss_G_gan: 4.795, loss_D: 0.366\n",
      "epoch: 214, loss_G_total: 6.502, loss_G_l1: 0.882, loss_G_gan: 4.737, loss_D: 0.433\n",
      "epoch: 215, loss_G_total: 6.542, loss_G_l1: 0.886, loss_G_gan: 4.770, loss_D: 0.435\n",
      "epoch: 216, loss_G_total: 6.475, loss_G_l1: 0.861, loss_G_gan: 4.753, loss_D: 0.425\n",
      "epoch: 217, loss_G_total: 6.361, loss_G_l1: 0.849, loss_G_gan: 4.664, loss_D: 0.408\n",
      "epoch: 218, loss_G_total: 6.154, loss_G_l1: 0.827, loss_G_gan: 4.501, loss_D: 0.409\n",
      "epoch: 219, loss_G_total: 6.146, loss_G_l1: 0.809, loss_G_gan: 4.528, loss_D: 0.416\n",
      "epoch: 220, loss_G_total: 6.158, loss_G_l1: 0.776, loss_G_gan: 4.606, loss_D: 0.411\n",
      "epoch: 221, loss_G_total: 9.448, loss_G_l1: 2.296, loss_G_gan: 4.856, loss_D: 0.389\n",
      "epoch: 222, loss_G_total: 8.864, loss_G_l1: 1.924, loss_G_gan: 5.015, loss_D: 0.370\n",
      "epoch: 223, loss_G_total: 9.009, loss_G_l1: 1.939, loss_G_gan: 5.131, loss_D: 0.379\n",
      "epoch: 224, loss_G_total: 6.389, loss_G_l1: 0.757, loss_G_gan: 4.876, loss_D: 0.415\n",
      "epoch: 225, loss_G_total: 6.282, loss_G_l1: 0.743, loss_G_gan: 4.797, loss_D: 0.408\n",
      "epoch: 226, loss_G_total: 8.686, loss_G_l1: 1.963, loss_G_gan: 4.760, loss_D: 0.366\n",
      "epoch: 227, loss_G_total: 5.908, loss_G_l1: 0.726, loss_G_gan: 4.456, loss_D: 0.407\n",
      "epoch: 228, loss_G_total: 8.413, loss_G_l1: 1.974, loss_G_gan: 4.464, loss_D: 0.381\n",
      "epoch: 229, loss_G_total: 9.567, loss_G_l1: 2.392, loss_G_gan: 4.782, loss_D: 0.361\n",
      "epoch: 230, loss_G_total: 6.185, loss_G_l1: 0.725, loss_G_gan: 4.735, loss_D: 0.413\n",
      "epoch: 231, loss_G_total: 6.232, loss_G_l1: 0.719, loss_G_gan: 4.793, loss_D: 0.419\n",
      "epoch: 232, loss_G_total: 8.960, loss_G_l1: 1.974, loss_G_gan: 5.011, loss_D: 0.353\n",
      "epoch: 233, loss_G_total: 8.962, loss_G_l1: 1.971, loss_G_gan: 5.021, loss_D: 0.352\n",
      "epoch: 234, loss_G_total: 6.269, loss_G_l1: 0.722, loss_G_gan: 4.825, loss_D: 0.418\n",
      "epoch: 235, loss_G_total: 5.250, loss_G_l1: 0.724, loss_G_gan: 3.802, loss_D: 0.604\n",
      "epoch: 236, loss_G_total: 7.602, loss_G_l1: 1.964, loss_G_gan: 3.673, loss_D: 0.673\n",
      "epoch: 237, loss_G_total: 8.699, loss_G_l1: 1.968, loss_G_gan: 4.763, loss_D: 0.406\n",
      "epoch: 238, loss_G_total: 6.593, loss_G_l1: 0.729, loss_G_gan: 5.135, loss_D: 0.419\n",
      "epoch: 239, loss_G_total: 10.145, loss_G_l1: 2.351, loss_G_gan: 5.443, loss_D: 0.417\n",
      "epoch: 240, loss_G_total: 6.691, loss_G_l1: 0.730, loss_G_gan: 5.230, loss_D: 0.417\n",
      "epoch: 241, loss_G_total: 9.905, loss_G_l1: 2.331, loss_G_gan: 5.244, loss_D: 0.405\n",
      "epoch: 242, loss_G_total: 6.369, loss_G_l1: 0.740, loss_G_gan: 4.890, loss_D: 0.406\n",
      "epoch: 243, loss_G_total: 5.997, loss_G_l1: 0.734, loss_G_gan: 4.530, loss_D: 0.410\n",
      "epoch: 244, loss_G_total: 8.107, loss_G_l1: 1.944, loss_G_gan: 4.220, loss_D: 0.438\n",
      "epoch: 245, loss_G_total: 8.990, loss_G_l1: 2.313, loss_G_gan: 4.364, loss_D: 0.421\n",
      "epoch: 246, loss_G_total: 9.359, loss_G_l1: 2.296, loss_G_gan: 4.768, loss_D: 0.399\n",
      "epoch: 247, loss_G_total: 9.451, loss_G_l1: 2.266, loss_G_gan: 4.919, loss_D: 0.386\n",
      "epoch: 248, loss_G_total: 9.650, loss_G_l1: 2.224, loss_G_gan: 5.202, loss_D: 0.447\n",
      "epoch: 249, loss_G_total: 6.651, loss_G_l1: 0.843, loss_G_gan: 4.966, loss_D: 0.392\n",
      "epoch: 250, loss_G_total: 6.490, loss_G_l1: 0.860, loss_G_gan: 4.770, loss_D: 0.395\n",
      "epoch: 251, loss_G_total: 6.644, loss_G_l1: 0.867, loss_G_gan: 4.911, loss_D: 0.373\n",
      "epoch: 252, loss_G_total: 6.633, loss_G_l1: 0.869, loss_G_gan: 4.894, loss_D: 0.392\n",
      "epoch: 253, loss_G_total: 6.408, loss_G_l1: 0.842, loss_G_gan: 4.725, loss_D: 0.385\n",
      "epoch: 254, loss_G_total: 9.320, loss_G_l1: 2.184, loss_G_gan: 4.952, loss_D: 0.387\n",
      "epoch: 255, loss_G_total: 8.644, loss_G_l1: 1.857, loss_G_gan: 4.930, loss_D: 0.382\n",
      "epoch: 256, loss_G_total: 6.371, loss_G_l1: 0.807, loss_G_gan: 4.758, loss_D: 0.402\n",
      "epoch: 257, loss_G_total: 6.375, loss_G_l1: 0.802, loss_G_gan: 4.770, loss_D: 0.401\n",
      "epoch: 258, loss_G_total: 9.484, loss_G_l1: 2.258, loss_G_gan: 4.968, loss_D: 0.387\n",
      "epoch: 259, loss_G_total: 9.551, loss_G_l1: 2.270, loss_G_gan: 5.011, loss_D: 0.382\n",
      "epoch: 260, loss_G_total: 6.502, loss_G_l1: 0.808, loss_G_gan: 4.886, loss_D: 0.400\n",
      "epoch: 261, loss_G_total: 9.655, loss_G_l1: 2.273, loss_G_gan: 5.108, loss_D: 0.372\n",
      "epoch: 262, loss_G_total: 8.856, loss_G_l1: 1.866, loss_G_gan: 5.124, loss_D: 0.354\n",
      "epoch: 263, loss_G_total: 6.641, loss_G_l1: 0.848, loss_G_gan: 4.944, loss_D: 0.398\n",
      "epoch: 264, loss_G_total: 6.603, loss_G_l1: 0.850, loss_G_gan: 4.904, loss_D: 0.384\n",
      "epoch: 265, loss_G_total: 6.409, loss_G_l1: 0.848, loss_G_gan: 4.713, loss_D: 0.379\n",
      "epoch: 266, loss_G_total: 6.395, loss_G_l1: 0.819, loss_G_gan: 4.757, loss_D: 0.379\n",
      "epoch: 267, loss_G_total: 6.326, loss_G_l1: 0.799, loss_G_gan: 4.727, loss_D: 0.372\n",
      "epoch: 268, loss_G_total: 6.265, loss_G_l1: 0.771, loss_G_gan: 4.724, loss_D: 0.373\n",
      "epoch: 269, loss_G_total: 6.343, loss_G_l1: 0.756, loss_G_gan: 4.831, loss_D: 0.393\n",
      "epoch: 270, loss_G_total: 8.982, loss_G_l1: 1.968, loss_G_gan: 5.045, loss_D: 0.355\n",
      "epoch: 271, loss_G_total: 9.840, loss_G_l1: 2.369, loss_G_gan: 5.102, loss_D: 0.368\n",
      "epoch: 272, loss_G_total: 6.435, loss_G_l1: 0.728, loss_G_gan: 4.980, loss_D: 0.407\n",
      "epoch: 273, loss_G_total: 6.429, loss_G_l1: 0.714, loss_G_gan: 5.001, loss_D: 0.406\n",
      "epoch: 274, loss_G_total: 6.433, loss_G_l1: 0.707, loss_G_gan: 5.019, loss_D: 0.398\n",
      "epoch: 275, loss_G_total: 6.344, loss_G_l1: 0.667, loss_G_gan: 5.010, loss_D: 0.391\n",
      "epoch: 276, loss_G_total: 6.223, loss_G_l1: 0.658, loss_G_gan: 4.907, loss_D: 0.391\n",
      "epoch: 277, loss_G_total: 6.093, loss_G_l1: 0.633, loss_G_gan: 4.827, loss_D: 0.403\n",
      "epoch: 278, loss_G_total: 6.013, loss_G_l1: 0.607, loss_G_gan: 4.799, loss_D: 0.412\n",
      "epoch: 279, loss_G_total: 6.002, loss_G_l1: 0.600, loss_G_gan: 4.802, loss_D: 0.416\n",
      "epoch: 280, loss_G_total: 6.026, loss_G_l1: 0.599, loss_G_gan: 4.827, loss_D: 0.418\n",
      "epoch: 281, loss_G_total: 10.346, loss_G_l1: 2.665, loss_G_gan: 5.016, loss_D: 0.397\n",
      "epoch: 282, loss_G_total: 6.206, loss_G_l1: 0.641, loss_G_gan: 4.923, loss_D: 0.422\n",
      "epoch: 283, loss_G_total: 9.610, loss_G_l1: 2.247, loss_G_gan: 5.116, loss_D: 0.364\n",
      "epoch: 284, loss_G_total: 9.719, loss_G_l1: 2.246, loss_G_gan: 5.227, loss_D: 0.353\n",
      "epoch: 285, loss_G_total: 6.539, loss_G_l1: 0.683, loss_G_gan: 5.172, loss_D: 0.449\n",
      "epoch: 286, loss_G_total: 6.595, loss_G_l1: 0.697, loss_G_gan: 5.201, loss_D: 0.457\n",
      "epoch: 287, loss_G_total: 9.813, loss_G_l1: 2.222, loss_G_gan: 5.370, loss_D: 0.353\n",
      "epoch: 288, loss_G_total: 6.493, loss_G_l1: 0.679, loss_G_gan: 5.135, loss_D: 0.448\n",
      "epoch: 289, loss_G_total: 9.607, loss_G_l1: 2.189, loss_G_gan: 5.229, loss_D: 0.347\n",
      "epoch: 290, loss_G_total: 6.290, loss_G_l1: 0.676, loss_G_gan: 4.939, loss_D: 0.433\n",
      "epoch: 291, loss_G_total: 10.136, loss_G_l1: 2.574, loss_G_gan: 4.988, loss_D: 0.364\n",
      "epoch: 292, loss_G_total: 9.969, loss_G_l1: 2.539, loss_G_gan: 4.890, loss_D: 0.365\n",
      "epoch: 293, loss_G_total: 9.052, loss_G_l1: 2.092, loss_G_gan: 4.868, loss_D: 0.353\n",
      "epoch: 294, loss_G_total: 9.820, loss_G_l1: 2.445, loss_G_gan: 4.930, loss_D: 0.356\n",
      "epoch: 295, loss_G_total: 6.398, loss_G_l1: 0.792, loss_G_gan: 4.813, loss_D: 0.425\n",
      "epoch: 296, loss_G_total: 9.746, loss_G_l1: 2.350, loss_G_gan: 5.047, loss_D: 0.355\n",
      "epoch: 297, loss_G_total: 9.666, loss_G_l1: 2.299, loss_G_gan: 5.068, loss_D: 0.353\n",
      "epoch: 298, loss_G_total: 6.764, loss_G_l1: 0.942, loss_G_gan: 4.880, loss_D: 0.422\n",
      "epoch: 299, loss_G_total: 6.846, loss_G_l1: 0.982, loss_G_gan: 4.883, loss_D: 0.418\n",
      "epoch: 300, loss_G_total: 6.875, loss_G_l1: 0.999, loss_G_gan: 4.878, loss_D: 0.414\n",
      "epoch: 301, loss_G_total: 6.841, loss_G_l1: 0.990, loss_G_gan: 4.860, loss_D: 0.412\n",
      "epoch: 302, loss_G_total: 6.773, loss_G_l1: 0.968, loss_G_gan: 4.838, loss_D: 0.411\n",
      "epoch: 303, loss_G_total: 6.681, loss_G_l1: 0.930, loss_G_gan: 4.821, loss_D: 0.410\n",
      "epoch: 304, loss_G_total: 6.604, loss_G_l1: 0.893, loss_G_gan: 4.817, loss_D: 0.410\n",
      "epoch: 305, loss_G_total: 6.538, loss_G_l1: 0.855, loss_G_gan: 4.827, loss_D: 0.409\n",
      "epoch: 306, loss_G_total: 6.479, loss_G_l1: 0.815, loss_G_gan: 4.849, loss_D: 0.408\n",
      "epoch: 307, loss_G_total: 6.437, loss_G_l1: 0.780, loss_G_gan: 4.876, loss_D: 0.407\n",
      "epoch: 308, loss_G_total: 9.788, loss_G_l1: 2.374, loss_G_gan: 5.039, loss_D: 0.351\n",
      "epoch: 309, loss_G_total: 6.425, loss_G_l1: 0.734, loss_G_gan: 4.957, loss_D: 0.407\n",
      "epoch: 310, loss_G_total: 10.046, loss_G_l1: 2.452, loss_G_gan: 5.142, loss_D: 0.341\n",
      "epoch: 311, loss_G_total: 10.138, loss_G_l1: 2.474, loss_G_gan: 5.191, loss_D: 0.340\n",
      "epoch: 312, loss_G_total: 9.452, loss_G_l1: 2.108, loss_G_gan: 5.236, loss_D: 0.327\n",
      "epoch: 313, loss_G_total: 9.475, loss_G_l1: 2.096, loss_G_gan: 5.282, loss_D: 0.326\n",
      "epoch: 314, loss_G_total: 9.477, loss_G_l1: 2.075, loss_G_gan: 5.328, loss_D: 0.325\n",
      "epoch: 315, loss_G_total: 6.658, loss_G_l1: 0.730, loss_G_gan: 5.199, loss_D: 0.424\n",
      "epoch: 316, loss_G_total: 9.428, loss_G_l1: 2.021, loss_G_gan: 5.386, loss_D: 0.323\n",
      "epoch: 317, loss_G_total: 6.742, loss_G_l1: 0.760, loss_G_gan: 5.221, loss_D: 0.422\n",
      "epoch: 318, loss_G_total: 9.326, loss_G_l1: 1.968, loss_G_gan: 5.389, loss_D: 0.319\n",
      "epoch: 319, loss_G_total: 6.746, loss_G_l1: 0.773, loss_G_gan: 5.199, loss_D: 0.413\n",
      "epoch: 320, loss_G_total: 9.967, loss_G_l1: 2.309, loss_G_gan: 5.349, loss_D: 0.324\n",
      "epoch: 321, loss_G_total: 6.717, loss_G_l1: 0.787, loss_G_gan: 5.142, loss_D: 0.399\n",
      "epoch: 322, loss_G_total: 9.761, loss_G_l1: 2.257, loss_G_gan: 5.247, loss_D: 0.331\n",
      "epoch: 323, loss_G_total: 6.669, loss_G_l1: 0.808, loss_G_gan: 5.053, loss_D: 0.380\n",
      "epoch: 324, loss_G_total: 6.638, loss_G_l1: 0.816, loss_G_gan: 5.005, loss_D: 0.383\n",
      "epoch: 325, loss_G_total: 6.655, loss_G_l1: 0.811, loss_G_gan: 5.034, loss_D: 0.382\n",
      "epoch: 326, loss_G_total: 6.708, loss_G_l1: 0.800, loss_G_gan: 5.108, loss_D: 0.385\n",
      "epoch: 327, loss_G_total: 6.704, loss_G_l1: 0.791, loss_G_gan: 5.122, loss_D: 0.390\n",
      "epoch: 328, loss_G_total: 6.589, loss_G_l1: 0.766, loss_G_gan: 5.056, loss_D: 0.398\n",
      "epoch: 329, loss_G_total: 6.592, loss_G_l1: 0.752, loss_G_gan: 5.087, loss_D: 0.408\n",
      "epoch: 330, loss_G_total: 10.130, loss_G_l1: 2.396, loss_G_gan: 5.338, loss_D: 0.336\n",
      "epoch: 331, loss_G_total: 6.799, loss_G_l1: 0.736, loss_G_gan: 5.327, loss_D: 0.427\n",
      "epoch: 332, loss_G_total: 6.831, loss_G_l1: 0.722, loss_G_gan: 5.387, loss_D: 0.444\n",
      "epoch: 333, loss_G_total: 10.664, loss_G_l1: 2.533, loss_G_gan: 5.597, loss_D: 0.359\n",
      "epoch: 334, loss_G_total: 9.810, loss_G_l1: 2.124, loss_G_gan: 5.562, loss_D: 0.324\n",
      "epoch: 335, loss_G_total: 6.750, loss_G_l1: 0.710, loss_G_gan: 5.331, loss_D: 0.443\n",
      "epoch: 336, loss_G_total: 6.663, loss_G_l1: 0.699, loss_G_gan: 5.265, loss_D: 0.436\n",
      "epoch: 337, loss_G_total: 6.533, loss_G_l1: 0.681, loss_G_gan: 5.170, loss_D: 0.427\n",
      "epoch: 338, loss_G_total: 6.349, loss_G_l1: 0.659, loss_G_gan: 5.031, loss_D: 0.424\n",
      "epoch: 339, loss_G_total: 6.175, loss_G_l1: 0.647, loss_G_gan: 4.882, loss_D: 0.429\n",
      "epoch: 340, loss_G_total: 6.109, loss_G_l1: 0.645, loss_G_gan: 4.819, loss_D: 0.426\n",
      "epoch: 341, loss_G_total: 6.111, loss_G_l1: 0.648, loss_G_gan: 4.816, loss_D: 0.421\n",
      "epoch: 342, loss_G_total: 6.115, loss_G_l1: 0.645, loss_G_gan: 4.825, loss_D: 0.419\n",
      "epoch: 343, loss_G_total: 10.355, loss_G_l1: 2.677, loss_G_gan: 5.002, loss_D: 0.345\n",
      "epoch: 344, loss_G_total: 9.554, loss_G_l1: 2.256, loss_G_gan: 5.041, loss_D: 0.333\n",
      "epoch: 345, loss_G_total: 9.584, loss_G_l1: 2.246, loss_G_gan: 5.092, loss_D: 0.332\n",
      "epoch: 346, loss_G_total: 6.280, loss_G_l1: 0.644, loss_G_gan: 4.993, loss_D: 0.417\n",
      "epoch: 347, loss_G_total: 6.289, loss_G_l1: 0.630, loss_G_gan: 5.030, loss_D: 0.417\n",
      "epoch: 348, loss_G_total: 6.271, loss_G_l1: 0.613, loss_G_gan: 5.044, loss_D: 0.417\n",
      "epoch: 349, loss_G_total: 6.232, loss_G_l1: 0.593, loss_G_gan: 5.045, loss_D: 0.416\n",
      "epoch: 350, loss_G_total: 9.524, loss_G_l1: 2.160, loss_G_gan: 5.203, loss_D: 0.333\n",
      "epoch: 351, loss_G_total: 10.279, loss_G_l1: 2.540, loss_G_gan: 5.199, loss_D: 0.347\n",
      "epoch: 352, loss_G_total: 9.399, loss_G_l1: 2.104, loss_G_gan: 5.191, loss_D: 0.327\n",
      "epoch: 353, loss_G_total: 6.357, loss_G_l1: 0.666, loss_G_gan: 5.025, loss_D: 0.410\n",
      "epoch: 354, loss_G_total: 6.384, loss_G_l1: 0.683, loss_G_gan: 5.019, loss_D: 0.408\n",
      "epoch: 355, loss_G_total: 9.212, loss_G_l1: 2.021, loss_G_gan: 5.169, loss_D: 0.322\n",
      "epoch: 356, loss_G_total: 6.419, loss_G_l1: 0.706, loss_G_gan: 5.007, loss_D: 0.404\n",
      "epoch: 357, loss_G_total: 9.130, loss_G_l1: 1.985, loss_G_gan: 5.159, loss_D: 0.322\n",
      "epoch: 358, loss_G_total: 9.895, loss_G_l1: 2.364, loss_G_gan: 5.167, loss_D: 0.325\n",
      "epoch: 359, loss_G_total: 6.515, loss_G_l1: 0.745, loss_G_gan: 5.025, loss_D: 0.400\n",
      "epoch: 360, loss_G_total: 9.027, loss_G_l1: 1.914, loss_G_gan: 5.199, loss_D: 0.320\n",
      "epoch: 361, loss_G_total: 8.999, loss_G_l1: 1.886, loss_G_gan: 5.227, loss_D: 0.318\n",
      "epoch: 362, loss_G_total: 6.700, loss_G_l1: 0.802, loss_G_gan: 5.096, loss_D: 0.400\n",
      "epoch: 363, loss_G_total: 6.734, loss_G_l1: 0.809, loss_G_gan: 5.115, loss_D: 0.400\n",
      "epoch: 364, loss_G_total: 6.733, loss_G_l1: 0.811, loss_G_gan: 5.111, loss_D: 0.396\n",
      "epoch: 365, loss_G_total: 8.833, loss_G_l1: 1.813, loss_G_gan: 5.206, loss_D: 0.334\n",
      "epoch: 366, loss_G_total: 6.745, loss_G_l1: 0.827, loss_G_gan: 5.090, loss_D: 0.389\n",
      "epoch: 367, loss_G_total: 6.700, loss_G_l1: 0.833, loss_G_gan: 5.035, loss_D: 0.396\n",
      "epoch: 368, loss_G_total: 8.909, loss_G_l1: 1.826, loss_G_gan: 5.257, loss_D: 0.338\n",
      "epoch: 369, loss_G_total: 7.014, loss_G_l1: 0.851, loss_G_gan: 5.312, loss_D: 0.425\n",
      "epoch: 370, loss_G_total: 10.211, loss_G_l1: 2.295, loss_G_gan: 5.621, loss_D: 0.329\n",
      "epoch: 371, loss_G_total: 7.201, loss_G_l1: 0.882, loss_G_gan: 5.438, loss_D: 0.446\n",
      "epoch: 372, loss_G_total: 7.213, loss_G_l1: 0.880, loss_G_gan: 5.452, loss_D: 0.449\n",
      "epoch: 373, loss_G_total: 9.445, loss_G_l1: 1.883, loss_G_gan: 5.679, loss_D: 0.343\n",
      "epoch: 374, loss_G_total: 7.140, loss_G_l1: 0.862, loss_G_gan: 5.417, loss_D: 0.445\n",
      "epoch: 375, loss_G_total: 7.077, loss_G_l1: 0.851, loss_G_gan: 5.375, loss_D: 0.440\n",
      "epoch: 376, loss_G_total: 6.983, loss_G_l1: 0.831, loss_G_gan: 5.321, loss_D: 0.434\n",
      "epoch: 377, loss_G_total: 6.900, loss_G_l1: 0.821, loss_G_gan: 5.257, loss_D: 0.427\n",
      "epoch: 378, loss_G_total: 6.754, loss_G_l1: 0.784, loss_G_gan: 5.186, loss_D: 0.421\n",
      "epoch: 379, loss_G_total: 6.613, loss_G_l1: 0.751, loss_G_gan: 5.111, loss_D: 0.416\n",
      "epoch: 380, loss_G_total: 6.461, loss_G_l1: 0.717, loss_G_gan: 5.026, loss_D: 0.413\n",
      "epoch: 381, loss_G_total: 9.169, loss_G_l1: 2.033, loss_G_gan: 5.103, loss_D: 0.330\n",
      "epoch: 382, loss_G_total: 6.210, loss_G_l1: 0.679, loss_G_gan: 4.852, loss_D: 0.410\n",
      "epoch: 383, loss_G_total: 6.141, loss_G_l1: 0.673, loss_G_gan: 4.795, loss_D: 0.410\n",
      "epoch: 384, loss_G_total: 6.065, loss_G_l1: 0.659, loss_G_gan: 4.748, loss_D: 0.410\n",
      "epoch: 385, loss_G_total: 6.009, loss_G_l1: 0.643, loss_G_gan: 4.724, loss_D: 0.411\n",
      "epoch: 386, loss_G_total: 9.201, loss_G_l1: 2.167, loss_G_gan: 4.867, loss_D: 0.351\n",
      "epoch: 387, loss_G_total: 6.031, loss_G_l1: 0.628, loss_G_gan: 4.775, loss_D: 0.409\n",
      "epoch: 388, loss_G_total: 6.059, loss_G_l1: 0.616, loss_G_gan: 4.827, loss_D: 0.408\n",
      "epoch: 389, loss_G_total: 9.452, loss_G_l1: 2.219, loss_G_gan: 5.015, loss_D: 0.342\n",
      "epoch: 390, loss_G_total: 6.195, loss_G_l1: 0.614, loss_G_gan: 4.966, loss_D: 0.407\n",
      "epoch: 391, loss_G_total: 10.394, loss_G_l1: 2.631, loss_G_gan: 5.133, loss_D: 0.338\n",
      "epoch: 392, loss_G_total: 6.292, loss_G_l1: 0.615, loss_G_gan: 5.061, loss_D: 0.407\n",
      "epoch: 393, loss_G_total: 10.458, loss_G_l1: 2.615, loss_G_gan: 5.229, loss_D: 0.329\n",
      "epoch: 394, loss_G_total: 10.457, loss_G_l1: 2.591, loss_G_gan: 5.276, loss_D: 0.325\n",
      "epoch: 395, loss_G_total: 9.643, loss_G_l1: 2.160, loss_G_gan: 5.324, loss_D: 0.313\n",
      "epoch: 396, loss_G_total: 6.497, loss_G_l1: 0.636, loss_G_gan: 5.224, loss_D: 0.413\n",
      "epoch: 397, loss_G_total: 9.594, loss_G_l1: 2.090, loss_G_gan: 5.413, loss_D: 0.307\n",
      "epoch: 398, loss_G_total: 9.558, loss_G_l1: 2.052, loss_G_gan: 5.454, loss_D: 0.305\n",
      "epoch: 399, loss_G_total: 10.262, loss_G_l1: 2.382, loss_G_gan: 5.497, loss_D: 0.310\n",
      "epoch: 400, loss_G_total: 6.840, loss_G_l1: 0.746, loss_G_gan: 5.348, loss_D: 0.418\n",
      "epoch: 401, loss_G_total: 9.375, loss_G_l1: 1.914, loss_G_gan: 5.547, loss_D: 0.302\n",
      "epoch: 402, loss_G_total: 10.035, loss_G_l1: 2.236, loss_G_gan: 5.564, loss_D: 0.305\n",
      "epoch: 403, loss_G_total: 7.056, loss_G_l1: 0.838, loss_G_gan: 5.380, loss_D: 0.415\n",
      "epoch: 404, loss_G_total: 7.077, loss_G_l1: 0.857, loss_G_gan: 5.364, loss_D: 0.406\n",
      "epoch: 405, loss_G_total: 6.963, loss_G_l1: 0.867, loss_G_gan: 5.229, loss_D: 0.370\n",
      "epoch: 406, loss_G_total: 9.425, loss_G_l1: 2.127, loss_G_gan: 5.171, loss_D: 0.427\n",
      "epoch: 407, loss_G_total: 8.953, loss_G_l1: 1.731, loss_G_gan: 5.491, loss_D: 0.315\n",
      "epoch: 408, loss_G_total: 9.915, loss_G_l1: 2.105, loss_G_gan: 5.705, loss_D: 0.308\n",
      "epoch: 409, loss_G_total: 7.538, loss_G_l1: 0.990, loss_G_gan: 5.559, loss_D: 0.442\n",
      "epoch: 410, loss_G_total: 9.258, loss_G_l1: 1.679, loss_G_gan: 5.901, loss_D: 0.317\n",
      "epoch: 411, loss_G_total: 7.889, loss_G_l1: 1.045, loss_G_gan: 5.800, loss_D: 0.445\n",
      "epoch: 412, loss_G_total: 10.191, loss_G_l1: 2.031, loss_G_gan: 6.130, loss_D: 0.390\n",
      "epoch: 413, loss_G_total: 9.862, loss_G_l1: 2.002, loss_G_gan: 5.858, loss_D: 0.378\n",
      "epoch: 414, loss_G_total: 9.003, loss_G_l1: 1.617, loss_G_gan: 5.768, loss_D: 0.339\n",
      "epoch: 415, loss_G_total: 7.634, loss_G_l1: 1.112, loss_G_gan: 5.410, loss_D: 0.436\n",
      "epoch: 416, loss_G_total: 7.489, loss_G_l1: 1.119, loss_G_gan: 5.251, loss_D: 0.429\n",
      "epoch: 417, loss_G_total: 8.315, loss_G_l1: 1.565, loss_G_gan: 5.184, loss_D: 0.308\n",
      "epoch: 418, loss_G_total: 6.933, loss_G_l1: 1.118, loss_G_gan: 4.698, loss_D: 0.391\n",
      "epoch: 419, loss_G_total: 7.341, loss_G_l1: 1.538, loss_G_gan: 4.266, loss_D: 0.388\n",
      "epoch: 420, loss_G_total: 6.388, loss_G_l1: 1.127, loss_G_gan: 4.134, loss_D: 0.413\n",
      "epoch: 421, loss_G_total: 6.354, loss_G_l1: 1.127, loss_G_gan: 4.100, loss_D: 0.391\n",
      "epoch: 422, loss_G_total: 6.343, loss_G_l1: 1.123, loss_G_gan: 4.097, loss_D: 0.390\n",
      "epoch: 423, loss_G_total: 6.705, loss_G_l1: 1.097, loss_G_gan: 4.511, loss_D: 0.372\n",
      "epoch: 424, loss_G_total: 7.881, loss_G_l1: 1.565, loss_G_gan: 4.750, loss_D: 0.341\n",
      "epoch: 425, loss_G_total: 6.808, loss_G_l1: 1.037, loss_G_gan: 4.735, loss_D: 0.351\n",
      "epoch: 426, loss_G_total: 6.824, loss_G_l1: 0.997, loss_G_gan: 4.829, loss_D: 0.346\n",
      "epoch: 427, loss_G_total: 6.828, loss_G_l1: 0.952, loss_G_gan: 4.924, loss_D: 0.342\n",
      "epoch: 428, loss_G_total: 6.807, loss_G_l1: 0.903, loss_G_gan: 5.001, loss_D: 0.340\n",
      "epoch: 429, loss_G_total: 6.749, loss_G_l1: 0.845, loss_G_gan: 5.058, loss_D: 0.335\n",
      "epoch: 430, loss_G_total: 6.720, loss_G_l1: 0.814, loss_G_gan: 5.092, loss_D: 0.330\n",
      "epoch: 431, loss_G_total: 6.645, loss_G_l1: 0.779, loss_G_gan: 5.087, loss_D: 0.327\n",
      "epoch: 432, loss_G_total: 6.766, loss_G_l1: 0.760, loss_G_gan: 5.246, loss_D: 0.375\n",
      "epoch: 433, loss_G_total: 9.434, loss_G_l1: 2.407, loss_G_gan: 4.620, loss_D: 0.638\n",
      "epoch: 434, loss_G_total: 5.630, loss_G_l1: 0.736, loss_G_gan: 4.157, loss_D: 0.645\n",
      "epoch: 435, loss_G_total: 8.898, loss_G_l1: 2.517, loss_G_gan: 3.865, loss_D: 0.694\n",
      "epoch: 436, loss_G_total: 5.265, loss_G_l1: 0.718, loss_G_gan: 3.830, loss_D: 0.558\n",
      "epoch: 437, loss_G_total: 9.101, loss_G_l1: 2.591, loss_G_gan: 3.919, loss_D: 0.465\n",
      "epoch: 438, loss_G_total: 9.326, loss_G_l1: 2.611, loss_G_gan: 4.105, loss_D: 0.395\n",
      "epoch: 439, loss_G_total: 5.720, loss_G_l1: 0.756, loss_G_gan: 4.207, loss_D: 0.417\n",
      "epoch: 440, loss_G_total: 5.917, loss_G_l1: 0.768, loss_G_gan: 4.380, loss_D: 0.431\n",
      "epoch: 441, loss_G_total: 6.046, loss_G_l1: 0.764, loss_G_gan: 4.518, loss_D: 0.447\n",
      "epoch: 442, loss_G_total: 6.136, loss_G_l1: 0.758, loss_G_gan: 4.620, loss_D: 0.464\n",
      "epoch: 443, loss_G_total: 10.258, loss_G_l1: 2.674, loss_G_gan: 4.911, loss_D: 0.465\n",
      "epoch: 444, loss_G_total: 6.132, loss_G_l1: 0.742, loss_G_gan: 4.648, loss_D: 0.467\n",
      "epoch: 445, loss_G_total: 6.034, loss_G_l1: 0.717, loss_G_gan: 4.600, loss_D: 0.463\n",
      "epoch: 446, loss_G_total: 5.781, loss_G_l1: 0.698, loss_G_gan: 4.384, loss_D: 0.430\n",
      "epoch: 447, loss_G_total: 9.674, loss_G_l1: 2.670, loss_G_gan: 4.335, loss_D: 0.443\n",
      "epoch: 448, loss_G_total: 4.002, loss_G_l1: 0.671, loss_G_gan: 2.660, loss_D: 0.530\n",
      "epoch: 449, loss_G_total: 5.835, loss_G_l1: 0.668, loss_G_gan: 4.499, loss_D: 0.418\n",
      "epoch: 450, loss_G_total: 6.296, loss_G_l1: 0.658, loss_G_gan: 4.980, loss_D: 0.447\n",
      "epoch: 451, loss_G_total: 6.585, loss_G_l1: 0.650, loss_G_gan: 5.285, loss_D: 0.522\n",
      "epoch: 452, loss_G_total: 10.678, loss_G_l1: 2.566, loss_G_gan: 5.546, loss_D: 0.651\n",
      "epoch: 453, loss_G_total: 6.581, loss_G_l1: 0.623, loss_G_gan: 5.336, loss_D: 0.523\n",
      "epoch: 454, loss_G_total: 6.460, loss_G_l1: 0.612, loss_G_gan: 5.236, loss_D: 0.496\n",
      "epoch: 455, loss_G_total: 6.307, loss_G_l1: 0.606, loss_G_gan: 5.095, loss_D: 0.468\n",
      "epoch: 456, loss_G_total: 9.356, loss_G_l1: 2.156, loss_G_gan: 5.043, loss_D: 0.443\n",
      "epoch: 457, loss_G_total: 9.144, loss_G_l1: 2.142, loss_G_gan: 4.860, loss_D: 0.427\n",
      "epoch: 458, loss_G_total: 8.911, loss_G_l1: 2.117, loss_G_gan: 4.677, loss_D: 0.418\n",
      "epoch: 459, loss_G_total: 5.806, loss_G_l1: 0.684, loss_G_gan: 4.438, loss_D: 0.415\n",
      "epoch: 460, loss_G_total: 5.705, loss_G_l1: 0.702, loss_G_gan: 4.302, loss_D: 0.413\n",
      "epoch: 461, loss_G_total: 8.349, loss_G_l1: 2.053, loss_G_gan: 4.242, loss_D: 0.415\n",
      "epoch: 462, loss_G_total: 8.223, loss_G_l1: 2.033, loss_G_gan: 4.158, loss_D: 0.418\n",
      "epoch: 463, loss_G_total: 5.499, loss_G_l1: 0.716, loss_G_gan: 4.068, loss_D: 0.420\n",
      "epoch: 464, loss_G_total: 5.496, loss_G_l1: 0.724, loss_G_gan: 4.047, loss_D: 0.422\n",
      "epoch: 465, loss_G_total: 5.479, loss_G_l1: 0.715, loss_G_gan: 4.049, loss_D: 0.423\n",
      "epoch: 466, loss_G_total: 5.484, loss_G_l1: 0.708, loss_G_gan: 4.069, loss_D: 0.422\n",
      "epoch: 467, loss_G_total: 9.075, loss_G_l1: 2.473, loss_G_gan: 4.130, loss_D: 0.425\n",
      "epoch: 468, loss_G_total: 9.116, loss_G_l1: 2.469, loss_G_gan: 4.178, loss_D: 0.422\n",
      "epoch: 469, loss_G_total: 5.676, loss_G_l1: 0.734, loss_G_gan: 4.208, loss_D: 0.416\n",
      "epoch: 470, loss_G_total: 5.737, loss_G_l1: 0.733, loss_G_gan: 4.272, loss_D: 0.414\n",
      "epoch: 471, loss_G_total: 5.754, loss_G_l1: 0.708, loss_G_gan: 4.338, loss_D: 0.411\n",
      "epoch: 472, loss_G_total: 5.762, loss_G_l1: 0.679, loss_G_gan: 4.405, loss_D: 0.410\n",
      "epoch: 473, loss_G_total: 5.789, loss_G_l1: 0.661, loss_G_gan: 4.468, loss_D: 0.409\n",
      "epoch: 474, loss_G_total: 5.799, loss_G_l1: 0.637, loss_G_gan: 4.525, loss_D: 0.408\n",
      "epoch: 475, loss_G_total: 5.825, loss_G_l1: 0.624, loss_G_gan: 4.576, loss_D: 0.408\n",
      "epoch: 476, loss_G_total: 9.825, loss_G_l1: 2.567, loss_G_gan: 4.690, loss_D: 0.398\n",
      "epoch: 477, loss_G_total: 5.906, loss_G_l1: 0.618, loss_G_gan: 4.670, loss_D: 0.408\n",
      "epoch: 478, loss_G_total: 5.949, loss_G_l1: 0.620, loss_G_gan: 4.710, loss_D: 0.409\n",
      "epoch: 479, loss_G_total: 6.006, loss_G_l1: 0.631, loss_G_gan: 4.744, loss_D: 0.409\n",
      "epoch: 480, loss_G_total: 6.046, loss_G_l1: 0.636, loss_G_gan: 4.774, loss_D: 0.409\n",
      "epoch: 481, loss_G_total: 10.151, loss_G_l1: 2.633, loss_G_gan: 4.886, loss_D: 0.395\n",
      "epoch: 482, loss_G_total: 6.053, loss_G_l1: 0.614, loss_G_gan: 4.825, loss_D: 0.409\n",
      "epoch: 483, loss_G_total: 9.351, loss_G_l1: 2.207, loss_G_gan: 4.938, loss_D: 0.386\n",
      "epoch: 484, loss_G_total: 9.340, loss_G_l1: 2.190, loss_G_gan: 4.960, loss_D: 0.385\n",
      "epoch: 485, loss_G_total: 9.311, loss_G_l1: 2.164, loss_G_gan: 4.982, loss_D: 0.383\n",
      "epoch: 486, loss_G_total: 10.072, loss_G_l1: 2.535, loss_G_gan: 5.002, loss_D: 0.386\n",
      "epoch: 487, loss_G_total: 6.230, loss_G_l1: 0.652, loss_G_gan: 4.926, loss_D: 0.407\n",
      "epoch: 488, loss_G_total: 6.261, loss_G_l1: 0.662, loss_G_gan: 4.937, loss_D: 0.406\n",
      "epoch: 489, loss_G_total: 6.266, loss_G_l1: 0.661, loss_G_gan: 4.943, loss_D: 0.405\n",
      "epoch: 490, loss_G_total: 6.261, loss_G_l1: 0.659, loss_G_gan: 4.943, loss_D: 0.405\n",
      "epoch: 491, loss_G_total: 9.922, loss_G_l1: 2.444, loss_G_gan: 5.033, loss_D: 0.376\n",
      "epoch: 492, loss_G_total: 9.080, loss_G_l1: 2.023, loss_G_gan: 5.034, loss_D: 0.370\n",
      "epoch: 493, loss_G_total: 9.042, loss_G_l1: 2.002, loss_G_gan: 5.038, loss_D: 0.369\n",
      "epoch: 494, loss_G_total: 6.383, loss_G_l1: 0.714, loss_G_gan: 4.954, loss_D: 0.402\n",
      "epoch: 495, loss_G_total: 9.795, loss_G_l1: 2.371, loss_G_gan: 5.053, loss_D: 0.371\n",
      "epoch: 496, loss_G_total: 9.749, loss_G_l1: 2.342, loss_G_gan: 5.065, loss_D: 0.369\n",
      "epoch: 497, loss_G_total: 6.525, loss_G_l1: 0.771, loss_G_gan: 4.982, loss_D: 0.400\n",
      "epoch: 498, loss_G_total: 9.640, loss_G_l1: 2.274, loss_G_gan: 5.091, loss_D: 0.366\n",
      "epoch: 499, loss_G_total: 8.802, loss_G_l1: 1.847, loss_G_gan: 5.108, loss_D: 0.361\n",
      "epoch: 500, loss_G_total: 8.751, loss_G_l1: 1.811, loss_G_gan: 5.129, loss_D: 0.358\n"
     ]
    }
   ],
   "source": [
    "# 20 seq len fixed, no embedding\n",
    "TGen = ToyGenerator()\n",
    "D = Discriminator()\n",
    "ganloss_G = nn.BCELoss()\n",
    "l1loss_G = nn.L1Loss()\n",
    "lambda_G = 2\n",
    "ganloss_D = nn.BCELoss()\n",
    "optimG = torch.optim.Adam(TGen.parameters(), lr = 0.01)\n",
    "optimD = torch.optim.Adam(D.parameters(), lr = 0.01)\n",
    "epochs = 500\n",
    "loss_G_l1_list = []\n",
    "loss_G_gan_list = []\n",
    "loss_D_list = []\n",
    "for epoch in range(epochs):\n",
    "    data_idx = np.random.randint(5) # len(td)) # get random sample\n",
    "    labels = td[data_idx]['lossmask'] # get lossmask (20, 32)\n",
    "    groundtruth = torch.cat((td[data_idx]['train'], td[data_idx]['groundtruth']), dim = 0)  # groundtruth (20, 32, 2)\n",
    "    \n",
    "    '''Train generator'''\n",
    "    loss_G = 0\n",
    "    optimG.zero_grad()\n",
    "    z = torch.randn(32, 32, 32)\n",
    "    prediction = TGen(z)\n",
    "    out_D_pred = D(prediction)\n",
    "    loss_G_l1 = l1loss_G(prediction[:,  0:td.peds_per_seq[data_idx]], groundtruth[:,  0:td.peds_per_seq[data_idx]]) # l1loss\n",
    "    loss_G_l1_list.append(loss_G_l1)\n",
    "    loss_G_gan = ganloss_G(out_D_pred.squeeze()[:, 0:td.peds_per_seq[data_idx]], labels[:, 0:td.peds_per_seq[data_idx]]) # ganloss\n",
    "    loss_G_gan_list.append(loss_G_gan)\n",
    "    loss_G = lambda_G*loss_G_l1 + loss_G_gan\n",
    "    loss_G.backward(retain_graph=True)\n",
    "    optimG.step()\n",
    "\n",
    "    '''Train discriminator'''\n",
    "    optimD.zero_grad()\n",
    "    loss_D = 0\n",
    "    out_D_real = D(groundtruth)\n",
    "    out_D_fake = D(prediction)\n",
    "    loss_D += ganloss_D(out_D_real.squeeze()[:, 0:td.peds_per_seq[data_idx]], labels[:, 0:td.peds_per_seq[data_idx]]) # on real\n",
    "    loss_D += ganloss_D(out_D_fake.squeeze()[:, 0:td.peds_per_seq[data_idx]], ((labels-1)*(-1))[:, 0:td.peds_per_seq[data_idx]]) # on fake\n",
    "    loss_D_list.append(loss_D)\n",
    "    loss_D.backward(retain_graph=True)\n",
    "    optimD.step()\n",
    "\n",
    "    print(\"epoch: %d, loss_G_total: %1.3f, loss_G_l1: %1.3f, loss_G_gan: %1.3f, loss_D: %1.3f\"%(epoch+1, loss_G, loss_G_l1, loss_G_gan, loss_D))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.randn(32, 32, 32)\n",
    "predicted = TGen(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[7.2274, 7.6265],\n",
       "         [8.3665, 8.2747]], grad_fn=<SliceBackward>),\n",
       " tensor([[10.3100,  5.9700],\n",
       "         [12.4900,  6.6000]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted[0, 0:td.peds_per_seq[data_idx]], groundtruth[0,0:td.peds_per_seq[data_idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f99d48fbef0>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4xLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvDW2N/gAAIABJREFUeJzsnXd4lGXW/z/PlGRSSAJJKEmA0EMPEIo0QUFQERtW1rIW7K64ttV1l3Xfn++ra2V1VewdV5qKlS5ITehIhwQCBAKk1yn37497WpJJMoFMZpK5P9fF9WSeeWbmTIDvnPne5z5HE0KgUCgUiuaDzt8BKBQKhaJhKOFWKBSKZoYSboVCoWhmKOFWKBSKZoYSboVCoWhmKOFWKBSKZoYSboVCoWhmKOFWKBSKZoYSboVCoWhmGHzxpHFxcSI5OdkXT61QKBQtkoyMjNNCiHhvrvWJcCcnJ5Oenu6Lp1YoFIoWiaZpWd5eq6wShUKhaGYo4VYoFIpmhhJuhUKhaGb4xONWKBTeYTabyc7Opry83N+hKJoIk8lEUlISRqPxnJ9DCbdC4Ueys7Np1aoVycnJaJrm73AUPkYIwZkzZ8jOzqZLly7n/DzKKlEo/Eh5eTmxsbFKtIMETdOIjY09729YSrgVCj+jRDu4aIy/byXcgcSJbZCt6t8VCkXdKOEOJJb9E358wt9RKIKMkydPcvPNN9O1a1eGDBnCBRdcwMKFC/0Wz8qVK1m7du15P8eUKVNqnH/jjTfo3r07mqZx+vTp83oNf6KEO5CwVkB5gb+jUAQRQgiuuuoqxo4dy6FDh8jIyGDu3LlkZ2f79HUtFkut952LcNf1fO6MGjWKpUuX0rlz5wY9f6ChhDuQsNmgosjfUSiCiOXLlxMSEsK9997rPNe5c2ceeughAKxWK48//jhDhw5lwIABvPPOO4AU13HjxjFt2jRSUlKYPn06QggAMjIyuPDCCxkyZAiTJk3ixIkTAIwbN46nn36aCy+8kNdff53vvvuO4cOHM2jQICZMmMDJkyfJzMzk7bff5tVXXyU1NZXVq1eTlZXFxRdfzIABA7j44os5cuQIALfffjuPPvoo48eP58knn/Tq/Q4aNIiW0Eep3nJATdN6AV+5neoK/E0I8VpjB3PgVBHJsREY9EH6eSKsUFHs7ygUfuIf3+3i9+OFjfqcfRKi+PsVfWu9f9euXQwePLjW+99//32io6PZtGkTFRUVjBo1iksuuQSALVu2sGvXLhISEhg1ahS//fYbw4cP56GHHuKbb74hPj6er776imeeeYYPPvgAgPz8fFatWgVAXl4e69evR9M03nvvPV588UVefvll7r33XiIjI3nssccAuOKKK7j11lu57bbb+OCDD3j44YdZtGgRAPv27WPp0qXo9fpG+X01F+oVbiHEXiAVQNM0PXAMaHQDLK+kkmlvr6NvQhT/vmkwbSJCGvslAh+bFcwl8qgLrn+IisDggQceYM2aNYSEhLBp0yZ++eUXtm/fzrx58wAoKChg//79hISEMGzYMJKSkgBITU0lMzOTmJgYdu7cycSJEwGZsXfo0MH5/DfccIPz5+zsbG644QZOnDhBZWVlrXXN69atY8GCBQDccsstPPGEax3ouuuuCzrRhoZvwLkYOCiE8LqLlbe0jgjhmct688yinUx9Yw2f3zWczrERjf0ygcPp/TD3ZvjjTxARK88JqzxWFoMp2n+xKfxCXZmxr+jbty/z58933n7zzTc5ffo0aWlpgPTA//3vfzNp0qQqj1u5ciWhoaHO23q9HovFghCCvn37sm7dOo+vFxHh+j/90EMP8eijjzJ16lRWrlzJrFmzvIrZvZzO/fmCiYZ6EjcCX/oiEIDr0jry33suoLjCwo1z1pN1psRXL+V/Tv0Op/dBfqbrnM0u3MouUTQRF110EeXl5bz11lvOc6Wlpc6fJ02axFtvvYXZbAakNVFSUvv/y169epGbm+sUbrPZzK5duzxeW1BQQGJiIgAff/yx83yrVq0oKnKt9YwcOZK5c+cC8PnnnzN69OiGvs0Wh9fCrWlaCDAV+LqW+2dompauaVp6bm7uOQeU2jGGL+4aQbnZyr2fbcZstZ3zcwU0VnPVI7gJt1qgVDQNmqaxaNEiVq1aRZcuXRg2bBi33XYbL7zwAgB33XUXffr0YfDgwfTr14977rmnzgqOkJAQ5s2bx5NPPsnAgQNJTU2ttUJk1qxZXHfddYwZM4a4uDjn+SuuuIKFCxc6Fydnz57Nhx9+yIABA/j00095/fXXvXpvy5YtIykpyfln3bp1zJ49m6SkJLKzsxkwYAB33XVXA35bgYPmWAmu90JNuxJ4QAhxSX3XpqWlifMdpPDLrhxmfJrBIxN68MiEnuf1XAHJtrmw8B649VvoeqE8958LZCZ+1zJISvNvfIomYffu3fTu3dvfYSiaGE9/75qmZQghvPqP3xCr5CZ8aJNU55K+7bkqNYE3lh/g8OkWaJnY7FmLx4y7cSsLFApFy8Ir4dY0LRyYCCzwbThVefry3uh1Gv9ZcaApX7ZpcFolla5zQnncCoWifrwSbiFEqRAiVgjRpNv62rYycdOwTizYcoyjZ0vrf0Bzwplxuwm3za2qRKFQKGoh4He63HthN/SaxturDvo7lMbFk1Ui1OKkQqGon4AX7vbRJq4alMCCzccoKDXX/4DmgtMqqXCds9kraJRwKxSKOgh44Qa49YJkysxWvs446u9QGg9bXR63Em6FQlE7zUK4+yVGk9a5NZ+sy8Jm8658MWAQAsryap53+NmeqkqUx61oQlpqW9fo6GhSU1MZMGAAEyZM4NSpU877f/zxR9LS0ujduzcpKSnOviizZs0iMTGR1NRU55/8/PzzisUXNAvhBrhtZDJHzpayct+p+i8OJDa9By+nwJlqHr2nqhKbBSvwXuEeSs0tbDFWEZC05LauY8aMYevWrWzfvp2hQ4fy5ptvArBz504efPBBPvvsM3bv3s3OnTvp2rWr83EzZ85k69atzj8xMTENiqUpaDbCPblfe9q2CuXjtY3eJsV3CAEb54ClHJbOqnpfLVbJ3hAjr1ceYf2J9U0WpiJ4CYa2rkIIioqKaN26NQAvvvgizzzzDCkpKQAYDAbuv//+Rv7N+pZmM+XdqNcxfXhnXl26j8OnS+gS1wyayxxZJ/uRtO8Pu7+Foxuh4zB5n6OqxOKecduotDfQsdi8awyvaEH8+BTk7Gjc52zfHy79v1rvbsltXVevXk1qaipnzpwhIiKC559/HpAZ95///Oda3/Orr77KZ599BkDr1q1ZsWJFrdf6i2Yj3AA3De/IGyv288m6TL90UmswGR9DaBT8YSH8ewhs+cwl3FYPddzCilknhdsmWmiPFkVA05Lauo4ZM4bFixcD8MILL/DEE0/w9ttv1/s7mDlzpvNDI1BpVsLdtpWJSX3bMz8jmycnp2AyBnAfXiFg93fQfxpExkN0IpSecd3vySqxWbHY35JFqIw76KgjM/YVwdLWderUqVx77bWAfM8ZGRkMHDjQq8cGIs3G43Zw07BOFJZb+HlXjr9DqZviU3IoQvv+8nZYayhzW52uZQOOWVMZt6LpCJa2rmvWrKFbt24APP744zz//PPs27cPAJvNxiuvvNLg5/QnzSrjBrigaywd24Qxd+NRrkxN9Hc4tZMvF1CI6SSPphjId1tY9WSV2KxYNKM87SgNVCh8iKOt68yZM3nxxReJj48nIiKiSlvXzMxMBg8ejBCC+Ph4p7/sCUdb14cffpiCggIsFguPPPIIffvWtDYdbV0TExMZMWIEhw8fBqSnPW3aNL755hv+/e9/M3v2bO644w7+9a9/ER8fz4cffujVe3N43EIIoqOjee+99wAYMGAAr732GjfddBOlpaVomsbll1/ufJy7xw2waNGigJtT6XVb14bQGG1d6+KN5ft56Zd9rHxsHMmBuki5Yx7MvxPuXw9te8OiB+DQCnj0d3n/ghmw/StInQ5X/UdaK/+I4efwcB5rF8ffhj/LdSnX+/c9KHyOausanDRlW9eAYdqQjug0+G96AO+kdGTc0R3lMSymFqvEnnHbM2xLSLj97rKmiFKhUDRDmqVwt482Mb5XW77OyMYSqBNy8o9AeCyERsrbYTHS87bYe5NU34BjF3KzXbgtagOOQqGohWYp3AA3DutEblEFK/ae+5g0n5J/xJVtg/S4wZV1V1+ctPcpsRjD7Kdb4PAIhULRKDRb4R7fK562rUL5atMRf4fimYKjroVJkFUlAOXVhbuqVWK2C7fNXN4UUSoUimZIsxVug17HtCFJLN9zipyCABM5IWTGXUW4HRm3veGUI9N2WCfOjNskjxZllSgUCs80W+EGuD6tIzYB8zf7tiFOgynJlf1JYjq7zjky7tqsEnsvbrMh1H46wD6MFApFwNCshTs5LoILusYyd9ORwGr3Wr2GG9w87moZt8MqcWTcOllab7O1oKERioBGr9eTmppK3759GThwIK+88go2eyKRnp7Oww8/fN6v8fbbb/PJJ5806DEjR44859f76KOPOH78+Dk/Hqq2eE1JSeG+++5z/l4AXnrpJVJSUujXrx8DBw50vr9x48bRq1cvZ1vYadOmnVccnvBqA46maTHAe0A/QAB3CCE872ltYm4c1pE/zd3KukNnGNU9zt/hSBwbbbzyuB0Zt93j1sk97xYl3IomIiwsjK1btwJw6tQpbr75ZgoKCvjHP/5BWlqac/v7uWKxWKp0H/SW8+nJ/dFHH9GvXz8SEhK8fozVaq3R98TRt8RmszF27FhWrVrF+PHjefvtt1myZAkbN24kKiqKgoKCKhuTPv/88/P+vdWFtxn368BPQogUYCCw22cRNZBJfdsTHWZk7qYAqunOt8cS415VEg1oroy7eq8SZ8Yt/+HYrKpXiaLpadu2LXPmzOGNN95ACMHKlSuZMmUKAKtWrXJmkYMGDXJuS3/xxRfp378/AwcO5KmnngJqtnCdNWsWL730kvO+mTNnMnbsWHr37s2mTZu45ppr6NGjB3/961+dsURGylLaulrIPvfccwwdOpR+/foxY8YMhBDMmzeP9PR0pk+fTmpqKmVlZSxbtoxBgwbRv39/7rjjDioq5NpScnIyzz33HKNHj+brr7+u9fdSWVlJeXm5szXs888/z3/+8x+ioqIAiI6O5rbbbmu0v4f6qDfj1jQtChgL3A4ghKgEKut6TFNiMuq5elAiX2w4Ql5JJa0jQvwdEpQXgM4Aoa1c53R6MEW5WSWOjNu+OOnIuO29Sqwq4w46Xtj4AnvO7mnU50xpk8KTw2rvVe2Jrl27YrPZqkyMAWkNvPnmm4waNYri4mJMJhM//vgjixYtYsOGDYSHh3P27Fnn9e4tXKs3kAoJCeHXX3/l9ddf58orryQjI4M2bdrQrVs3Zs6cSWxsbJXrPbWQHT16NA8++CB/+9vfANk5cPHixUybNo033niDl156ibS0NMrLy7n99ttZtmwZPXv25NZbb+Wtt97ikUceAcBkMrFmzRqPvwvH9vesrCwuvfRSUlNTKSoqoqioyNn7xBPTp08nLExWiE2cOJF//etfXvzmvcebjLsrkAt8qGnaFk3T3tM0rcY+c03TZmialq5pWnpubtPWVt8wtCOVVhsLtxxr0tetFXMZGMNrnndvNFVbHbfTKlEZt8J/eGqFMWrUKB599FFmz55Nfn4+BoOBpUuX8sc//pHwcPnvvU2bNs7r3Vu4Vmfq1KkA9O/fn759+9KhQwdCQ0Pp2rUrR4/W/PbsaCGr0+mcLWQBVqxYwfDhw+nfvz/Lly/32NBq7969dOnShZ49ewJw22238euvv3oVp2MazqlTpygpKWHu3LkIIap0KPTE559/7pyg09iiDd553AZgMPCQEGKDpmmvA08Bz7pfJISYA8wB2auksQOti94dohiYFM1Xm47yx1HJ9f5SfY65xLNwm2Jqt0qqZdxqcTL4aGhm7CsOHTqEXq+nbdu27N7tckWfeuopLr/8cn744QdGjBjB0qVL6xSxulquOlrC6nS6Ku1hdTqdxzFknlrIlpeXc//995Oenk7Hjh2ZNWsW5eU1q7Hq68fkTWtYo9HI5MmT+fXXX7nxxhuJiIjg0KFDVUaeNSXeZNzZQLYQYoP99jykkAcU16V1ZO/JInYeK/R3KFBZCiG1ZNyOxcnq3QEdvUrUBByFH8nNzeXee+/lwQcfrCHIBw8epH///jz55JOkpaWxZ88eLrnkEj744ANnK1h3q8TXOEQ6Li6O4uJi57AHqNoaNiUlhczMTA4cOADAp59+yoUXXtig1xJCsHbtWqc98pe//IUHHniAwkKpN4WFhcyZM+e835O31JtxCyFyNE07qmlaLyHEXuBi4Hffh9YwrhiYwHOLf2dexlH6J0X7N5harZIYuaMSarVKzMhyI6sSbkUTUVZWRmpqKmazGYPBwC233MKjjz5a47rXXnuNFStWoNfr6dOnD5deeimhoaFs3bqVtLQ0QkJCuOyyy5wjwnxNTEwMd999N/379yc5OZmhQ4c677v99tu59957CQsLY926dXz44Ydcd911WCwWhg4d6nWVi8PjNpvNDBgwwDmb8r777qO4uJihQ4diNBoxGo1VxqG5e9xxcXEsXbq0Ed+5l21dNU1LRZYDhgCHgD8KIfJqu97XbV1r46Evt7B6fy4bnr6YUIMfp+N8cqXMuu9aUvX84pnw+zfwxCH4V3e5UUfTw9/Pwont8M4YnhgyhR/PbueasE784/rv/RO/oslQbV2DkyZp6yqE2CqESBNCDBBCXFWXaPuTaUOSyC81s3z3qfov9iW1WSUme2tXIapm2jarK+O2T75RVolCoaiNZr1zsjqjusUSFxnC4h0n/BtIXVUlwgoVRU5PG5Aibt+RZbFbJTahJuAoFArPtCjhNuh1TOrbnuW7T1FW2UTCZ6mEeXfA8S2uc7VVlTgaTZXny6oSzW7nWCtrZNxqdFnw4IspVIrApTH+vluUcANc3r8DZWYrK/Y2kV2SswN2zodDq1znarNKHGJuLpNZdoi9DMlqdlWViKpHRcvGZDJx5swZJd5BghCCM2fOYDKZzut5mt2w4PoY1qUNsREhfL/jBJf17+D7FzxmX4StcE2lrtUqMZhc9wsrGMOgolDunqyWcSurJDhISkoiOzubpt60pvAfJpOJpKSk83qOFifcBr2Oyf3aM39zNsUVFiJDffwWj2XIY4Vb/XhtVolDuCvt022cGXelszzQkWlbRYCOZFM0KkajkS5duvg7DEUzo8VZJQDXDE6k3Gzjh6ZYpMyulnFbzVKEPVkl9l7bVBbLo7GmVWJWVolCoaiHFincgzu1pktcBPMzfDxgofQsnD0of3YItyOb9pRx28eSuYTbfttaCfYM22WVqIxboVB4pkUKt6ZpXDMokQ2Hz3L0rA9HgB3fLI86o+wICOCYzu7RKrFn3BV24XZk5dbKGouTyipRKBS10SKFG+DqwYkAvu0YmJ0BaJA01JVxm8vkMcRD4xqnx13NKrG4lwMq4VYoFHXTYoU7qXU4F3SNZcHmbN+VWp36HVonQ1SCa3HSaZWE1bzei4zbbHMItyoPUygUnmmxwg1ykTLzTCkZWT7aoZ9/RAq3Kcot467LKnFk3EVVr3HbgOO0SlAZt0Kh8EyLFu5L+3cgzKhv2BT4vEzY+5NzC3qd5GdB685y0k25PeP2RridGbeHqhJ7WaDKuBWKZoDNJjVj38/w22xY8b9N8rItro7bnchQA5f2a8/i7Sf4+xV9MRnr6Bhos8Lc6bDvR3n70n/B8Bm1X19RDKVnIKazrAixVoClQu6ahFrKAavVcXuoKrEIu3CjhFuhCBisZjh7CHL3wum98pi7F07vB0uZ67qYzjDuKfDxMJcWLdwAVw1KZMGWY6zYc4pL69pJue4NKdqjH5XVIktnQY+J0KaWzRGOSe6tO0PJGflzRZFbxu1hcVJvBDS3xUl3j1sKtsPjtijhVij8Q1ke5OyU7SxO2o+5e1xDTwCiO0J8L0geA/E9IT4F4npCeJvan7cRafHCPbJbLHGRoXyz9Xjtwp27F5b/D/S+Ai7+GxQegzdHwJJn4YbPPD8mzy7cMcmyKgTkAqVTuD0sTmqazLodfniVnZMyx3Zk3DYl3AqF7yk5IxO1YxlwfKsU6UI3azUiHtr1g+H3QNu+UqzjekJopP9iJgiE26DXccXADny+/ggFZWaiw4w1L/pttqzFnvKaFNfoJBg0HdI/lJaIp78k94y7OEf+XF5Yt1UCsrLEU8YNuHfgtiJk325/z89UKFoKlSVwYpsU6WMZcGyz6/8xmhTkTiOgfX9o3w/a9YdW7fwacm20eOEGuCo1kQ9/y+SnnSe4YWinqneWnoWd82DgTRAR5zqfcjlseBsOLoc+U2s+aV6WtEPCY+XiJNitEod/XcsAUmOY58VJTeccFAxgBWmf6D180CgUivopy4Mj6yHrN8haKzNqRyuJ6E6QOAiG3gkJgyEh1fX/uBkQFMI9ICmaLnERLNpyvKZwb/0cLOUw9K6q5zuNlIMP9nzvWbgdFSWaBqFR8lxFodyAo+lrF9zaMm59CBa35NqqaXKxUwm3QuEdxblwZC1k2oX65E5AgD4EEofA6EcgaRgkDobItv6O9rzwSrg1TcsEipCJoMXbuWiBgqZpTB2YwOzl+8kpKKd9tL26Qwhph3S6QH41ckdvgJ6Xwt7vZUZcXUDzsiDG/iHgnnFXlspMujaLw2ByLWY6fHBLBWh6zFTLuN0XQxQKRVVKzkDmajj8K2SukdUeAIYw6DgMxv0FkkdJ0fa05tSMaUjGPV4IcdpnkfiYqwYl8vqy/Xy37Th3j+0qT+bskE2iRj3s+UEpl8G2L+TXrS5jXOeFkBl38mh522SfKl9eWHtLVwfuGbe7VWKwYrGLvQZY0ZRwKxTulOXLTPrwr1KwT+6U540R0PkCSL0JOo+CDqlgCPFvrD4mKKwSgC5xEQxMimbR1mMu4d79LWg6SJni+UEOYa4u3KVnpfi27ixvOzNuu1VS16e7wQSOihFDqLRV7FUlZnvCbdIZsWpWmYkrFMFKRbH8v3d4lRTqE9vkfgeDCToOh4v+Cl0uhIRBQWcpeivcAvhF0zQBvCOEmOPDmHzGVYMS+cd3v7Mnp5CU9lHw+7fyE9p9UdKdsNYQ3xuOrq96Pj9THmPswm0IBX2oFG6HVVIbBreRRTqD9N/sW94tdqvEpDNioUJl3IrgQghZL71/CRxYAlnr5GxWnVE2chv7hEygkoa6+v4EKd4K9yghxHFN09oCSzRN2yOE+NX9Ak3TZgAzADp16uTpOfzOlamJPP/Dbr5Oz+bZ4XrpiVVflKxOp+Gwc6Hc2qqzdwjIPyKPjowbZNbt2IBTp1XiLtxGu3DL4QuOqpJQnZEKDSXcipZPRZG0Pvb/AgeWQcFReT6+N4y4F7pdBB1H1F5eG6R4JdxCiOP24ylN0xYCw4Bfq10zB5gDkJaWFpC7R9pEhDChdzsWbjnGX1ptk2++dy02iYOOIyDjI8jdDe36ynPOzTduwm2KsnvcpfVYJW6Zgt4gv+JZK8Bmc1aVmHQh5IGyShQtk8ITctF/z/dweLXMqkMioes4GPNn6D4BYjr6O8qApl7h1jQtAtAJIYrsP18CPOfzyHzE9Wkd+XFnDoU7fqRN+/6yJWtddBohj0fWuYQ7P0vaKKYo13WOjLuyVNZ210YdVokj4zbpQ2Q5oMq4FS2F0wdgz3dSrLM3yXNtusmsusclMkFq4QuKjYk3GXc7YKEmRcUAfCGE+MmnUfmQMT3i6BJpIfr0Zhgzs/4HtE6GyHZwZIPLVsnLqpptg6zldmx5r6+qxIHOKP+x2rsDWjRpxYTqQ2U5oMq4Fc0VIWS/+p0LYM9i6V2DrPi46K+yICA+Re0MPkfqFW4hxCFgYBPE0iQY9DruScpCn2mjPPkiTPU9QNNkTagjSwCZcTuybwehUfJ8fVaJ+316Y9WMWye7F5r0oQhNw2apaNl9dxUtjzMHpVjvnCfFWtPJAoAhf5S7kZUF0igETTmgOxfpt1EgwvmtqDOXefOAhMGw+ztZBmiKkYuTvao90tGTu96qEveMW++2OGnFYhfuUL28xmopV8KtCHwKsmHXQtgxD05slec6jYTLX4beV0JkvH/ja4EEn3ALQfzJNSzVDeT733O5LNWLDCBxsDwe3wJte8sMuXU1q8QUBSWnpAg3qKrEKC0RYcPssErs11gtZQRXdaqi2VB6FnbOl9n1kbXyXMIguOR/oO/VslGbwmcEn3Dn7EArzqGw4x9ZvvsUpZUWwkPq+TUkDJLHY5tdolzd4+59BRxYKputt6qj73eVqhI3q8Qt4zbpHcKtPG5FAGG1wMFlsOUz2PujrAaJT4Hxf4V+10BsN39HGDQEn3AfWAJAl+FXUrb/ED/uyOHaIfVkB6ZoiO0h+/Y6PLrqwt1lLDy0Wdah1inc9VeVhNp9cKu1vEFvTaHwCbn7YOtnsO0r2cI4PBaG3Q2pN8te1WqBsckJPuHevxTaD2BQ3xSSY3P4OuNo/cIN0i45tEquioOrwZQ7mub5vDtO4dakx20IhdISsFlcGbfBLtxmJdwKP1FeIG2QrZ/LhXlNL8v2Bk2HHpNU6Z6fCS7hLsuHoxtg9CNomsa0IUm89Ms+jpwppVNsPTuzEgbD9q+krxfZHoz11qN4xiHcjt4K+lC3XiUOj9uRcSurRNGECAHZ6ZD+vlxstJRLK2TiP2HADQE7VCAYCS7hPrRCNlLvPhGAawYn8fKSfczLOMqjl/Sq+7GdhstjcQ5MmHXuMTiEW2f/1RtC7IuTViz2LfWh9iEMSrgVTUJliawI2fQe5GyXuxgH3gSDbpHfNJUVEnAEl3Af/hVCWskmNUBCTBiju8excOsxZk7siVbXP9CEQTBjJcT1Or++CY7FSZ0j43YsTrqqSsIcwq0WJxW+5PR+KdZbv4SKAmjbR5bwDbihWU2DCUaCS7iz1srMWe9621elJvLnr7ex+UgeQzrXM6HZUV1yPjitEnsM7t0BayxOKuFWNDJCyPYNa/8Ne3+QCUSfK+Wu4E4jVHbF1cMIAAAgAElEQVTdTAge4S49K3dy9b+uyulJ/drzzKIdLNxyrH7hbgyM1a2SUGmV2KyYdTrA5ioHVL1KFI2FzSo3ka2dLQflhrWWbVKH3d3sx3gFI8Ej3EfWyWPnkVVOR4YauKRPexZvP8HfpvQlxODjvYpOj7va4qSwOkeXuXZOqoxbcZ7YrHKhcdWLso1x6y5w2UuQOl21Sm3GBI9wZ62VtkTC4Bp3XT0okW+3HWfVvlwm9vHxyrnD49a7LU46qkp0GjpNh9FecaIybsU5U12w43vDtA+lLWIvO1U0X4JLuBPTPJbxje4RR2xECIu2HGsC4a5mlbjvnESHQdOh1+R/LCXcigYjhNzB+8uzsod8fIpdsK9yDQJRNHuCQ7htVjmvbuSDHu826nVcMTCBLzYeobDcTJTJhx1CPFWVAFjKMOvAqDcq4VacGzk74Je/wqGV0KYrTPsA+lytBLsFEhx/o1azrN92TGP3wFWDEqm02PhpR45vY7FvrnFZJXYhN5dhQcOgM6C3f5W12pRwK7ygKAcWPQBvj5EJyuT/g/s3QL9rlWi3UIIj4xZWedRq9/YGJkXTJS6ChVuOcf1QH/YMdmbcDqvEIdylmI0aRp17xm32XRyK5o/VApveheX/T46/G/mgHP0V1trfkSl8THAIt80ij7ra366maVyVmshry/ZxPL+MhJg6hiGcD9WrShw9H8xlWELOT7grLTZW789l3cEz7DpeSE5hOWWVVgx6jbjIUJJjw+mXGM3YnvH0aBtZ94YjRWBzdCMsfhRO7oBuF8Nl/1Ld+YKIIBFue8Zdz2r6VYMSeHXpPr7ddpx7L/TRfwJnVUk1j9tcjpmwalaJd8KdX1rJe6sP89mGLPJLzYQYdPRNiKJvQhThIXrMVkFuUQUbDp9l0dbj8P1uEmPCuLh3W65P60i/xNotJEWAUZYPS56FzZ9AVCJc/wn0nqo2zgQZXgu3pml6IB04JoSoZzR6gCFs8liHVQLQOTaCIZ1bs3DzMe4Z29U3GammSXvE8SHiFO5SLFp4tYy7bo9bCMG8jGz+ufh3CsstTO7bnhuGdmRk91hCDZ7f64mCMlbsyWX5nlP8N/0on6zLYlhyG24flcykvu3R65QABCz7foHvHobiUzDyIbjwKQiN9HdUCj/QkIz7T8BuIKq+CwMOp1VSf/3qVYMSeXbRTnafKKJPgo/eqtHkZpW4FifNGjLj1urPuEsrLTz+9Xa+33GCocmtee7KfvTuUH+8HaLDuHl4J24e3omCMjNfpx/lo7WZ3P/5Zvp0iOKvU3ozslvceb9FRSNSlgc/PQ3bvpD9RG78wjWVSRGUeLXkrGlaEnA58J5vw/ERXlolAFP6d8Cg01i09Zjv4jGYqrZ1BZlxg8y4HVaJ1eLx4WdLKrn+nXX8sPMET05O4asZF3gl2tWJDjNy15iurHp8PK/fmEpBmZmb393AjE/Syc4rPZd3pmhs9v0M/7lAthQe85hsdKZEO+jxtlboNeAJwObDWHyHF1UlDlpHhDCuV1u+2XoMq034Jh5DaNW2rjJILIBep3dl3I643cgrqeTmd9ez/2Qx79+Wxn3juqE7T3tDr9O4MjWRZX++kMcn9WLNgdNMfm01X206ghA++h0o6qYsDxbeB19cL6tE7l4GFz9bdfSdImipV7g1TZsCnBJCZNRz3QxN09I1TUvPzc1ttAAbBS+qSty5elAiJwsrWH/ojG/iiWwPEXY7Qu/6j2jVwKC5WSXVPifLzVbu+iSdQ6dLePfWNC5KadxdniajngfGd+fnR8bSLzGKJ+fv4M6P0zlVpCbxNCnuWfbYx2WW3RidKRUtBm8y7lHAVE3TMoG5wEWapn1W/SIhxBwhRJoQIi0+Pr6RwzxPbHYB9LJHw8W929Iq1MCCzT6yS27+Sk7DBtfiJGCBqlUlwiXcQgieXrCDjKw8Xr0+lbE9ffc77tgmnC/uGsHfr+jDbwdOM2X2GtIzz/rs9RR2yvJg4b1Vs+yL/qqybEUN6hVuIcRfhBBJQohk4EZguRDiDz6PrDER3nvcIDPPKQM78MOOExSW+2ATTHgbV6N6Q1Xh1mvuVolLuP+bfpQFW47xyIQeXD6gjmHEjYROp/HHUV345sFRhIfouXHOej5Yc1hZJ75i70/2LPu/KstW1Etw7Id1WCVeeNwObhzaiTKzlW+2HvdRUHaqWyUeMu7Dp0v4x3e/M7JbLA9f1MO38VQjpX0U3z40mvEpbXlu8e/8+b/bqLQ0z6WOgKTkNMy/C768QWXZCq9pkHALIVY2uxpuaFBViYMBSdH06RDFlxt8vEDnlnFbqbk4KYTgyfnbMeg0Xr5+4HkvRJ4LUSYj7/xhCDMn9GTBlmPc+sEGCkrVdvzzQgg55/HNYbBrEYz7C8xYpbJshVcEV8bt5eIkyC3wNw3vxO8nCtlxrMBHgVEl47Ygqi1OChZuOcbGw2d5+rLedIj20TZ8L9DpNP40oQev3ZDK5qx8rn7rN46cUSWD50T+UfjyRph/J7ROhnt+hXFPVfkQVyjqIjiE28udk9W5MjWBMKOeLzce8UFQdqotTup1ejerxMrzP+xmUKcYrk/zYeOrBnDVoEQ+vXMYZ4orufo/v7H5SJ6/Q2o+VJbCyv+DN9Lg0CqY9DzcuQTa9fF3ZIpmRnAIt9MqadjbjTIZmTKgA99uPU5xhefNMOdNFatEVFmcrLTZOF1cyXNT+/nFIqmN4V1jWXD/SCJCDdw0Zz0r9pzyd0iBjdUCW7+QtsjK/4Vel8GDm+CCB9Q0GsU5ESTC3XCrxMFNwztRUmnlu20+WqR0X5yk2pZ3IbgyNYH+SYHXBKpbfCQL7x9Jj3aRzPg0nR92nPB3SIGH1QxbPpcZ9qL75OLj7d/DdR9CTGB8g1I0T4JDuBuwc7I6gzrGkNK+FZ+tz/LNIqXeNW3HgsCgM6DT5F+LQPDYJb0a/zUbidjIUL64ewQDk2J48IvNzMvI9ndIgcGZg7Dkb/BKb/jmfln6eeOX0stOHu3v6BQtANXWtR40TeP2kck8tWAH6w6eYWT3Rm7A5OgWaK3AKqRVknm6DAChQceYmjMyA4kok5FP7hzGjE8yeOzrbZRWWrj1gmR/h9W02GxwYqvc8bj/Zzi+RSYJPSdD2h+h+wTVdlXRqASZcJ/b271qUCIv/bKPOasPNb5wg31gcAUWu8f98i8HAbBo2L8tBPYXo/AQA+/dlsZDX27hb9/sorjCwv3juvs7LN9iLpezHfd8J9utlpwCNEgaChNmwcCboFV7/8aoaLEEh3Cfh1UCcifl7SM789Iv+9ibU0Sv9q0aMTjkAmWlXJw8XWxh2e5TtOoNVjT5oaP34fDiRsJk1POf6YN57OttvPjTXgrKzDw1OaVlTdkRAjLXQPoHsP8XqCyG0CiZUfecJI8RqiWuwvcEdip3vlSWyv9s51hV4s704Z0JM+p5d/WhRgrODfsCpQVB+uEC2rYyoUejWKdx40+38vuZ3xv/NX2AUa/j1etTuWVEZ95ZdYinF+7wXYfFpiZrHbx3MXw8BQ6tgP7T4A/z4fGDcrFx4I1KtBVNRssV7rJ8eLkXRdu/4nCJveLhHK0SkO1ebxjakW+2HuNkYSN3y7OXBFqEjZOFZmZO7Ile03HYaGTX2d3NRrhBbtR57sq+PDi+O19uPMrDc7c07y3y5jL4/s/w4WQoPAFTXoNHd8MVr8sMW22aUfiBlivcOTugopBPj/zM7XvelefO0SpxcMeoLlhtgg9/yzz/+NyxZ9xWIWgdHsp1Q5LQo5Gvl389FdaKxn09H6NpGo9N6sUzl/Xm++0nuPuTdMoqa/YWD3gKT8D7E2HTezDiAXgoQy42Gv23g1WhgJYs3Cd3AlBkLaPIas+QzyPjBugUG86l/Trw+Yasxt2Qow/BhqwiGZYcj0GvQ6/pyLNXwZRZyhrvtZqQu8d25YVr+7N6fy63vL+BgrJm1N/k7CF4/xI4exhu/homPw8h4f6OSqEAgkC4LcLqao/aCLvUZoztSlG5ha82HT3v53JiCMGRj/bpEAOATtPIa6YZtzs3DO3EGzcPZlt2Pje8s46cgmYwlKHoJHx6tVx8vP176HmJvyNSKKrQcoU7xy7cNis2hJwlo53/2x3YMYZhXdrwwZrDmK2N490WmHVY7NUXoQZZQWJAT5l9MbXc0gzErg4u69+BD24fSnZeGVf/5zf25hT5O6TasVTIQQbFuTB9HiSk+jsihaIGLVO4rRY4tVv+aC8FtMJ5WyUOZozpyrH8skbZ5m222jh4ttKZcTu2u+vdPmSaq1Xizpge8Xx1zwisNsG0t9ey9uBpf4fkmV+elZtprn0Xkob4OxqFwiMtU7jPHgS7veAQboumNVpDn4tS2tItPoJ3Vx86723wn67LIq9Ch9WecRvsHy46N+FuzlaJO30Toln4wCjaR5m47YONfLPVR6PhzpX9S2DjOzDifki53N/RKBS10jKFO2eH80eLzS3jPs+qEgc6ncbdY7qy81gh6w6e+0Dh08UVvLp0H1GRETiW7RwZt8FNuJu7VeJOYkwY8+4dyaBOrfnT3K28vnQ/tsaq9baaYfXL8N0jzm9cXmMuhx8eh7iecuejQhHAtEzhPrlL2iIGExaHVaJpjWaVgNwGHxcZypzz2JDz4k97KKu0kpIY68y4Hb243TPucmvLEW6A6HAjn945jKsHJfLq0n3c+1nG+VfpVJbAh5fCsudkC9X/jICN73r/+HVvQN5huPQFNTZMEfC0TOE+ewhiOoPB5KwoMcN57ZysjmMb/Mq9uee02LbtaD7/Tc/mjtFdaBURgdW+M9ygyQ8Xvdu3g5aUcTsINeh55fqBPDulD8v2nOLqN3/j8OmSc3/CPd9D9ia48k348x7Z4OmHx2H3d/U/tvQsrHkVUqZAt4vOPQaFoomoV8k0TTNpmrZR07Rtmqbt0jTtH00R2HlReByiE0FncC1OalqjWSUOznUbvBCC//fDbuIiQ3joou72csCqHndLtUrc0TSNO0d34dM7hnG6uIKpb6w596EMexZDZHsYeDOEt4FpH0LiEFhwD+Rl1f3Yje/K0r/xT5/baysUTYw3KWgFcJEQYiCQCkzWNG2Eb8M6TwqPQZQUbos947ZqNKpVAue+DX7Z7lNsPHyWP03oSSuTEfShshMgrky7JS5O1sbI7nF8++BoOrYO546PN/HmigMNW/Q1l8P+pdDrUte3qpBwuO4j2U71u4dlzxpPVBTDhreg56XQru95vxeFoimoV7iFpNh+02j/E7idg6wWKMpxCTd24abxqkrcuXO03Ab/zirvsu5ys5wj2TUughuH2qegGEKwUNXjdrdKWkI5YH10bBPO/PtGcsWABP71817u/iSD/NJK7x58eBWYS6TV4U5MR5j4nGy/uuUzz4/d/AmU5cGYR88rfoWiKfHK9NU0Ta9p2lbgFLBECLHBwzUzNE1L1zQtPTc3t7Hj9J6SU7KNa1QC6PROj9ui0ehWCUjBmTYkic/WZ3Esv36BfWXJPg6dLuEfV/bFaN8ZiT6khsdtcPuQaekZt4OwED2v35jKs1P6sGrfKS6fvYaMTC+qdvZ8DyGtoMuYmvcN+SN0HAFLZ0mBdsdmhQ1vy/s7DmuU96BQNAVeCbcQwiqESAWSgGGapvXzcM0cIUSaECItPj6+seP0ngJ7bXBUIuiNLuH2UcYN8KcJPQGYvXR/ndetO3iG91Yf4qZhnRjTw+13pA+t4XHr7B8yYbqQFutxe8Lhe8+7dyRGzYLxgwms/eTZuksGc7ZDUprnahCdDi77F5SdhRX/W/W+vT9CfhaMuK9x34RC4WMaVGYhhMgHVgKTfRJNY1BoF+7oah63Tuez8VGJMWHcckFn/ptxlBV7PS+u7TxWwIxP0ukaH8nTl6VUvdMQ4vK4q1kl7UKiW1w5oDcM7BjDj2OzGKA7RM7+zdz24UZOF9fyzSMvE9p0qf3JOgyAtDtg07twdJM8JwSsexOiO9W0WBSKAMebqpJ4TdNi7D+HAROAPb4O7JwptE9jt3vcVrsdb/WBTeLOY5f0ole7VjwydytHzpRWuW/DoTP84f0NtDIZ+OSOYXJB0h23jNu15V0e24ZEU24p982g4kDGXEbY2pcBSEsIZePhs1z6+mrWHqi2Vb68UFogMZ3rfr6L/w6tEuS0dXMZZHwER9bCqIdBHxyDoBQtB28y7g7ACk3TtgObkB73Yt+GdR4UHgODCcJag07vzLgtPrJJHISF6HnnFtnb4vp31rEnp5DCcjOvL93PH97fQJvwEL6cMYKEGA+9nN0ybodV4si824ZEIRBU2rxcqGspbJsLRScgpBWdImHRA6OIMhmY/v4GXvllr2s4Q7691K91PcJtioIr34Az++XmnJ/+Al3HQ9qdvn0fCoUPqDfVEEJsBwY1QSyNg6MU0L5T0plx+1i4ATrHRjB3xghu/WAjk19b7Tx/Wf/2/O/VA4gOr2V2pN6tqqR6xm2MBGQtd6g+iHb07V8CrZMhuiOYy+jdIYrvHhrN377ZxezlB/hm23GempzCZP1h+ZtrnVz/c3YbDzd8DuvfkrevfrtRN2UpFE1Fy/uOWHhcVpQA6IxYnMLdNP9Be3eIYtEDo1i87Thmq41R3eMY1Kl13Q/Sh7qqSpwZtzy2NcjBxOWWcqJDo30Wd0BhNcPhX+Vcx8LjUHwSkNPkX7puIFMGdOD5H3Zz3+ebmRX3K7cDZREd8WouTe8p8o9C0YxpecJdcAySR8ufdQasQoAGZh973O4kxoRxz4XdvH+A285J5+KkTk+ozUaUTs40DJaSQACy06GySG4/3zkfzFXXDMb1asvo7nF8nZGN4adPKRThjHwlncn9OnD1oERGdI1Fr2tB0+UVimq0LOG2WaUvGp0ob+v0WETTZtznhHvG7darJNpmI0xr3uPLzomDy+XQiy5jZcmeueZ7N+h13DSsE2KfldIzyVyW2IEfd+QwLyObKJOBAUkxDEiKpneHKJJjI+gUG050WC1WlULRzGhZwl18Um6+adVB3tYZsFqbzuM+ZwwhmLWqHnfP6K5oB5cTat/6HlQlgQeXQ2IahMXIreuVtTef0vKziGjfkxenDeS5K/uxdPdJfjtwmu3ZBcz59RAWt/rvViYDbSJCaB0eQutwI63tP0eZjDgSdJ1OI8yoJyJUT1iIgYgQPWEheiJCDM5zkSEGosIMaD4qL1Uo6qNlCXdBtjzGdJJHnQGLQ7gbYWyZz4hKrGGV3JfyB/jp/9hkL/ypsASJVVKWB8c3w9gn5G1jmMeMG5C12PlZ0GMiIDs2ThmQwJQBco2j3GzlUG4JR86WkHWmlOP5ZeSVmskrrSS3uIJ9J4vJK62k9Bwm0Bv1GnGRocS3CiXecWxV9XZ4iIEQg4ZRr0Ov0+QfTUPnftRp6DQ5Y1Rn/yBwfB5oyA1JOg31IaGoQssS7vwj8ugm3I7/kpZAFu62vbFOeRk2Pu9cnHQ0xDLZBT1oMu7Dv4KwudqrGsPBUgY2W80KkOKTYCmvtaLEZNTTJyGKPglRdb6kxW12qMUmKDdbKam0UlZpoaTCSkmlhbJKea60wkJxhYUzJZXkFlWQW1TB8YJytmUXcKakotZeVo2FQ8T1Og2DU/hdou4u+v7Elx80Tf3eBLKjp+OvVoiqtxGua2IjQ/n1ifE+j6llCne0vXmT3lUOaAlkjxuwGkyAy+N2CXfLGBhcK/lHoLwA2veXtw8uh9Ao2ZIVpHCDFO+QiJqPhfo339SDQa9z+1kKfkx4w5/HYrVxttQl6OVmK5VWQaXFhs0msAqB1Saw2Y9Wm0AInOcB50Yr4RQDEAhs9hsC7I8Hq82G2er6pHA+1i0mIXy2YbhWfPnhJfzU305DQ9NcHxpajW9H8v6I0KaR1JYn3GFtIFTWPsvugJKAtkoAi5CROqwSR1+VFptxm8vgixtkZz9NJwcgDLwJDiyXi5KO3YwO4TZ7EO5SewOqiLimi7sODHodbVuZaNvK5O9QFC2cliXcBUdlK08HzUm4bXbh1qoJt/0jvcVl3BkfSdEe+wRkb5Rb0Y9ugIIjMPpPrutCHMJdWvM5ygvkMSzG5+EqFIFEyxLu/CMQ38t127lzUgtsjxuw2ocaOz1uu4CbRAsUbnM5rHkNOo+Gi56RtxfPlGIOVceHGe3baio9CHdZvjyalHArgouWI9xCQP5R6D7Recqm6RD2jNUa4KvyjhFrNRcnJS3KKtnyKRTnwLX2Yb5GE1z9FgyaDrl7oU1X17VGLzLu0LoXHxWKlkbLEe6S03IBy1FRAljcRpVZAnwnXU2rxD5QwSbQa/qWlXFv+Qw6pEJytcEHyaNdu14duHvc1SnPlwMUVHc/RZAR2P5BQ6heCkjVSpJA97gdGXf1xUlNWAnVh7acjDsvC05shb5Xe1fuUF/GbQqS/i0KhRuBrWYNocAh3K7FSfce3Ba/V7bWTY2MW9NktYWwYjKYWk7Gvftbeewz1bvrHR63J+Euy1cLk4qgpOUId/Uabqr2Jwn0jNtis6DTdFWmu6PpwWYhzBDWcppM/f4NtB9Q1ceui5C6rBKVcSuCk8BWs4aQfxRCo6tkYO6VJNYA97itwurafONAZwCbtEpaRJOpohzI3uR9tg0uq8RTv5LyfFVRoghKWo5wnz1UYwqK+9Qbc1PH00CsNqvL33ZgF26TwdQyMu7D9uES3Sd4/5g6FydVxq0ITlrOcnzuXug8ssopd3sk0K0Szxm3DmwWTPpm5HFv/hQOLJWj4y56FiJiXfdlrpale+0HeP989XncSrgVQUi9wq1pWkfgE6A9YAPmCCFe93VgDaK8EAqzq26+oao9Yg1spwSzzew54xZWTEYTxZXF/gmsIVQUw09PSbEty4PKYrj2Pdf9Wb/JD9eGtNjVG0FnrCncVosctqAWJxVBiDdpqAX4sxCiNzACeEDTtD6+DauBnN4vj/EpVU5X8bgDvKrEKqyuzTcO7IuTzaYccNdCKdY3fA5jHoMdX8P+pfK+ohw4cwA6j2r484aE17RKKgrlUWXciiCkXuEWQpwQQmy2/1wE7AYSfR1Yg8jdI49te1c5bXGrE7YE+s5Jm9VVCuhAZ5BWSXMpB9zyKcT1hI7DYMyjENsDfnpSTibKXCOvqb7BxhuMHoYplKvt7orgpUHGr6ZpyciJ7xt8Ecw5k7sb9KE12nu6+9qWwNZtzxm3zgA2G+GGcIoqi5xtOwOS3H2ySdTgW2UNuiFU9iE5c0CWAO5Z3HB/24GnYQqO7e4q41YEIV4Lt6ZpkcB84BEhRKGH+2dompauaVp6bm5uY8ZYP7l7Ia5Hja3P7v1JGj7jpGkx28weMm65OJnSJoW8ijyyi7P9E5w3ZNkz6t5XuM71niqz7sUzpY0y7O5z255ujKgp3I4GU8rjVgQhXgm3pmlGpGh/LoRY4OkaIcQcIUSaECItPj6+MWOsn9w9NfxtaGYed23lgMLK0PZDAUjPSfdDZF5ycpfMqN2/9ej00jIpz4eek2H8M+f23MYwMFe3SlTGrQhe6hVuTY56eB/YLYR4xfchNZDKEns7Vw/C7f5zYOt2HVaJha7RXWljakP6yUAW7t+hbZ+a/UcG3CAXK6d90LBqEnc8LU46PW4l3Irgw5uMexRwC3CRpmlb7X8u83Fc3nPyd3lsW1O4m5NVYrV5qOPW9GCzomkaQ9oNYVPOJv8EVx9CyIy7Xd+a9+n00HtKzek1DcEYXrMc0JlxK6tEEXzUazgKIdbg/9mjtZP1mzx2HFHjruYk3BZh8eBxS+EGSGuXxpKsJRwrPkZiZGAV9VB4DCoKoJ2PqkSNYTUHKZTlyw+28/lAUCiaKYG9ndAbMldLmySypq9etRywKYNqOBabxYPHLeu4AafPveFEYBX0ADLbBmjXzzfPb/RklRTIhckAL/NUKHxB8xZuqxmOrK+1NtiRceuFqOJ3ByJWUUsdt71Pd/eY7iREJLAka4kfoqsHh3BXq6NvNDxaJWq7uyJ4ad7CfWKb3KlXi3A7xDpUCPvsycDFarNi1BmrnrQvTgJomsakLpNYf3w9+Y6FuUDh5C7ZTtdXQmoM8+xxK39bEaQ0b+HOtHeb61xLxm0/SuEObCzCg1WiuTxugMnJk7EIC0uPLG26wEpOw/q3YcX/ykVIT5zc6XlhsrEIiQBrpexP4qD0jKrhVgQtzVu4j6yHuF4e/W1w+dohzcEq8bjlvapw927Tm85Rnfnp8E9NE5SlAt4aJbetr/o/KdDVKcuTdfRJab6Lo3qHQCHgzEFo0813r6lQBDDNW7jPHPRYBujAsekmVAgsAW6V1Lc4CdIumZw8mU0nN3G67LTvgzqxXU5jn/hPOUZt9+Ka1xzdKI+dLvBdHE7hti9QFuXIJlNxPX33mgpFANN8hdtmg/ysGv1J3HGIdbPwuGubgCOqmjyXdrkUm7DxS+Yvvg8q2y7K/a+Twrz7u5rXHFkn264mDPZdHCGt5NFRu316nzzGK+FWBCfNV7iLc6Tv2Tq51ksckmeyBb5wW2yWWndOutMtphs9Wvfgp8wmsEuyN8lFx6gOsgfJqV3yW447R9ZDQqprNqQvcMynPGt/bYdwq4xbEaQ0X+HOy5TH1nVl3JIQIbAEcmc97OWANRYndVU8bgeTkyez5dQWThSf8G1Q2eku7zrlcnn8fZHrfnM5HMuATjU3PzUqjsw6d6/rGNIKWnXw7esqFAFKMxbuLHmMSa71Emtzskpq7cftWbgBvj/8ve8CKjwBBUchSW78IaYTJI+BTR/I+nmA45vltx5f+tsgywwj27sy7dP7ZDdItflGEaQEnHDbhM27C/OzAA1iOtZ6icPjDmkmi5PeWCWU5dOprIi0dmnM2zcPqwdhbxSO2RtaOYQbYORDckTczgWysmP1yxAS6XvhBpl1OzLu0/tqjKlTKIKJgBHuEnMJt/54K1/u+dK7B+RlQlSCbP208ckAABplSURBVNhfC445kyYhsAa4VVJ7r5Jqwr3ieXhnLDfHDuZY8TFWZa/yTUDHMuQHh/vgg+4TIb43rH5JivaBpXDRXyG8jW9icCeulxTs8kIoOiEzboUiSAkY4Y4wRmC1WZm7Z653k17y6q4oAZy+tiwH9DKT9xO1tnWt/g0kcw3YLIxfOZv2YfF8sfsL3wR05qBc+DWa3OLRwcXPwtlDsPyf0CEVhs3wzetXJ76XLAF0bLqKUxm3IngJGOEGuDHlRjILM1l/Yn39F+dl1llRAi6P29gMMm6PHremq5pxl+XBqd+h//UYyvL4g6EtG3I28O72d0nPSefl9Jd5YeMLbMvddv4B5WVC6y41z6dcDk8cgunz4aa5595ju6E4KkhW/K/cUZowqGleV6EIQM5hjpTvmJQ8iZfSX+LLPV9yQUIdvqmlQn5drqOiBKRwG4TAAFiba8bt7mEf2QAIGHIbFGRzy+kT7O59ObO3zAYgRBcCwPz985kzcQ6pbVPPLRghpHB3HO75flM09Jhwbs99rjg87ZM7YMjtEB1grW0ViiYkoDLuEH0I1/a4llXZq8gpyan9wvyjgKg347YImxRuIbB4u+jpJzzPnKy2OOnY7JI4BJJHozuxnf8Z8gR39ruTvwz7C7/d9Bs/T/uZ+LB47l96f92/w7ooy5O2RBsPGbe/iGwHodFgMMGFT/o7GoXCrwSUcANc2/NahBAs3L/Q8wU2G6x8Xv5cT2MjCwI9oIfmUQ7occu7e8a9TloExjBIHgXChiE7nUeGPMLNvW/GZDARFxbH2xPeptJWySsZr7D37F5mrZ3VsC3yZw/LYz0fjE2KpsGwu+T2+6gEf0ejUPiVgBPuxMhELki4gAUHFngudVv+T9g5HybMgvb963wuqxDom0HGbRM2BKKWxUn778BSAce3uDa7JA2T2bdjurobHaM6cnvf2/nx8I/c9tNtzN8/n0dXPkqFtYLS6u1RPZHnEO4AyrgBLv4bDG+ixVCFIoDxZljwB5qmndI0zUNrON9wbY9rySnJYe3xtVXvsNkg40O5/XrUI/U+jxUbBkAvwBrAwu34gKo5c9JtcbIkV252ie0ub4eEy12NmTWFG+COfneQEJFAfFg8Twx9gi2ntnDBFxcw6stRLD7koVmUO3kBmHErFAon3ixOfgS8AXzi21BcjO84njamNszfP58xSWNcd5zaJf3XXpd5tWvOImwy40ZgQ2ATNnRawH3JwCKkONe0StwWJ8sL5dEU5bq/80hY85rsmufooGcn3BjO11O/JlQfSqg+lOjQaHae3smuM7uYtXYWXaO70ifWNSOysLKQVsZWaJomFyYj2/m2/4hCoThn6lUxIcSvwNkmiMWJUW/kym5Xsuroqqre7GF7DW/yGM8PrIZFuDJuwHe7DM8Tiz2rrnNxssIu3KFuwt1+gLRScvd4fN6okChC9XKD0tRuU3l6+NPMHj+b1qbWzFgyg3XH1wFwqOAQ474axzXfXsOyI8vgbGbg2SQKhcJJ4KWfdq7pcQ0WYWHRAbemRpmr5df3Ora5u2MVVvQC9PaFSUdmG2g4rZIaHrceENIicrQ0dR8P5hjOe/J3r18rNiyW9y55jzhTHPcuvZc1R1by+c6P0dCwCRuPr3qcM/mZyiZRKAKYRhNuTdNmaJqWrmlaem5u7nk/X3J0Mmnt0liwf4HsX2KzQtZvXmfbIAXRABgCPeO2f6DU7Mdtz8CF1c0qcRPuNl3AEOYa1uslnaM688Wln9HFGMOspQ/y3f4FXN71cl4d9ypmm5mFBFgpoEKhqEKjCbcQYo4QIk0IkRYf73mUWEO5psc1HC06KncC5myXWWeXsV4/3iIs6AGDPeO2isAUbscHiseZkyDtkgp7xu1ulej0cmPKqYYJN0D45k+Ylfk7p3RQpsH0blfRNaYrw2NS+G9UJFZfTWxXKBTnTcBaJSAXKUN0IXLaS8ZHoA+FruO9frzFZsGA5vS4LdUbNgUIjg8Ujx43yG8bnhYnQdayN8AqASBnJyz9O6nJE3g0+UpuLiiiV1kxADca23LCYGCVMfBapn6862OWZC3xdxgKhd/xphzwS2Ad0EvTtGxN0+70fViSyJBIRiaOZEnmz9i2fgGpN9c6GNgTVmG1b8Cxe9wBKtyOuDzWcYPMuMsLZN22wVT1mnZ9oeSUnMbuLT89BaYYmPpvbh/2Z/5yNg+ObQZg3KlM2gmNuYcXU2mtZOaKmaw55rnksCk5mH+Ql9Nf5vXNr3vXhEyhaMF4U1VykxCigxDCKIRIEkK83xSBObik8yWcLMtlh1EHox5u0GOlx625PO4AtUqc5YCe2rqCzLgrCmW2Xb0Msq29pM9bn7sgWy7yDrsbIuJkS9bWybKNq7kMw9GNXNeqF+tOrOP5Dc+z9MhSXst4ze9iOWf7HASCrMIsDuQf8GssCoW/CWirBGBcq64YheDnjv1dswe9RHrcGg45DNTFybqrSnAtToZWs0nAte3fW+HeMU8e+13rOpc4RGbcRzeAtZJre92AQWdg/v75tDG1YW/eXjbmbGzAO2pcMgsy+SnzJ6Z2m4qGxtIjS/0Wi0IRCAS2cAtBq1/+zqhyMz8bLA0WXotNCrdBBHg5oKhlcdLdKqkorFpR4iCyrRzoe2iFdy+2Yx4kpkFsN9e5xCFyss3af4OmJ67HJCZ2nohBMzBn4hzamNrw8a6Pz+GdNQ6/ZP2CTdiYOWQmg9oOYmmWEm5FcBNYwl16Vu7aO7Ie1r0JH18BB5YwpesUTpWfaXDW57BK9HbhDviMu8aWd7eqkvLCmguTDvpdCweWQfGp2l/kxHb49iHZFnXA9VXvSxgsjweWwqg/QWgrnhn+DJ9f/jm92vRieu/prD62mk92Ndnm2SpsytlEz9Y9iQuL4+JOF7Mvbx9Hi476JRaFIhAIqH7cvJwC1grX7dgeMO5pxo18kMh5a1l8aHHdfbqrYRVW9JrmfJOB6nGbbXL4bu0Zt1UuTrpnye4MvBF+e0023xpxnzwnBPz4pOx30mUszLtD/jzgRrnI607SUBjzGHS7SHYdBKJDo/9/e+ceHUWVJvDf1915k5CQJwYIIcKiEAQMgrOA4woCspph9OzBs6MccQQPw4wOq7sqM76OMw7D7J4zclSWdR1l1QEZRFB8DocVHBYEhRAQQngJiSEJSYBA6CTdffePqm466e68IN3Ven9Jn6q6davq669uf3Xru4+PvnFGDX/uyLkcrD/I0l1LefPAm3jwMDl3Mj+59ifk9+3d/t6t7lb21OzhzmGGa2fCVcYkW3tq9jAwuWsDsTSa7xrWMtwzloA9FhLTof91kNIfgDjg1sG38tGxj/jVhF+R4Ejo+Dwm0d8dsH3jZBBXCUDWNYa+Sv4M4x80GjB3vwFf/Kexf8fLRqzIOe8F75VjdxghyULgsDlYMmkJA/oMoPZiLc3uZt47+h4fHv+Ql255qecBG7rAvrp9ON1OxmUbQYsL+haQ6Ehkb+1ebi+4vdeuq9FYGWsZ7qL7Qu6amT+Td8rfYUvFFqYNntal0xkBeG2WH/LebL5lxNpj2+7oSuOklzH3wAePwPqfGTXnjx43Rpne8hSUvg2T/7VbXSnbE2OPYVHRIt925flK5n86n3mfzmPVzFUMSe1ew3FX2XlqJwDXZ18PGG8lIzJGUHq6tFeup9FEA9bycXfA9dnXkx6fzsfHP+7yMYaP2+ZrnLSqj7u2yZgiIDOhnWH1ukpczdDSGLrGDVB0vxEZZs+bsPZ+I2/xizBwHNy29LKMdjBy++Typ2l/ItYey1PbnjKmJegFvP7t1PhUX1phRiFlDWW+B55G830jagy33WZnSt4UtlZs7VowAKLHx13dVA1AVmJW2x1e14nzjLEM1TgJRgT2m5+ABzbD3E/g4b2dxuS8XDITjbm+99TuMfpZ90Jf7/11+xmd2dYVU5hRiMvj4mB98FkRNZrvOlFjuMEIJux0O9lSuaVL+V0eFw6xWd7HXX2hmtS4VOLbj4r01ribzFl1O3KVeMkdC4PGhy36+u1Dbmdq3lRe3PMiD/71QZ7b/hzbKrd1fmAXONt8lsaWRgalDGqTXphhRD4qrdXuEs33k6gy3GOzxpIen85r+16jwdnQaX6jxm2z/JD36qZqshOzA3d4je9F87t2VOOOECLCH276A48UPcKBugNsOLKBhzY/dEW661WcrwBgQPKANunZSdlkJWax9/Tey76GRhONRJXhttvsPDH+Ccobyrl7492dBsA1BuDYLD/kvaaphuykjgx3N2rcEcAmNuaMmMOW2VvY8KMNOGwOntn2zGW7TioaTcPdZ0DAvlEZo3SNW/O9JaoMNxjdAl+Z9gqV5ytZf3h9h3ndHjcOsV+a1tWijZPVTdWB/m0IdJVYsMbdnpykHBYVLWLHqR08u/3Zy9J55flKwGgIbU9hZiEV5yuod4Y1OJNGYwmiznADjMkaw6jMUXxw7IMO87lUOx+3BbsDNrubqXfWB3eVSHtXSWpgHgty19C7eKDwAf5y6C/cueFOFn++mK/rujn1LEaNOy0ujT6xfQL2ef3c+06HLYa1RmMZotJwg9Gv+1DDIcobykPmcXva+ritWOOuaTKGqQf3cfegcdICiAi/GPsLnv3Bs2QlZrH55Gbu3ng3S3cu7VY7Q0VjRYB/28uI9BHYxObrz13RWMH8T+fzm+2/Ydu32yI+m6FG05tEreGeNngadrHz/tH3Q+ZxeVzYxe7zcVuxxu0z3EF93KbhtnDjZEfMGjqLFbeu4KM7P+KuoXex8uuV/HLzL7vUsAxG42QwNwkYUewLUgsorS3lbPNZFmxawO6a3aw/sp75n85n9sbZvPH1G5TVl+F0Oa/k19JoIo61Rk52g/SEdCYPmMzKr1dyTfo1TB88PSCPWxk+bivXuKsvGH24cxJzAnfazOfqxXoj+o8jLoySXTlSYlP49Y2/ZmjaUJ7/4nmmrJnCDf1vIDsxm5lDZjIuZ1zAMS6Pi6rzVR2Okh2VMYpPvvmEBX9dQEVjBSumrmBU5ig2Ht3I6/tfZ8nOJQAIQk5SDoOSB5GbnMtVSVeR3zef8f3H++Zj0Wiiiag13ADPTXyOn2/6OY9+9iiPfvYoE/pPYMXUFYgISinDcNusWeM+euYoL5e8TGaiMaKxw8bJxlOQEB3+7Y6YPXw243LGsergKr6q+YqS2hLWlq9lYu5Epg+ezrC0YeQm55ISm0J1UzUu5Qrao8RLYUYha8vXUtZQxtKbllKUUwQYNf1ZQ2dx8txJSk+X8k3jN5w4d4ITjSfYUrHF1xvJJjauy7yOibkTmZQ7ieH9hiPtA1VoNBYkqg13SmwKy6cuZ03ZGg7UH+D9o++ztXIrkwdM9pu4ydbhtK6HGg7xdtnbHDlzhLkj5zJpQNejyPcUj/Lw5LYnKaktwSY2kmKSgjbA+Ronm8/ByB/3ulzhoCC1gMUTFgPgdDlZ+fVKVpet9oVHc9gcFBcUMzRtKBDYh9ufmwbexM0VNzN35NygE10NTBnIwJTAGQSdLicH6w/yeeXnfF75Oct2L2PZ7mVkJmRy41U3MiZrDIUZhVydenXgjI0ajQWIasMNkOBI4N4R99LqaWV3zW6WlyxnUu4kv+AEDhwYtaj2/bhrm2q554N78CgPafFpLNi0gHE547hl0C0MTR3KwOSBZCVmXfEf7zvl71BSW8KojFHsPb03eMMkXKpxA9ww74rKYAXiHfHMGzWPBwofoKyhjIrGCrZXbWdd+TpaPC1A8K6AXjISMnjhH17o0XVHZ41mdNZoFo5ZyOmLp/lb5d/YWrmVzyo+Y8ORDYBRtkakj+Ca9GsY0ncI+X3zGdJ3CGnxaT37whrNFaJLhltEpgN/BOzAK0qp3/WqVD0gxhbDTwt/yjP/9wwv7H6Be669B8DwcYvhK27fo2HZ7mW0eFp4t/hd+if1540Db7CufB2/++LS13PYHPSL70d6fDr9EoxlTlIO+X3zye+bT15yHn1i+/hcM3axB33dVkrR7G5m49GN/HbHbynKLuKlKS8xa/2sgCHdPrwPjLyJl0KUfQcREYb3G87wfsOZkjeFhaMXsqVyCw3Ohg4N95UiIyGD4quLKb66GKUUJxpPsLd2L6WnSymtLWVN2Rqc7ksNnKlxqeQk5ZCRkEFWYhYZCRmkx6eTHJvs+/SJ6UNKbApJsUkk2BNw2BzaDaO5YnRquEXEDrwITAUqgJ0iskEp1f2Oub1M8dXF7KrexSulr7D64GoAHDa7r8Z8ofUC51rO4fK42HRiE+8efpc5I+aQl2JMxjR35FzuG3EfVReqONF4gpONJ6lsrKTOWUfdxTrqnHUcbjhM7cXaNrPhJTgSaHW34lIubGIjKzGL9Ph0Lrou0uRq4kLrBZpam3w1/hv738jSm5aS4EjgrZlvBc7D7aVPFqTmweR/6UWtWY/U+FTuKLgjItcWEfJS8shLyfPN9+1RHqouVHH0zFGOnT3G8XPHqWmqoaaphrL6MuqcdZ3OjmgTG3H2OBIcCcTZ44h3xBNvjyfeEe/bjrXF4rA5iLHFtF3aY3CIsYyxxQTuN5cigl3s2MTmW/p/2qcF2w52DkGMpfngEQQR8aV79eb7kxDLEPs03Uc66+8qIjcCTyulppnbjwMopZ4PdUxRUZHatWvXlZSzW+w8tZP1h9dzqOEQT6RPYOTRbVzvKQ/4cQ1LG8Zr018jOTa5W+dvcbdwsvEkx84e42TjSWov1hJnjyPOHkeLu4VTF05R31xPoiORREciSTFJJMUkkRiTSHZiNjPyZwQGBtZELW6PmzPNZzjfep7zLec513KOxpZGzreep7GlkWZ3M06XE6fbSbOrGafbidPlbJPudDlp8bTg8rho9bTi8rjarLd6Wntt6lwr4H1ACILxLx0+MEI9FPzzedd9S/MZ4Uvzz98uzUvQc8ml/L58Zp60uDRen9Gz+Kwi8qVSqqgrebtiPXIB/xmDKoDxQS46D5gHMGhQiFf/MDEuZ1zbLmY/WMQfT/5vm4mPRmaMZHTm6B498WPtsRSkFlCQGiKUmOZ7hd1mJz0hnfSE9F69jtvjxqVcgUbd3UqravW56zzKg1u522x7P27lxuPx4MHc9pj7ubTPrdwoVJt9Sinj4/0z18F4I/Hf9q57lCcgTSmFBw8oAs4VsDQy+c7j3QdcOn+Q6/pve2mf1v4Yf/zzBD1OtTunX57uVgJ7SlcMdzDLFlBNV0qtAFaAUeO+TLmuOD8c+MNIi6DRXBZ2mx07duLs0dmfX3Pl6MrIyQrAv0/VAODb3hFHo9FoNJ3RFcO9ExgqIvkiEgvMBjb0rlgajUajCUWnrhKllEtEFgIfY3QHfFUptb/XJdNoNBpNULrUtUEp9QHQ8RyqGo1GowkLUTs7oEaj0Xxf0YZbo9FoogxtuDUajSbK0IZbo9FoooxOh7z36KQitcA3PTw8A+g4fHtk0HJ1H6vKpuXqHlqu7tMT2fKUUpldydgrhvtyEJFdXR2vH060XN3HqrJpubqHlqv79LZs2lWi0Wg0UYY23BqNRhNlWNFwr4i0ACHQcnUfq8qm5eoeWq7u06uyWc7HrdFoNJqOsWKNW6PRaDQdYBnDLSLTRaRMRA6LyGMRlGOgiGwWkQMisl9EHjLTnxaRShHZY35ui5B8x0Wk1JRhl5nWT0Q+FZFycxnWaLYi8nd+etkjIudE5OFI6ExEXhWRGhHZ55cWVD9i8IJZ5vaKyNgIyLZURA6a118nIqlm+mARueinu+VhlivkvRORx02dlYnItDDLtdpPpuMissdMD6e+QtmI8JUzX2SLCH4wZh08AgwBYoES4NoIydIfGGuuJwOHgGuBp4FHLKCr40BGu7TfA4+Z648BSyJ8L08BeZHQGTAZGAvs60w/wG3AhxjBQiYAOyIg262Aw1xf4ifbYP98EZAr6L0zfwslQByQb/5u7eGSq93+fweejIC+QtmIsJUzq9S4bwAOK6WOKqVagFVAcSQEUUpVKaW+MtcbgQMY4dusTDHgDXT3OvCjCMpyC3BEKdXTAViXhVJqC1DfLjmUfoqBlcpgO5AqIv3DKZtS6hOllMvc3I4RqCSshNBZKIqBVUqpZqXUMeAwxu83rHKJEXPwn4A/98a1O6IDGxG2cmYVwx0srmXEjaWIDAbGADvMpIXmq86r4XZH+KGAT0TkSzHifAJkK6WqwChUQFaEZAMj0Ib/j8kKOgulH6uVu7kYNTMv+SKyW0Q+E5FJEZAn2L2zis4mAdVKqXK/tLDrq52NCFs5s4rh7lJcy3AiIn2AtcDDSqlzwMtAATAaqMJ4TYsEf6+UGgvMAH4mIpMjJEcAYkRIugNYYyZZRWehsEy5E5HFgAt400yqAgYppcYAi4C3RCQljCKFundW0dndtK0ghF1fQWxEyKxB0i5LZ1Yx3JaKaykiMRg35E2l1DsASqlqpZRbKeUB/oteej3sDKXUt+ayBlhnylHtffUylzWRkA3jYfKVUqralNESOiO0fixR7kRkDvCPwD8r0ylquiLqzPUvMXzJw8IlUwf3LuI6ExEH8GNgtTct3PoKZiMIYzmziuG2TFxL03f238ABpdR/+KX7+6RmAfvaHxsG2ZJEJNm7jtGwtQ9DV3PMbHOA9eGWzaRNLcgKOjMJpZ8NwL1mq/8E4Kz3VTdciMh04N+AO5RSTX7pmSJiN9eHAEOBo2GUK9S92wDMFpE4Eck35foiXHKZTAEOKqUqvAnh1FcoG0E4y1k4WmG72FJ7G0br7BFgcQTlmIjxGrMX2GN+bgP+Byg10zcA/SMg2xCMFv0SYL9XT0A6sAkoN5f9IiBbIlAH9PVLC7vOMB4cVUArRk3n/lD6wXiFfdEsc6VAUQRkO4zh//SWteVm3jvNe1wCfAXcHma5Qt47YLGpszJgRjjlMtNfAx5slzec+gplI8JWzvTISY1Go4kyrOIq0Wg0Gk0X0YZbo9FoogxtuDUajSbK0IZbo9FoogxtuDUajSbK0IZbo9FoogxtuDUajSbK0IZbo9Foooz/B73vVUthIXaBAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_G_l1_list[0:200], label = 'Generator L1')\n",
    "plt.plot(loss_G_gan_list[0:200], label = 'Generator BCE')\n",
    "plt.plot(loss_D_list[0:200], label = 'Discriminator BCE')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mapsgan)",
   "language": "python",
   "name": "mapsgan"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
